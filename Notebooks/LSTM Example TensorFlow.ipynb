{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dsi/mbouchouia/code/git/PRIM/Notebooks\n"
     ]
    }
   ],
   "source": [
    "cd /home/dsi/mbouchouia/code/git/PRIM/Notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import pymongo\n",
    "import os\n",
    "sys.path.append('../scripts/')\n",
    "sys.path.append('../papers code/DCRNN/')\n",
    "import Plotting\n",
    "import folium\n",
    "import matplotlib\n",
    "import osmMerger\n",
    "import CustomUtils\n",
    "import OsmProcessing\n",
    "import pandas as pd\n",
    "from tf_cuda_subprocess import *\n",
    "from   functools import reduce\n",
    "import seaborn as sns\n",
    "import models\n",
    "from mongoConnection import *\n",
    "sns.set()\n",
    "results_path = \"../images/model results/\"\n",
    "def saveFig():\n",
    "    if not os.path.isdir(results_path+model_name):\n",
    "        os.makedirs(results_path+model_name)\n",
    "    print(results_path+model_name)\n",
    "    plt.savefig(results_path+model_name+input(),dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# @RunAsCUDASubprocess(num_gpus=1)\n",
    "def createSession():\n",
    "    config = tf.ConfigProto(intra_op_parallelism_threads=8, \n",
    "                            inter_op_parallelism_threads=8,\n",
    "                            allow_soft_placement=True,\n",
    "                            log_device_placement=True\n",
    "                           )\n",
    "\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "    session = tf.Session(config=config,graph=tf.get_default_graph())\n",
    "\n",
    "    tf.keras.backend.set_session(session)\n",
    "createSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import dcrnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeds = pd.read_pickle(\"../data/monthsSpeed__0.pckl\")\n",
    "counts = pd.read_pickle('../data/monthsCount__0.pckl')\n",
    "\n",
    "speeds_05 = pd.read_pickle(\"../data/monthsSpeed__5.pckl\")\n",
    "counts_05 = pd.read_pickle('../data/monthsCount__5.pckl')\n",
    "\n",
    "speeds_10 = pd.read_pickle(\"../data/monthsSpeed__10.pckl\")\n",
    "counts_10 = pd.read_pickle('../data/monthsCount__10.pckl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeResults,segmentsMeta, mergedSegments= osmMerger.mergeSegments(minValidData = osmMerger.MIN_VALID_DATA, weights=np.array([2,1,1,1]),speedsMx=speeds)\n",
    "# mergeResults.to_pickle(\"../data/mergeResults.pckl\")\n",
    "# segmentsMeta.to_pickle(\"../data/segmentsMeta.pckl\")\n",
    "# mergedSegments.to_pickle(\"../data/mergedSegments.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeResults=pd.read_pickle(\"../data/mergeResults.pckl\")\n",
    "segmentsMeta=pd.read_pickle(\"../data/segmentsMeta.pckl\")\n",
    "mergedSegments=pd.read_pickle(\"../data/mergedSegments.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner = models.DataCleaner(speeds,segmentsMeta,mergeResults,counts,0.80)\n",
    "\n",
    "\n",
    "data_cleaner_05 = models.DataCleaner(speeds_05,segmentsMeta,mergeResults,counts_05,0.80)\n",
    "\n",
    "\n",
    "data_cleaner_10 = models.DataCleaner(speeds_10,segmentsMeta,mergeResults,counts_10,0.80)\n",
    "\n",
    "intersectIndexes= np.intersect1d(np.intersect1d(data_cleaner.data.index,data_cleaner_05.data.index),data_cleaner_10.data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedIndex=pd.Series(data=segmentsMeta.loc[mergeResults]['segmentID'].values,index = segmentsMeta['segmentID'].values)\n",
    "speedssegs = speeds\n",
    "speedssegs = speedssegs.assign(newIndex =mergedIndex.reindex(speedssegs.index).values)\n",
    "speedssegs = speedssegs[~speedssegs.newIndex.isna()]\n",
    "\n",
    "naStats=speedssegs.groupby('newIndex').apply(lambda x : pd.Series([(x.isna().sum(axis=1)/x.columns.size).mean(),(x.isna().sum(axis=1)/x.columns.size).min(),len(x)],index=[\"mean\",\"min\",\"nSeg\"])).reindex(intersectIndexes)\n",
    "\n",
    "intersectIndexes= np.setdiff1d(np.intersect1d(np.intersect1d(data_cleaner.data.index,data_cleaner_05.data.index),data_cleaner_10.data.index),naStats[naStats['min']>0.4].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedDF = data_cleaner.data.reindex(intersectIndexes)\n",
    "data_cleaner.data=data_cleaner.data.reindex(intersectIndexes)\n",
    "countDF = data_cleaner.counts.reindex(intersectIndexes)\n",
    "data_cleaner.counts = data_cleaner.counts.reindex(intersectIndexes)\n",
    "\n",
    "speedDF_05 = data_cleaner_05.data.reindex(intersectIndexes)\n",
    "countDF_05 = data_cleaner_05.counts.reindex(intersectIndexes)\n",
    "speedDF_10 = data_cleaner_10.data.reindex(intersectIndexes)\n",
    "countDF_10 = data_cleaner_10.counts.reindex(intersectIndexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" we don't want to center using unseen data\"\"\"\n",
    "valid_split = 0.67\n",
    "sequence_length=20\n",
    "n_dates=len(set(pd.to_datetime(speedDF.columns).date)) \n",
    "train_dates_index = speedDF.columns[:int(valid_split*n_dates)*sequence_length]\n",
    "train_intercept=speedDF[train_dates_index].groupby(pd.to_datetime(train_dates_index).time,axis=1).mean()\n",
    "intercept_extended_df= pd.concat([train_intercept]*n_dates,axis=1)\n",
    "intercept_extended_df.columns=speedDF.columns\n",
    "# centering\n",
    "speedDF = speedDF-intercept_extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix= OsmProcessing.getAdjacencyMatrix(segmentsMeta)\n",
    "\n",
    "adjacency_matrix = OsmProcessing.addLevel(adjacency_matrix,3)\n",
    "adjacency_matrix = OsmProcessing.mergeAdjacencyMatrix(adjacency_matrix, mergeResults, segmentsMeta)\n",
    "adjacency_matrix = adjacency_matrix[speedDF.index].loc[speedDF.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(adjacency_matrix.clip())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = adjacency_matrix.clip(upper = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMlpModel(loss,optimizer,nlayers,neuronsPerLayer,relu_output):\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    for i in range(nlayers)  :    \n",
    "        model.add(tf.keras.layers.Dense(neuronsPerLayer*(i+1)))\n",
    "    model.add(tf.keras.layers.Dense(nSegments,name='out_layer'))\n",
    "    if relu_output :\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def createStackedLstmModel(loss,optimizer,nlayers,neuronsPerLayer,relu_output):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #layers\n",
    "    for i in range(nlayers)  :    \n",
    "        model.add(tf.keras.layers.LSTM(neuronsPerLayer*(i+1),return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(nSegments,name='out_layer'))\n",
    "    if relu_output :\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_grid= { 'optimizer':['Adam'],\n",
    "              'nlayers':[0,1,3],\n",
    "              'neuronsPerLayer':[64],\n",
    "              'loss':['MSE','MAE'],\n",
    "              'relu_output':[False]\n",
    "             }\n",
    "\n",
    "train_grid={\n",
    "    \"batch_size\": [8,32,64], \n",
    "    \"epochs\": [100],\n",
    "    \"validation_split\": [0.5]\n",
    "}\n",
    "\n",
    "preprocess_grid={\n",
    "    \n",
    "    'scale_Max':[True],\n",
    "    'scale_log':[False],\n",
    "    'input_lag':[1,6],\n",
    "    'output_lag':[1],\n",
    "    'time_as_input':[True],\n",
    "    \"validation_split\":[0.5]\n",
    "}\n",
    "\n",
    "def getParamsFromGrid(grid):\n",
    "    return [dict(list(zip(list(grid.keys()),x ))) for x in list(itertools.product(*list(grid.values())))]\n",
    "\n",
    "def grid_search(data,buildfunc,build_grid,train_grid,preprocess_grid):\n",
    "\n",
    "    grid = list(itertools.product( getParamsFromGrid(build_grid),\n",
    "                                   getParamsFromGrid(train_grid),\n",
    "                                   getParamsFromGrid(preprocess_grid)\n",
    "                                 )\n",
    "               )\n",
    "    \n",
    "    losses=[]\n",
    "    val_losses=[]\n",
    "    mseLoss=[]\n",
    "    mseValLoss=[]\n",
    "    for build_param,train_param,preprocess_param in grid :\n",
    "        print(build_param,train_param,preprocess_param)\n",
    "        x,y = preprocessData(data,**preprocess_param)\n",
    "        xtrain = x[:int(len(x)*(train_param['validation_split']))]\n",
    "        xtest = x[int(len(x)*(train_param['validation_split'])):]\n",
    "        ytrain = y.reshape(-1,nSegments)[:int(len(x)*(train_param['validation_split']))]\n",
    "        ytest = y.reshape(-1,nSegments)[int(len(x)*(train_param['validation_split'])):]\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.keras.backend.set_session(tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=32, inter_op_parallelism_threads=32)))\n",
    "\n",
    "        model=buildfunc(**build_param)\n",
    "        model.fit(xtrain,ytrain.reshape(-1,nSegments), validation_data = (xtest,ytest), **train_param,verbose=0)\n",
    "        \n",
    "        losses.append(model.evaluate(xtrain,ytrain,verbose=0))\n",
    "        val_losses.append(model.evaluate(xtest,ytest,verbose=0))\n",
    "        \n",
    "        mseLoss.append(mse(reversePreProcess(ytrain,**preprocess_param),reversePreProcess(model.predict(xtrain),**preprocess_param)))\n",
    "        mseValLoss.append(mse(reversePreProcess(ytest,**preprocess_param),reversePreProcess(model.predict(xtest),**preprocess_param)))\n",
    "        print(losses[-1],val_losses[-1],mseLoss[-1],mseValLoss[-1])\n",
    "        \n",
    "    resDF = pd.DataFrame(list(zip(grid,losses,val_losses,mseLoss,mseValLoss)),columns=['params','loss_value','val_loss_value','mseLoss','mseValLoss'])\n",
    "    \n",
    "    return  pd.concat([resDF.params.apply(lambda x : pd.Series({**x[0],**x[1],**x[2]})),resDF[['loss_value','val_loss_value','mseLoss','mseValLoss']]],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resMlpDF = grid_search(data,createMlpModel,build_grid,train_grid,preprocess_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resLstmDF.groupby(\"time_as_input\")['mseLoss'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resLstmDF = grid_search(data,createStackedLstmModel,build_grid,train_grid,preprocess_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    lstm=tf.keras.layers.LSTM(100,name=\"speed_lstm\")(main_input)\n",
    "    lstm_branch = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"lstm_branch\")(lstm)\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([lstm, daytime_input])\n",
    "    x=tf.keras.layers.Dense( 100, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\" )(x)\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse , optimizer=optimizer)   \n",
    "    data_model.model.summary()\n",
    "\"\"\"\n",
    "def multiInputMultiOutputModelLstm():\n",
    "    \"\"\"\n",
    "        __________________________________________________________________________________________________\n",
    "        Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "        ==================================================================================================\n",
    "        speed_input (InputLayer)        (None, 5, 748)       0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        speed_lstm (LSTM)               (None, 100)          339600      speed_input[0][0]                \n",
    "        __________________________________________________________________________________________________\n",
    "        day_time_input (InputLayer)     (None, 2)            0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        Merge (Concatenate)             (None, 102)          0           speed_lstm[0][0]                 \n",
    "                                                                         day_time_input[0][0]             \n",
    "        __________________________________________________________________________________________________\n",
    "        dense_15 (Dense)                (None, 100)          10300       Merge[0][0]                      \n",
    "        __________________________________________________________________________________________________\n",
    "        Output (Dense)                  (None, 748)          75548       dense_15[0][0]                   \n",
    "        ==================================================================================================\n",
    "        Total params: 425,448\n",
    "        Trainable params: 425,448\n",
    "        Non-trainable params: 0\n",
    "        __________________________________________________________________________________________________\n",
    "\n",
    "    \"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:], name=\"speed_input\")\n",
    "    lstm = tf.keras.layers.LSTM(100,name=\"speed_lstm\")(main_input)\n",
    "    lstm_branch = tf.keras.layers.Dense(nSegments,name=\"lstm_branch\")(lstm)\n",
    "\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([lstm, daytime_input])\n",
    "    x = tf.keras.layers.Dense( 100, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    \n",
    "#     x=tf.keras.layers.Dense( nSegments, activation = tf.keras.activations.sigmoid )( x)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\")(x)\n",
    "    \n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer,lstm_branch] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse, optimizer=optimizer)\n",
    "    data_model.model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monoInputModelLstm():\n",
    "    \"\"\"\n",
    "        __________________________________________________________________________________________________\n",
    "        Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "        ==================================================================================================\n",
    "        speed_input (InputLayer)        (None, 5, 748)       0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        speed_lstm (LSTM)               (None, 100)          339600      speed_input[0][0]                \n",
    "        __________________________________________________________________________________________________\n",
    "        day_time_input (InputLayer)     (None, 2)            0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        Merge (Concatenate)             (None, 102)          0           speed_lstm[0][0]                 \n",
    "                                                                         day_time_input[0][0]             \n",
    "        __________________________________________________________________________________________________\n",
    "        dense_15 (Dense)                (None, 100)          10300       Merge[0][0]                      \n",
    "        __________________________________________________________________________________________________\n",
    "        Output (Dense)                  (None, 748)          75548       dense_15[0][0]                   \n",
    "        ==================================================================================================\n",
    "        Total params: 425,448\n",
    "        Trainable params: 425,448\n",
    "        Non-trainable params: 0\n",
    "        __________________________________________________________________________________________________\n",
    "\n",
    "    \"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    lstm=tf.keras.layers.LSTM(300,name=\"speed_lstm\")(main_input)\n",
    "    lstm_branch = tf.keras.layers.Dense(300,activation= tf.keras.activations.sigmoid,name=\"lstm_branch\")(lstm)\n",
    "    x=tf.keras.layers.Dense( 300, activation = tf.keras.activations.sigmoid )( lstm_branch)\n",
    "    x=tf.keras.layers.Dense( 300, activation = tf.keras.activations.sigmoid )(x)\n",
    "    x=tf.keras.layers.Dense( 300, activation = tf.keras.activations.sigmoid )(x)\n",
    "    output_layer = tf.keras.layers.Dense( nSegments,name=\"Output\",use_bias=False )(x)\n",
    "    \n",
    "    \n",
    "    data_model.model = tf.keras.Model(inputs = [main_input], outputs= [output_layer])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse, optimizer=optimizer)\n",
    "    data_model.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiInputModelLstmWithLastQuarter():\n",
    "    \"\"\"\n",
    "        __________________________________________________________________________________________________\n",
    "        Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "        ==================================================================================================\n",
    "        speed_input (InputLayer)        (None, 4, 748)       0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        speed_lstm (LSTM)               (None, 50)           159800      speed_input[0][0]                \n",
    "        __________________________________________________________________________________________________\n",
    "        last_quarter_speeds (Lambda)    (None, 748)          0           speed_input[0][0]                \n",
    "        __________________________________________________________________________________________________\n",
    "        lstm_branch (Dense)             (None, 100)          5100        speed_lstm[0][0]                 \n",
    "        __________________________________________________________________________________________________\n",
    "        MLP_branch (Dense)              (None, 100)          74900       last_quarter_speeds[0][0]        \n",
    "        __________________________________________________________________________________________________\n",
    "        day_time_input (InputLayer)     (None, 2)            0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        Merge (Concatenate)             (None, 202)          0           lstm_branch[0][0]                \n",
    "                                                                         MLP_branch[0][0]                 \n",
    "                                                                         day_time_input[0][0]             \n",
    "        __________________________________________________________________________________________________\n",
    "        dense (Dense)                   (None, 100)          20300       Merge[0][0]                      \n",
    "        __________________________________________________________________________________________________\n",
    "        Output (Dense)                  (None, 748)          75548       dense[0][0]                      \n",
    "        ==================================================================================================\n",
    "        Total params: 335,648\n",
    "        Trainable params: 335,648\n",
    "        Non-trainable params: 0\n",
    "        __________________________________________________________________________________________________\n",
    "    \n",
    "    \"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    lstm=tf.keras.layers.LSTM(200,name=\"speed_lstm\")(main_input)\n",
    "    lstm_branch = tf.keras.layers.Dense(200,activation= tf.keras.activations.sigmoid,name=\"lstm_branch\")(lstm)\n",
    "    lambda_slice_layer = tf.keras.layers.Lambda(lambda x : x[:,data_model.input_lag-1,:],name=\"last_quarter_speeds\")(main_input)\n",
    "    mlp_breanch = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"MLP_branch\")(lambda_slice_layer)\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([lstm_branch, mlp_breanch, daytime_input])\n",
    "    x=tf.keras.layers.Dense( 200, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\" )(x)\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse , optimizer=optimizer)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiInputModelLstmWithLastQuarter():\n",
    "    \"\"\"\n",
    "        __________________________________________________________________________________________________\n",
    "        Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "        ==================================================================================================\n",
    "        speed_input (InputLayer)        (None, 4, 748)       0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        speed_lstm (LSTM)               (None, 50)           159800      speed_input[0][0]                \n",
    "        __________________________________________________________________________________________________\n",
    "        last_quarter_speeds (Lambda)    (None, 748)          0           speed_input[0][0]                \n",
    "        __________________________________________________________________________________________________\n",
    "        lstm_branch (Dense)             (None, 100)          5100        speed_lstm[0][0]                 \n",
    "        __________________________________________________________________________________________________\n",
    "        MLP_branch (Dense)              (None, 100)          74900       last_quarter_speeds[0][0]        \n",
    "        __________________________________________________________________________________________________\n",
    "        day_time_input (InputLayer)     (None, 2)            0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        Merge (Concatenate)             (None, 202)          0           lstm_branch[0][0]                \n",
    "                                                                         MLP_branch[0][0]                 \n",
    "                                                                         day_time_input[0][0]             \n",
    "        __________________________________________________________________________________________________\n",
    "        dense (Dense)                   (None, 100)          20300       Merge[0][0]                      \n",
    "        __________________________________________________________________________________________________\n",
    "        Output (Dense)                  (None, 748)          75548       dense[0][0]                      \n",
    "        ==================================================================================================\n",
    "        Total params: 335,648\n",
    "        Trainable params: 335,648\n",
    "        Non-trainable params: 0\n",
    "        __________________________________________________________________________________________________\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    lstm=tf.keras.layers.LSTM(250,name=\"speed_lstm\")(main_input)\n",
    "    lstm_branch = tf.keras.layers.Dense(250,activation= tf.keras.activations.sigmoid,name=\"lstm_branch\")(lstm)\n",
    "    \n",
    "    lambda_slice_layer = tf.keras.layers.Lambda(lambda x : x[:,data_model.input_lag-1,:],name=\"last_quarter_speeds\")(main_input)\n",
    "    mlp_breanch = tf.keras.layers.Dense(250,activation= tf.keras.activations.sigmoid,name=\"MLP_branch\")(lambda_slice_layer)\n",
    "    \n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    \n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([lstm_branch, mlp_breanch, daytime_input])\n",
    "    \n",
    "    x=tf.keras.layers.Dense( 250, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    x=tf.keras.layers.Dense( 250, activation = tf.keras.activations.sigmoid )(x)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\", activation = tf.keras.activations.tanh  , use_bias=False)(x)\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mae , optimizer=optimizer)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiInputModelMLP():\n",
    "    \"\"\"\n",
    "    __________________________________________________________________________________________________\n",
    "    Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "    ==================================================================================================\n",
    "    speed_input (InputLayer)        (None, 4, 748)       0                                            \n",
    "    __________________________________________________________________________________________________\n",
    "    flatten_multi_speed (Flatten)   (None, 2992)         0           speed_input[0][0]                \n",
    "    __________________________________________________________________________________________________\n",
    "    full_speed_mpl (Dense)          (None, 100)          299300      flatten_multi_speed[0][0]        \n",
    "    __________________________________________________________________________________________________\n",
    "    last_quarter_speeds (Lambda)    (None, 748)          0           speed_input[0][0]                \n",
    "    __________________________________________________________________________________________________\n",
    "    full_mlp_branch (Dense)         (None, 100)          10100       full_speed_mpl[0][0]             \n",
    "    __________________________________________________________________________________________________\n",
    "    slice_MLP_branch (Dense)        (None, 100)          74900       last_quarter_speeds[0][0]        \n",
    "    __________________________________________________________________________________________________\n",
    "    day_time_input (InputLayer)     (None, 2)            0                                            \n",
    "    __________________________________________________________________________________________________\n",
    "    Merge (Concatenate)             (None, 202)          0           full_mlp_branch[0][0]            \n",
    "                                                                     slice_MLP_branch[0][0]           \n",
    "                                                                     day_time_input[0][0]             \n",
    "    __________________________________________________________________________________________________\n",
    "    dense (Dense)                   (None, 100)          20300       Merge[0][0]                      \n",
    "    __________________________________________________________________________________________________\n",
    "    Output (Dense)                  (None, 748)          75548       dense[0][0]                      \n",
    "    ==================================================================================================\n",
    "    Total params: 480,148\n",
    "    Trainable params: 480,148\n",
    "    Non-trainable params: 0\n",
    "    \"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    flatten_layer = tf.keras.layers.Flatten(name=\"flatten_multi_speed\")(main_input)\n",
    "    mlp=tf.keras.layers.Dense(100,name=\"full_speed_mpl\")(flatten_layer)\n",
    "    \n",
    "    mlp = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"full_mlp_branch\")(mlp)\n",
    "    \n",
    "    lambda_slice_layer = tf.keras.layers.Lambda(lambda x : x[:,data_model.input_lag-1,:],name=\"last_quarter_speeds\")(main_input)\n",
    "    mlp_breanch = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"slice_MLP_branch\")(lambda_slice_layer)\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([mlp, mlp_breanch, daytime_input])\n",
    "    x=tf.keras.layers.Dense( 100, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\" )(x)\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse , optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monoInputModelMLP():\n",
    "    \"\"\"\n",
    "    __________________________________________________________________________________________________\n",
    "    Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "    ==================================================================================================\n",
    "    speed_input (InputLayer)        (None, 4, 748)       0                                            \n",
    "    __________________________________________________________________________________________________\n",
    "    flatten_multi_speed (Flatten)   (None, 2992)         0           speed_input[0][0]                \n",
    "    __________________________________________________________________________________________________\n",
    "    full_speed_mpl (Dense)          (None, 100)          299300      flatten_multi_speed[0][0]        \n",
    "    __________________________________________________________________________________________________\n",
    "    last_quarter_speeds (Lambda)    (None, 748)          0           speed_input[0][0]                \n",
    "    __________________________________________________________________________________________________\n",
    "    full_mlp_branch (Dense)         (None, 100)          10100       full_speed_mpl[0][0]             \n",
    "    __________________________________________________________________________________________________\n",
    "    slice_MLP_branch (Dense)        (None, 100)          74900       last_quarter_speeds[0][0]        \n",
    "    __________________________________________________________________________________________________\n",
    "    day_time_input (InputLayer)     (None, 2)            0                                            \n",
    "    __________________________________________________________________________________________________\n",
    "    Merge (Concatenate)             (None, 202)          0           full_mlp_branch[0][0]            \n",
    "                                                                     slice_MLP_branch[0][0]           \n",
    "                                                                     day_time_input[0][0]             \n",
    "    __________________________________________________________________________________________________\n",
    "    dense (Dense)                   (None, 100)          20300       Merge[0][0]                      \n",
    "    __________________________________________________________________________________________________\n",
    "    Output (Dense)                  (None, 748)          75548       dense[0][0]                      \n",
    "    ==================================================================================================\n",
    "    Total params: 480,148\n",
    "    Trainable params: 480,148\n",
    "    Non-trainable params: 0\n",
    "    \"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    flatten_layer = tf.keras.layers.Flatten(name=\"flatten_multi_speed\")(main_input)\n",
    "    mlp = tf.keras.layers.Dense(350,name=\"full_speed_mpl\")(flatten_layer)\n",
    "    \n",
    "    mlp = tf.keras.layers.Dense(350,activation= tf.keras.activations.sigmoid,name=\"full_mlp_branch\")(mlp)\n",
    "    \n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    \n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([mlp, daytime_input])\n",
    "    \n",
    "    x=tf.keras.layers.Dense( 350, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    x=tf.keras.layers.Dropout(0.5)(x)\n",
    "    x=tf.keras.layers.Dense( 350, activation = tf.keras.activations.sigmoid )( x)\n",
    "    x=tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    x=tf.keras.layers.Dense( 350, activation = tf.keras.activations.sigmoid )( x)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\", activation = tf.keras.activations.tanh , use_bias=False)(x)\n",
    "    \n",
    "    \n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse , optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp():\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:], name=\"speed_input\")\n",
    "\n",
    "    flat = tf.keras.layers.Flatten()(main_input)\n",
    "    \n",
    "    dense=tf.keras.layers.Dense(nSegments,activation='sigmoid')(flat)\n",
    "\n",
    "    # densePReLU = tf.keras.layers.PReLU(alpha_initializer=\"zeros\")(dense)\n",
    "    \n",
    "    #     lstm=tf.keras.layers.Dense(250,activation= tf.keras.activations.sigmoid)(lstm)\n",
    "\n",
    "    dense = tf.keras.layers.Dropout(0.7)(dense)\n",
    "    dense=tf.keras.layers.Dense(nSegments,activation='sigmoid')(dense)\n",
    "#     densePReLU = tf.keras.layers.PReLU(alpha_initializer=\"zeros\")(dense)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\")(dense)\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input], outputs= [output_layer])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse, optimizer=optimizer)\n",
    "    data_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statefullLstm():\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],batch_size=batch_size,name=\"speed_input\")\n",
    "\n",
    "    lstm=tf.keras.layers.LSTM(nSegments,name=\"speed_lstm\",stateful=\"true\")(main_input)\n",
    "\n",
    "    lstm_branch = tf.keras.layers.Dense(nSegments,activation= tf.keras.activations.sigmoid,name=\"lstm_branch\")(lstm)\n",
    "\n",
    "\n",
    "    x=tf.keras.layers.Dense( nSegments, activation = tf.keras.activations.sigmoid )( lstm_branch)\n",
    "    # x=tf.keras.layers.Dense( 350, activation = tf.keras.activations.sigmoid )( x)\n",
    "\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\")(x)\n",
    "\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input], outputs= [output_layer])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse, optimizer=optimizer)\n",
    "    data_model.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    lstm=tf.keras.layers.LSTM(100,name=\"speed_lstm\")(main_input)\n",
    "    lstm_branch = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"lstm_branch\")(lstm)\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([lstm, daytime_input])\n",
    "    x=tf.keras.layers.Dense( 100, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\" )(x)\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse , optimizer=optimizer)   \n",
    "    data_model.model.summary()\n",
    "\"\"\"\n",
    "\n",
    "def multiInputModelLstm():\n",
    "    \"\"\"\n",
    "            __________________________________________________________________________________________________\n",
    "            Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "            ==================================================================================================\n",
    "            speed_input (InputLayer)        (None, 5, 748)       0                                            \n",
    "            __________________________________________________________________________________________________\n",
    "            speed_lstm (LSTM)               (None, 100)          339600      speed_input[0][0]                \n",
    "            __________________________________________________________________________________________________\n",
    "            day_time_input (InputLayer)     (None, 2)            0                                            \n",
    "            __________________________________________________________________________________________________\n",
    "            Merge (Concatenate)             (None, 102)          0           speed_lstm[0][0]                 \n",
    "                                                                             day_time_input[0][0]             \n",
    "            __________________________________________________________________________________________________\n",
    "            dense_15 (Dense)                (None, 100)          10300       Merge[0][0]                      \n",
    "            __________________________________________________________________________________________________\n",
    "            Output (Dense)                  (None, 748)          75548       dense_15[0][0]                   \n",
    "            ==================================================================================================\n",
    "            Total params: 425,448\n",
    "            Trainable params: 425,448\n",
    "            Non-trainable params: 0\n",
    "            __________________________________________________________________________________________________\n",
    "\n",
    "    \"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:], name=\"speed_input\")\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:], name=\"day_time_input\")\n",
    "\n",
    "    lstm=tf.keras.layers.LSTM(nSegments, name=\"speed_lstm\")(main_input)\n",
    "    \n",
    "    lstm_branch = tf.keras.layers.Dense(nSegments, activation= tf.keras.activations.relu, name=\"lstm_branch\")(lstm)\n",
    "    \n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([lstm_branch, daytime_input])\n",
    "    \n",
    "    x=tf.keras.layers.Dense( nSegments, activation = tf.keras.activations.relu )( merge_layer)\n",
    "\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(nSegments, name=\"Output\")(x)\n",
    "    \n",
    "    \n",
    "    data_model.model = tf.keras.Model(inputs = [main_input, daytime_input], outputs= [output_layer])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse, optimizer=optimizer, metrics=['mse'])\n",
    "    data_model.model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization objective for Lasso is:\n",
    "\n",
    "$(1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21$\n",
    "Where:\n",
    "\n",
    "\n",
    "\n",
    "$||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcrnnModel(n_units=300):\n",
    "    main_input = tf.keras.layers.Input( x_train.shape[1:], name=\"speed_input\",batch_size=batch_size)\n",
    "    \n",
    "    layer = tf.keras.layers.RNN(dcrnn_cell.DCGRUCell(1,adjacency_matrix.values,3,nSegments,reuse=True))(main_input)\n",
    "    layer = tf.keras.layers.Dense(n_units,activation=\"sigmoid\")(layer)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\")(layer)\n",
    "\n",
    "    model =tf.keras.Model(inputs = [main_input], outputs= [output_layer])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss=smape_loss, optimizer=optimizer,metrics=[\"mse\"])\n",
    "    data_model.model =model\n",
    "    model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    lstm=tf.keras.layers.LSTM(100,name=\"speed_lstm\")(main_input)\n",
    "    lstm_branch = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"lstm_branch\")(lstm)\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([lstm, daytime_input])\n",
    "    x=tf.keras.layers.Dense( 100, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\" )(x)\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse , optimizer=optimizer)   \n",
    "    data_model.model.summary()\n",
    "\"\"\"\n",
    "\n",
    "def bnActivationDropout(x,units,act,drp,reg_rate,bn=False):\n",
    "    x=tf.keras.layers.Dense( units,kernel_regularizer=tf.keras.regularizers.l1(reg_rate))(x)\n",
    "    if bn:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x= tf.keras.layers.Activation(act)(x)\n",
    "    x =tf.keras.layers.Dropout(drp)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def simpleMonoInputModelLstm():\n",
    "    \"\"\"\n",
    "        __________________________________________________________________________________________________\n",
    "        Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "        ==================================================================================================\n",
    "        speed_input (InputLayer)        (None, 5, 748)       0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        speed_lstm (LSTM)               (None, 100)          339600      speed_input[0][0]                \n",
    "        __________________________________________________________________________________________________\n",
    "        day_time_input (InputLayer)     (None, 2)            0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        Merge (Concatenate)             (None, 102)          0           speed_lstm[0][0]                 \n",
    "                                                                         day_time_input[0][0]             \n",
    "        __________________________________________________________________________________________________\n",
    "        dense_15 (Dense)                (None, 100)          10300       Merge[0][0]                      \n",
    "        __________________________________________________________________________________________________\n",
    "        Output (Dense)                  (None, 748)          75548       dense_15[0][0]                   \n",
    "        ==================================================================================================\n",
    "        Total params: 425,448\n",
    "        Trainable params: 425,448\n",
    "        Non-trainable params: 0\n",
    "        __________________________________________________________________________________________________\n",
    "\n",
    "    \"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    \n",
    "    lstm=tf.keras.layers.LSTM(nSegments,name=\"speed_lstm\")(main_input)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\",kernel_regularizer=tf.keras.regularizers.l1(0.0001))(lstm)\n",
    "    \n",
    "    \n",
    "    data_model.model = tf.keras.Model(inputs = [main_input], outputs= [output_layer])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    data_model.model.compile(loss=smape_loss, optimizer=optimizer,metrics=['mse',coeff_determination])\n",
    "\n",
    "    data_model.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countInputModelLstm():\n",
    "    main_input = tf.keras.layers.Input( x_train.shape[1:], name=\"speed_input\")\n",
    "    count_input = tf.keras.layers.Input( count_train.shape[1:], name=\"count_input\")\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "\n",
    "    \n",
    "    count_lstm=tf.keras.layers.LSTM( 100, name=\"count_lstm\")( count_input )\n",
    "    \n",
    "    count_lstm=bnActivationDropout(count_lstm,100,\"tanh\",0.2)\n",
    "\n",
    "    lstm=tf.keras.layers.LSTM(100, name=\"speed_lstm\")( main_input)\n",
    "\n",
    "    lstm=bnActivationDropout(lstm,100,\"tanh\",0.2)\n",
    "\n",
    "    \n",
    "    count_speed_merge = tf.keras.layers.Concatenate(1,name=\"count_speed_merge\")([count_lstm, lstm])\n",
    "\n",
    "    \n",
    "    x = bnActivationDropout(count_speed_merge,100,\"tanh\",0.2)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(1,name=\"time_input_concat\")([daytime_input,x])\n",
    "\n",
    "    x = bnActivationDropout(x,100,\"tanh\",0)\n",
    "\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(nSegments*output_lag,name=\"Output\")(x)\n",
    "    \n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,count_input,daytime_input], outputs= [output_layer])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=smape_loss, optimizer=optimizer,metrics=[\"mse\"])\n",
    "    data_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = tf.keras.backend\n",
    "\n",
    "# def rmse (y_true, y_pred):\n",
    "#     return K.sqrt(K.mean(K.square(y_pred -y_true)))\n",
    "\n",
    "# def smape_loss(true, predicted):\n",
    "#     \"\"\"\n",
    "#     Differentiable SMAPE loss\n",
    "#     :param true: Truth values\n",
    "#     :param predicted: Predicted values\n",
    "#     :param weights: Weights mask to exclude some values\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     epsilon = 0.1 # Smoothing factor, helps SMAPE to be well-behaved near zero\n",
    "# #     true_o = tf.expm1(true)\n",
    "# #     pred_o = tf.expm1(predicted)\n",
    "#     true_o = true\n",
    "#     pred_o = predicted\n",
    "#     summ  = tf.maximum(tf.abs(true_o) + tf.abs(pred_o) + epsilon, 0.5+ epsilon)\n",
    "#     smape = tf.abs(pred_o - true_o) / summ * 2.0\n",
    "#     return smape\n",
    "\n",
    "# # def smape_loss(y_true, y_pred):\n",
    "\n",
    "# #     summ = tf.abs(y_true) + tf.abs(y_pred)\n",
    "# #     smape = tf.abs(y_pred - y_true) / summ * 2.0\n",
    "# #     return smape\n",
    "\n",
    "\n",
    "\n",
    "# def coeff_determination(y_true, y_pred):\n",
    "#     SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    \n",
    "#     SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "#     return -( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "# nSegments = len(speedDF)\n",
    "# input_lag, output_lag, sequence_length =9, 1, 60#speedDF.columns.size\n",
    "# valid_split = 0.7\n",
    "# model_name =\"LSTM timed shift mean max scale\"\n",
    "\n",
    "# params        = {\"scale_max\":True, \"scale_output\":True}\n",
    "# data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "# data_model_05 = models.DataModel( speedDF_05, input_lag, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "# data_model_10 = models.DataModel( speedDF_10, input_lag, output_lag, sequence_length, valid_split = valid_split, **params )                                                                  \n",
    "\n",
    "# data_model.preprocessData()\n",
    "# data_model_05.preprocessData()\n",
    "# data_model_10.preprocessData()\n",
    "\n",
    "# x_train_00, y_train_00, x_test_00, y_test_00 = data_model.trainSplit()\n",
    "# x_train_05, y_train_05, x_test_05, y_test_05 = data_model.trainSplit()\n",
    "# x_train_10, y_train_10, x_test_10, y_test_10 = data_model.trainSplit()\n",
    "\n",
    "# x_train = np.concatenate([x_train_00,x_train_05,x_train_10])\n",
    "# y_train = np.concatenate([y_train_00,y_train_05,y_train_10])\n",
    "\n",
    "# count_data = models.DataModel( countDF, input_lag, output_lag, sequence_length, valid_split = valid_split)\n",
    "# count_train_00, _, count_test_00, _ = count_data.trainSplit()\n",
    "\n",
    "\n",
    "# count_data_05 = models.DataModel( countDF_05, input_lag, output_lag, sequence_length, valid_split = valid_split)\n",
    "# count_train_05, _, count_test_05, _ = count_data.trainSplit()\n",
    "\n",
    "# count_data_10 = models.DataModel( countDF_10, input_lag, output_lag, sequence_length, valid_split = valid_split)\n",
    "# count_train_10, _, count_test_10, _ = count_data.trainSplit()\n",
    "\n",
    "# count_train =  np.concatenate([ count_train_00,\n",
    "#                                 count_train_05,\n",
    "#                                 count_train_10\n",
    "#                               ]\n",
    "#                              )\n",
    "\n",
    "\n",
    "# data_model.count_data = count_data\n",
    "\n",
    "# data_model.time_data=  data_model.getDaysTypes()\n",
    "\n",
    "# train_days_00, test_days_00 =  data_model.getDaysTypes()\n",
    "# train_days_05, _ = data_model_05.getDaysTypes()\n",
    "# train_days_10, _ = data_model_10.getDaysTypes()\n",
    "\n",
    "# train_days= np.concatenate([train_days_00,train_days_05,train_days_10])\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# config = tf.ConfigProto(intra_op_parallelism_threads=10, \n",
    "#                         inter_op_parallelism_threads=10,\n",
    "#                         allow_soft_placement=True, \n",
    "#                         log_device_placement=True\n",
    "#                        )\n",
    "# session = tf.Session(config=config,graph=tf.get_default_graph())\n",
    "\n",
    "# tf.keras.backend.set_session(session)\n",
    "\n",
    "# B1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiMlps(diffusionDF):\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    \n",
    "#     layers= []\n",
    "#     for column in diffusionDF.columns:\n",
    "#         col_values = (diffusionDF.loc[column]>0).reset_index( drop = True )\n",
    "#         col_values = col_values[col_values==True].index\n",
    "#         layers.append(tf.keras.layers.Dense(1)(tf.keras.layers.Lambda(lambda x : tf.gather(x,col_values.values,axis=2),name=\"Seg_\"+str(column))(main_input)))\n",
    "    \n",
    "#     output_layer = tf.keras.layers.Concatenate()(layers)\n",
    "\n",
    "    output_layer = CustomConnected(nSegments, diffusionDF.values,name=\"Output\")(main_input)\n",
    "    \n",
    "    \n",
    "    data_model.model = tf.keras.Model(inputs = [main_input], outputs= [output_layer])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    data_model.model.compile(loss=\"mse\", optimizer=optimizer,metrics=['mse'])\n",
    "\n",
    "    data_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConnected(tf.keras.layers.Dense):\n",
    "\n",
    "    def __init__(self,units,connections,**kwargs):\n",
    "\n",
    "        #this is matrix A\n",
    "        self.connections = tf.constant(connections,dtype = tf.float32,name=\"diffusion_const\")                        \n",
    "        print(self.connections)\n",
    "        #initalize the original Dense with all the usual arguments   \n",
    "        super(CustomConnected,self).__init__(units,**kwargs)  \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        output = K.dot(inputs, self.kernel * self.connections)\n",
    "        if self.use_bias:\n",
    "            output = K.bias_add(output, self.bias)\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "#     def call(self,inputs):\n",
    "#         #change the kernel before calling the original call:\n",
    "#         self.kernel = self.kernel * self.connections        \n",
    "#         #call the original calculations:\n",
    "#         return super(CustomConnected,self).call(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B1(n_units=300,reg_rate=0.005):\n",
    "    \"[['flatten', 'dense'], [300, 300], ['flatten', 'dense'], [300, 300], ['dense', 'dense'], [300, 300]]_20\"\n",
    "    main_input = tf.keras.layers.Input( x_train.shape[1:], name=\"speed_input\")\n",
    "    count_input = tf.keras.layers.Input( count_train.shape[1:], name=\"count_input\")\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    \n",
    "    speed_layer = tf.keras.layers.Flatten()(main_input)\n",
    "    speed_layer = tf.keras.layers.Dense(nSegments,kernel_regularizer=tf.keras.regularizers.l1(reg_rate),activation=\"sigmoid\")(speed_layer)\n",
    "    \n",
    "    count_layer = tf.keras.layers.Flatten()(count_input)\n",
    "    count_layer = tf.keras.layers.Dense(nSegments,kernel_regularizer=tf.keras.regularizers.l1(reg_rate),activation=\"sigmoid\")(count_layer)\n",
    "    \n",
    "    layer = tf.keras.layers.Concatenate(1,name=\"count_speed_merge\")([speed_layer, count_layer,daytime_input])\n",
    "    \n",
    "    layer = tf.keras.layers.Dense(n_units,kernel_regularizer=tf.keras.regularizers.l1(reg_rate),activation=\"sigmoid\")(layer)\n",
    "    layer = tf.keras.layers.Dense(n_units,kernel_regularizer=tf.keras.regularizers.l1(reg_rate),activation=\"sigmoid\")(layer)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,kernel_regularizer=tf.keras.regularizers.l1(reg_rate),name=\"Output\")(layer)\n",
    "#     output_layer = tf.keras.layers.PReLU(alpha_initializer=\"ones\",alpha_regularizer=tf.keras.regularizers.l1(0.001))(output_layer)\n",
    "\n",
    "    model =tf.keras.Model(inputs = [main_input,count_input,daytime_input], outputs= [output_layer])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer,metrics=[\"mse\"])\n",
    "    data_model.model =model\n",
    "    mlpModel=data_model\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateDepthWiseLstmPerDay(hist_layer,day_length,reg_rate):\n",
    "\n",
    "    n_convs = hist_layer.shape[1]//day_length\n",
    "    conv_layers=[]\n",
    "    for i in range(n_convs) :\n",
    "        conv_layers.append(tf.keras.layers.Lambda(lambda x : x[:,i*day_length:(i+1)*day_length,:],name=\"historical_conv_\"+str(i))(hist_layer))\n",
    "        conv_layers[i]= tf.keras.layers.LSTM(200, name=\"LSTM_\"+str(i),kernel_regularizer=tf.keras.regularizers.l1(reg_rate) )(conv_layers[i])\n",
    "        conv_layers[i] = tf.keras.layers.Reshape((1,conv_layers[i].shape[1]))(conv_layers[i])\n",
    "\n",
    "    conv_layer = tf.keras.layers.Concatenate(1,name=\"hist_lstm_layers_concat\")(conv_layers)\n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "def separateDepthWiseConvPerDay(hist_layer,day_length,reg_rate):\n",
    "    hist_layer = tf.keras.layers.Reshape((hist_layer.shape[1],1,hist_layer.shape[2]))(hist_layer)\n",
    "\n",
    "    n_convs = hist_layer.shape[1]//day_length\n",
    "    conv_layers=[]\n",
    "    for i in range(n_convs) :\n",
    "        conv_layers.append(tf.keras.layers.Lambda(lambda x : x[:,i*day_length:(i+1)*day_length,:],name=\"historical_conv_\"+str(i))(hist_layer))\n",
    "        conv_layers[i] = tf.keras.layers.DepthwiseConv2D((day_length,1),depth_multiplier = 1,name=\"conv1D_\"+str(i),activation=\"tanh\",depthwise_regularizer=tf.keras.regularizers.l1(reg_rate))(conv_layers[i])\n",
    "        conv_layers[i] = tf.keras.layers.Reshape((conv_layers[i].shape[1],conv_layers[i].shape[3]))(conv_layers[i])\n",
    "    \n",
    "    conv_layer = tf.keras.layers.Concatenate(1,name=\"conv_layers_concat\")(conv_layers)\n",
    "    return conv_layer\n",
    "\n",
    "def convMultiInputModelLstm(day_length,reg_rate):\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    \n",
    "    historical_speeds_layer = tf.keras.layers.Lambda(lambda x : x[:,:-(data_model.input_lag%day_length),:],name=\"historical_speeds\")(main_input)\n",
    "    \n",
    "    historical_speeds_layer = separateDepthWiseConvPerDay(historical_speeds_layer,day_length,reg_rate)\n",
    "    historical_speeds_layer = tf.keras.layers.LSTM(200, name=\"speed_hist_lstm\",kernel_regularizer=tf.keras.regularizers.l1(reg_rate))(historical_speeds_layer)\n",
    "    \n",
    "#     historical_speeds_layer = bnActivationDropout( historical_speeds_layer,200,\"tanh\",0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    local_speeds_layer = tf.keras.layers.Lambda(lambda x : x[:, -(data_model.input_lag%day_length):,:],name=\"local_speeds\")(main_input)\n",
    "    local_speeds_layer = tf.keras.layers.LSTM(200, name=\"speed_local_lstm\",kernel_regularizer=tf.keras.regularizers.l1(reg_rate))(local_speeds_layer)\n",
    "\n",
    "#     local_speeds_layer = bnActivationDropout(local_speeds_layer, 200, \"tanh\", 0)\n",
    "\n",
    "    speed_merge = tf.keras.layers.Concatenate(1, name=\"speed_merge\")([local_speeds_layer, historical_speeds_layer])\n",
    "\n",
    "    \n",
    "    x = bnActivationDropout(speed_merge, 200, \"tanh\",0.25,reg_rate)\n",
    "    x = bnActivationDropout(x, 200, \"tanh\", 0,reg_rate)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\")(x)\n",
    "\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input], outputs= [output_layer])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(0.0012)\n",
    "    \n",
    "    data_model.model.compile(loss=smape_loss, optimizer=optimizer, metrics=['mse',smape_loss])\n",
    "\n",
    "    data_model.model.summary()\n",
    "\n",
    "    \n",
    "\n",
    "def convMultiInputModelMLP():\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    \n",
    "    historical_speeds_layer = tf.keras.layers.Lambda(lambda x : x[:,:-(data_model.input_lag%20),:],name=\"historical_speeds\")(main_input)\n",
    "    \n",
    "    historical_speeds_layer = separateDepthWiseConvPerDay(historical_speeds_layer)\n",
    "    historical_speeds_layer =tf.keras.layers.Flatten()(historical_speeds_layer)\n",
    "    historical_speeds_layer = bnActivationDropout( historical_speeds_layer,300,\"tanh\",0.2 )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    local_speeds_layer = tf.keras.layers.Lambda(lambda x : x[:, -(data_model.input_lag%20):, :],name=\"local_speeds\")(main_input)\n",
    "    local_speeds_layer =tf.keras.layers.Flatten()(local_speeds_layer)\n",
    "\n",
    "\n",
    "    local_speeds_layer = bnActivationDropout(local_speeds_layer, 300, \"tanh\", 0.2)\n",
    "\n",
    "    speed_merge = tf.keras.layers.Concatenate(1, name=\"speed_merge\")([local_speeds_layer, historical_speeds_layer])\n",
    "\n",
    "    \n",
    "    x = bnActivationDropout(speed_merge, 300, \"tanh\",0.2)\n",
    "    x = bnActivationDropout(x, 300, \"tanh\", 0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\")(x)\n",
    "\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input], outputs= [output_layer])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    data_model.model.compile(loss=\"mse\", optimizer=optimizer, metrics=['mse',smape_loss])\n",
    "\n",
    "    data_model.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "speed_input (InputLayer)        (None, 63, 536)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "historical_speeds (Lambda)      (None, 60, 536)      0           speed_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 60, 1, 536)   0           historical_speeds[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "historical_conv_0 (Lambda)      (None, 20, 1, 536)   0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "historical_conv_1 (Lambda)      (None, 20, 1, 536)   0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "historical_conv_2 (Lambda)      (None, 20, 1, 536)   0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1D_0 (DepthwiseConv2D)      (None, 1, 1, 536)    11256       historical_conv_0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1D_1 (DepthwiseConv2D)      (None, 1, 1, 536)    11256       historical_conv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1D_2 (DepthwiseConv2D)      (None, 1, 1, 536)    11256       historical_conv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 536)       0           conv1D_0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 536)       0           conv1D_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1, 536)       0           conv1D_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "local_speeds (Lambda)           (None, 3, 536)       0           speed_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_layers_concat (Concatenate (None, 3, 536)       0           reshape_1[0][0]                  \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "speed_local_lstm (LSTM)         (None, 200)          589600      local_speeds[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "speed_hist_lstm (LSTM)          (None, 200)          589600      conv_layers_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "speed_merge (Concatenate)       (None, 400)          0           speed_local_lstm[0][0]           \n",
      "                                                                 speed_hist_lstm[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 200)          80200       speed_merge[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 200)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 200)          0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          40200       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 200)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, 536)          107736      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,441,104\n",
      "Trainable params: 1,441,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K = tf.keras.backend\n",
    "\n",
    "def rmse (y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred -y_true)))\n",
    "\n",
    "def smape_loss(true, predicted):\n",
    "    \"\"\"\n",
    "    Differentiable SMAPE loss\n",
    "    :param true: Truth values\n",
    "    :param predicted: Predicted values\n",
    "    :param weights: Weights mask to exclude some values\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    epsilon = 0.1 # Smoothing factor, helps SMAPE to be well-behaved near zero\n",
    "#     true_o = tf.expm1(true)\n",
    "#     pred_o = tf.expm1(predicted)\n",
    "    true_o = true\n",
    "    pred_o = predicted\n",
    "    summ  = tf.maximum(tf.abs(true_o) + tf.abs(pred_o) + epsilon, 0.5+ epsilon)\n",
    "    smape = tf.abs(pred_o - true_o) / summ * 2.0\n",
    "    return smape\n",
    "\n",
    "\n",
    "\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return -( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "nSegments = len(speedDF)\n",
    "input_lag, output_lag, sequence_length =63, 1, speedDF.columns.size\n",
    "valid_split = 0.7\n",
    "model_name =\"LSTM timed shift mean max scale\"\n",
    "\n",
    "params        = {\"scale_max\":True, \"scale_output\":True}\n",
    "data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params )                                                  \n",
    "\n",
    "data_model.preprocessData()\n",
    "\n",
    "x_train_00, y_train_00, x_test_00, y_test_00 = data_model.trainSplit()\n",
    "\n",
    "x_train = np.concatenate([x_train_00])\n",
    "y_train = np.concatenate([y_train_00])\n",
    "\n",
    "count_data = models.DataModel( countDF, input_lag, output_lag, sequence_length, valid_split = valid_split)\n",
    "count_train_00, _, count_test_00, _ = count_data.trainSplit()\n",
    "\n",
    "\n",
    "count_train =  np.concatenate([ count_train_00,\n",
    "                              ]\n",
    "                             )\n",
    "\n",
    "\n",
    "data_model.count_data = count_data\n",
    "\n",
    "data_model.time_data  =  data_model.getDaysTypes()\n",
    "\n",
    "train_days_00, test_days_00 =  data_model.getDaysTypes()\n",
    "train_days= np.concatenate([train_days_00])\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "convMultiInputModelLstm(20,reg_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.728550100764077 7.907591243897151\n"
     ]
    }
   ],
   "source": [
    "print(data_model.mae(data_model.predict('train', y_step=0),y_train_00,y_step=0), data_model.mae(data_model.predict('test',y_step=0),y_test_00,y_step=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 865 samples, validate on 372 samples\n",
      "Epoch 1/400\n",
      "865/865 [==============================] - 2s 2ms/sample - loss: 1.3584 - mean_squared_error: 0.0073 - smape_loss: 0.1870 - val_loss: 0.7874 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1845\n",
      "Epoch 2/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.5025 - mean_squared_error: 0.0068 - smape_loss: 0.1798 - val_loss: 0.3141 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1836\n",
      "Epoch 3/400\n",
      "865/865 [==============================] - 1s 901us/sample - loss: 0.2585 - mean_squared_error: 0.0068 - smape_loss: 0.1798 - val_loss: 0.2222 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1839\n",
      "Epoch 4/400\n",
      "865/865 [==============================] - 1s 940us/sample - loss: 0.2050 - mean_squared_error: 0.0068 - smape_loss: 0.1800 - val_loss: 0.1999 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1844\n",
      "Epoch 5/400\n",
      "865/865 [==============================] - 1s 924us/sample - loss: 0.1935 - mean_squared_error: 0.0068 - smape_loss: 0.1799 - val_loss: 0.1957 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1839\n",
      "Epoch 6/400\n",
      "865/865 [==============================] - 1s 950us/sample - loss: 0.1909 - mean_squared_error: 0.0068 - smape_loss: 0.1796 - val_loss: 0.1947 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1838\n",
      "Epoch 7/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1900 - mean_squared_error: 0.0068 - smape_loss: 0.1793 - val_loss: 0.1940 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1837\n",
      "Epoch 8/400\n",
      "865/865 [==============================] - 1s 994us/sample - loss: 0.1897 - mean_squared_error: 0.0068 - smape_loss: 0.1795 - val_loss: 0.1938 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1837\n",
      "Epoch 9/400\n",
      "865/865 [==============================] - 1s 868us/sample - loss: 0.1896 - mean_squared_error: 0.0068 - smape_loss: 0.1794 - val_loss: 0.1940 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1838\n",
      "Epoch 10/400\n",
      "865/865 [==============================] - 1s 920us/sample - loss: 0.1895 - mean_squared_error: 0.0068 - smape_loss: 0.1792 - val_loss: 0.1934 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1835\n",
      "Epoch 11/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1885 - mean_squared_error: 0.0067 - smape_loss: 0.1788 - val_loss: 0.1931 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1835\n",
      "Epoch 12/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1886 - mean_squared_error: 0.0067 - smape_loss: 0.1789 - val_loss: 0.1937 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1839\n",
      "Epoch 13/400\n",
      "865/865 [==============================] - 1s 979us/sample - loss: 0.1890 - mean_squared_error: 0.0067 - smape_loss: 0.1790 - val_loss: 0.1935 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1837\n",
      "Epoch 14/400\n",
      "865/865 [==============================] - 1s 989us/sample - loss: 0.1881 - mean_squared_error: 0.0067 - smape_loss: 0.1785 - val_loss: 0.1931 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1836\n",
      "Epoch 15/400\n",
      "865/865 [==============================] - 1s 998us/sample - loss: 0.1880 - mean_squared_error: 0.0067 - smape_loss: 0.1785 - val_loss: 0.1930 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1834\n",
      "Epoch 16/400\n",
      "865/865 [==============================] - 1s 991us/sample - loss: 0.1885 - mean_squared_error: 0.0067 - smape_loss: 0.1788 - val_loss: 0.1937 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1837\n",
      "Epoch 17/400\n",
      "865/865 [==============================] - 1s 877us/sample - loss: 0.1892 - mean_squared_error: 0.0067 - smape_loss: 0.1789 - val_loss: 0.1935 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1837\n",
      "Epoch 18/400\n",
      "865/865 [==============================] - 1s 923us/sample - loss: 0.1882 - mean_squared_error: 0.0067 - smape_loss: 0.1786 - val_loss: 0.1930 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1835\n",
      "Epoch 19/400\n",
      "865/865 [==============================] - 1s 873us/sample - loss: 0.1878 - mean_squared_error: 0.0067 - smape_loss: 0.1784 - val_loss: 0.1931 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1836\n",
      "Epoch 20/400\n",
      "865/865 [==============================] - 1s 877us/sample - loss: 0.1881 - mean_squared_error: 0.0067 - smape_loss: 0.1785 - val_loss: 0.1927 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1832\n",
      "Epoch 21/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1878 - mean_squared_error: 0.0067 - smape_loss: 0.1784 - val_loss: 0.1929 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1834\n",
      "Epoch 22/400\n",
      "865/865 [==============================] - 1s 918us/sample - loss: 0.1879 - mean_squared_error: 0.0067 - smape_loss: 0.1783 - val_loss: 0.1926 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1833\n",
      "Epoch 23/400\n",
      "865/865 [==============================] - 1s 887us/sample - loss: 0.1875 - mean_squared_error: 0.0067 - smape_loss: 0.1782 - val_loss: 0.1924 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1831\n",
      "Epoch 24/400\n",
      "865/865 [==============================] - 1s 957us/sample - loss: 0.1876 - mean_squared_error: 0.0067 - smape_loss: 0.1782 - val_loss: 0.1927 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1834\n",
      "Epoch 25/400\n",
      "865/865 [==============================] - 1s 928us/sample - loss: 0.1876 - mean_squared_error: 0.0067 - smape_loss: 0.1783 - val_loss: 0.1927 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1833\n",
      "Epoch 26/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1879 - mean_squared_error: 0.0067 - smape_loss: 0.1783 - val_loss: 0.1924 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1831\n",
      "Epoch 27/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1879 - mean_squared_error: 0.0067 - smape_loss: 0.1785 - val_loss: 0.1936 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1841\n",
      "Epoch 28/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1877 - mean_squared_error: 0.0067 - smape_loss: 0.1782 - val_loss: 0.1922 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 29/400\n",
      "865/865 [==============================] - 1s 2ms/sample - loss: 0.1875 - mean_squared_error: 0.0067 - smape_loss: 0.1782 - val_loss: 0.1923 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 30/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1873 - mean_squared_error: 0.0067 - smape_loss: 0.1781 - val_loss: 0.1934 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1833\n",
      "Epoch 31/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1911 - mean_squared_error: 0.0068 - smape_loss: 0.1794 - val_loss: 0.1936 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1832\n",
      "Epoch 32/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1886 - mean_squared_error: 0.0067 - smape_loss: 0.1786 - val_loss: 0.1930 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1836\n",
      "Epoch 33/400\n",
      "864/865 [============================>.] - ETA: 0s - loss: 0.1873 - mean_squared_error: 0.0067 - smape_loss: 0.1782\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "865/865 [==============================] - 1s 2ms/sample - loss: 0.1873 - mean_squared_error: 0.0067 - smape_loss: 0.1782 - val_loss: 0.1924 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1833\n",
      "Epoch 34/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1845 - mean_squared_error: 0.0067 - smape_loss: 0.1781 - val_loss: 0.1887 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1832\n",
      "Epoch 35/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1833 - mean_squared_error: 0.0067 - smape_loss: 0.1779 - val_loss: 0.1883 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 36/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1832 - mean_squared_error: 0.0067 - smape_loss: 0.1779 - val_loss: 0.1883 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 37/400\n",
      "865/865 [==============================] - 1s 2ms/sample - loss: 0.1828 - mean_squared_error: 0.0066 - smape_loss: 0.1776 - val_loss: 0.1883 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1832\n",
      "Epoch 38/400\n",
      "865/865 [==============================] - 1s 2ms/sample - loss: 0.1833 - mean_squared_error: 0.0067 - smape_loss: 0.1779 - val_loss: 0.1881 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 39/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1829 - mean_squared_error: 0.0067 - smape_loss: 0.1777 - val_loss: 0.1889 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1834\n",
      "Epoch 40/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1842 - mean_squared_error: 0.0067 - smape_loss: 0.1781 - val_loss: 0.1887 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1832\n",
      "Epoch 41/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1831 - mean_squared_error: 0.0067 - smape_loss: 0.1778 - val_loss: 0.1885 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1834\n",
      "Epoch 42/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1827 - mean_squared_error: 0.0066 - smape_loss: 0.1776 - val_loss: 0.1879 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1829\n",
      "Epoch 43/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1827 - mean_squared_error: 0.0066 - smape_loss: 0.1777 - val_loss: 0.1880 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 44/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1826 - mean_squared_error: 0.0066 - smape_loss: 0.1776 - val_loss: 0.1879 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1831\n",
      "Epoch 45/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1826 - mean_squared_error: 0.0066 - smape_loss: 0.1776 - val_loss: 0.1880 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 46/400\n",
      "865/865 [==============================] - 1s 2ms/sample - loss: 0.1827 - mean_squared_error: 0.0067 - smape_loss: 0.1777 - val_loss: 0.1886 - val_mean_squared_error: 0.0070 - val_smape_loss: 0.1832\n",
      "Epoch 47/400\n",
      "864/865 [============================>.] - ETA: 0s - loss: 0.1840 - mean_squared_error: 0.0067 - smape_loss: 0.1781\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1840 - mean_squared_error: 0.0067 - smape_loss: 0.1781 - val_loss: 0.1880 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1829\n",
      "Epoch 48/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1815 - mean_squared_error: 0.0066 - smape_loss: 0.1776 - val_loss: 0.1861 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1828\n",
      "Epoch 49/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1809 - mean_squared_error: 0.0066 - smape_loss: 0.1776 - val_loss: 0.1862 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1829\n",
      "Epoch 50/400\n",
      "865/865 [==============================] - 1s 2ms/sample - loss: 0.1808 - mean_squared_error: 0.0066 - smape_loss: 0.1775 - val_loss: 0.1861 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1829\n",
      "Epoch 51/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1808 - mean_squared_error: 0.0066 - smape_loss: 0.1775 - val_loss: 0.1860 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1829\n",
      "Epoch 52/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1807 - mean_squared_error: 0.0066 - smape_loss: 0.1775 - val_loss: 0.1860 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1829\n",
      "Epoch 53/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1806 - mean_squared_error: 0.0066 - smape_loss: 0.1775 - val_loss: 0.1861 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 54/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1806 - mean_squared_error: 0.0066 - smape_loss: 0.1774 - val_loss: 0.1861 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 55/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1808 - mean_squared_error: 0.0067 - smape_loss: 0.1775 - val_loss: 0.1862 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 56/400\n",
      "864/865 [============================>.] - ETA: 0s - loss: 0.1804 - mean_squared_error: 0.0066 - smape_loss: 0.1773\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1804 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1860 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 57/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1798 - mean_squared_error: 0.0066 - smape_loss: 0.1774 - val_loss: 0.1852 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 58/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1796 - mean_squared_error: 0.0066 - smape_loss: 0.1774 - val_loss: 0.1852 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 59/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1795 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1852 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 60/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1794 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1851 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 61/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1794 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1852 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 62/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1796 - mean_squared_error: 0.0066 - smape_loss: 0.1774 - val_loss: 0.1853 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 63/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1798 - mean_squared_error: 0.0066 - smape_loss: 0.1775 - val_loss: 0.1852 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 64/400\n",
      "865/865 [==============================] - 1s 984us/sample - loss: 0.1795 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1852 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 65/400\n",
      "832/865 [===========================>..] - ETA: 0s - loss: 0.1790 - mean_squared_error: 0.0066 - smape_loss: 0.1768\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1794 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1851 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 66/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1848 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 67/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1849 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1831\n",
      "Epoch 68/400\n",
      "865/865 [==============================] - 1s 999us/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1848 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 69/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1848 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 70/400\n",
      "865/865 [==============================] - 1s 997us/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1848 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 71/400\n",
      "865/865 [==============================] - 1s 991us/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1848 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 72/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1792 - mean_squared_error: 0.0066 - smape_loss: 0.1774 - val_loss: 0.1849 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1831\n",
      "Epoch 73/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1848 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 74/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1774 - val_loss: 0.1849 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1831\n",
      "Epoch 75/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1848 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 76/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1848 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 77/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1848 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 78/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1790 - mean_squared_error: 0.0066 - smape_loss: 0.1772 - val_loss: 0.1848 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 79/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1789 - mean_squared_error: 0.0066 - smape_loss: 0.1772 - val_loss: 0.1849 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1832\n",
      "Epoch 80/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1848 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "Epoch 81/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1849 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1832\n",
      "Epoch 82/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1849 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1831\n",
      "Epoch 83/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1849 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1831\n",
      "Epoch 84/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1791 - mean_squared_error: 0.0066 - smape_loss: 0.1773 - val_loss: 0.1849 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1831\n",
      "Epoch 85/400\n",
      "865/865 [==============================] - 1s 1ms/sample - loss: 0.1790 - mean_squared_error: 0.0066 - smape_loss: 0.1772 - val_loss: 0.1849 - val_mean_squared_error: 0.0069 - val_smape_loss: 0.1830\n",
      "111.84347155126326 117.00245667671294\n",
      "6.9297951649720595 7.159522277669584\n"
     ]
    }
   ],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor=0.5,verbose=1, patience=5, min_lr=0.0001, cooldown=5)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "batch_size = 32\n",
    "\n",
    "modelHist = data_model.model.fit([x_train_00, count_train_00, train_days_00], [y_train_00], validation_data=([x_test_00,count_test_00,test_days_00],y_test_00), batch_size=batch_size,epochs=400,callbacks=[reduce_lr,early_stop])\n",
    "\n",
    "print(data_model.mse(data_model.predict('train'), y_train_00), data_model.mse(data_model.predict('test'),y_test_00))\n",
    "print(data_model.mae(data_model.predict('train'), y_train_00), data_model.mae(data_model.predict('test'),y_test_00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@RunAsCUDASubprocess(num_gpus=1)\n",
    "def train_model():\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    config = tf.ConfigProto(intra_op_parallelism_threads=10, \n",
    "                            inter_op_parallelism_threads=10,\n",
    "                            allow_soft_placement=True, \n",
    "                            log_device_placement=True\n",
    "                           )\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.65\n",
    "\n",
    "    session = tf.Session(config=config,graph=tf.get_default_graph())\n",
    "    tb_callback =tf.keras.callbacks.TensorBoard(log_dir='./logs/'+time.ctime(), histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "\n",
    "    tf.keras.backend.set_session(session)\n",
    "    \n",
    "    multiMlps(diffusionDF.clip(upper=1))\n",
    "    \n",
    "    \n",
    "    print(data_model.mae(data_model.predict('train', y_step=0),y_train_00,y_step=0), data_model.mae(data_model.predict('test',y_step=0),y_test_00,y_step=0))\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor=0.5,verbose=1,                              patience=5, min_lr=0.0001, cooldown=5)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "    batch_size = 32\n",
    "    modelHist = data_model.model.fit([x_train_00, count_train_00, train_days_00], [y_train_00], validation_data=([x_test_00,count_test_00,test_days_00],y_test_00), batch_size=batch_size,epochs=400,callbacks=[reduce_lr,early_stop,tb_callback])\n",
    "    data_model.model.save('my_model.h5')\n",
    "    print(data_model.mse(data_model.predict('train'), y_train_00), data_model.mse(data_model.predict('test'),y_test_00))\n",
    "    print(data_model.mae(data_model.predict('train'), y_train_00), data_model.mae(data_model.predict('test'),y_test_00))\n",
    "\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_model.mse(data_model.predict('train'), y_train_00), data_model.mse(data_model.predict('test'),y_test_00))\n",
    "print(data_model.mae(data_model.predict('train'), y_train_00), data_model.mae(data_model.predict('test'),y_test_00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in data_model.model.get_weights()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww=data_model.model.get_weights()[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ww);\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(36,10))\n",
    "\n",
    "plt.plot(ww);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(grid,batch_sizes):\n",
    "    for batch_size in batch_sizes :\n",
    "        for model in grid:\n",
    "            nSegments = len(speedDF)\n",
    "            input_lag, output_lag, sequence_length = 4, 1, 20\n",
    "            valid_split = 0.7\n",
    "            params        = {\"scale_max\":True, \"shift_mean\":False}\n",
    "            data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "            data_model_05 = models.DataModel( speedDF_05, input_lag, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "            data_model_10 = models.DataModel( speedDF_10, input_lag, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "            data_model.preprocessData()\n",
    "            data_model_05.preprocessData()\n",
    "            data_model_10.preprocessData()\n",
    "            x_train_00, y_train_00, x_test_00, y_test_00 = data_model.trainSplit()\n",
    "            x_train_05, y_train_05, x_test_05, y_test_05 = data_model.trainSplit()\n",
    "            x_train_10, y_train_10, x_test_10, y_test_10 = data_model.trainSplit()\n",
    "            x_train = np.concatenate([x_train_00,x_train_05,x_train_10])\n",
    "            y_train = np.concatenate([y_train_00,y_train_05,y_train_10])\n",
    "            count_data = models.DataModel( countDF, input_lag, output_lag, sequence_length, valid_split = valid_split)\n",
    "            count_train, _, count_test, _ = count_data.trainSplit()\n",
    "            count_data_05 = models.DataModel( countDF_05, input_lag, output_lag, sequence_length, valid_split = valid_split)\n",
    "            count_train_05, _, count_test_05, _ = count_data.trainSplit()\n",
    "            count_data_10 = models.DataModel( countDF_10, input_lag, output_lag, sequence_length, valid_split = valid_split)\n",
    "            count_train_10, _, count_test_10, _ = count_data.trainSplit()\n",
    "            count_train =  np.concatenate([ count_train,\n",
    "                                            count_train_05,\n",
    "                                            count_train_10])\n",
    "            data_model.count_data = count_data\n",
    "            data_model.time_data=  data_model.getDaysTypes()\n",
    "            train_days, test_days = data_model.time_data\n",
    "            train_days_05, _ = data_model_05.getDaysTypes()\n",
    "            train_days_10, _ = data_model_10.getDaysTypes()\n",
    "            train_days= np.concatenate([train_days,train_days_05,train_days_10])\n",
    "            tf.keras.backend.clear_session()\n",
    "            config = tf.ConfigProto(intra_op_parallelism_threads=10, \n",
    "                                    inter_op_parallelism_threads=10,\n",
    "                                    allow_soft_placement=True, \n",
    "                                    log_device_placement=True\n",
    "                                   )\n",
    "            session = tf.Session(config=config,graph=tf.get_default_graph())\n",
    "            tf.keras.backend.set_session(session)\n",
    "\n",
    "            data_model.model = createLSTM(*model,x_train,count_train,train_days)\n",
    "            \n",
    "            \n",
    "            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor=0.2,\n",
    "                                  patience=5, min_lr=0.0001)\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "\n",
    "            modelHist = data_model.model.fit([x_train,count_train,train_days],[y_train],validation_data=([x_test_00,count_test,test_days],y_test_00),batch_size=batch_size,epochs=200,callbacks=[reduce_lr,early_stop],verbose=0)\n",
    "            # modelHist = data_model.model.fit([x_train,train_days],[y_train],validation_data=([x_test,test_days],[y_test]),batch_size=batch_size,epochs=200,callbacks=[reduce_lr,early_stop])\n",
    "\n",
    "            res[str(model)+\"_\"+str(batch_size)]=[modelHist.history,modelHist.params,\n",
    "                                             data_model.mse(data_model.predict('train'),y_train_00),data_model.mse(data_model.predict('test'),y_test_00),\n",
    "                                             data_model.mae(data_model.predict('train'),y_train_00),data_model.mae(data_model.predict('test'),y_test_00),\n",
    "                                             model\n",
    "                                            ]\n",
    "            model_plotting = models.ModelPlots(data_model,data_cleaner)\n",
    "                        \n",
    "            model_plotting.cdfPlot(error_type=\"mape\",label=\"mape\")\n",
    "\n",
    "            plt.legend()\n",
    "            plt.savefig(str(model)+\"_\"+str(batch_size)+\".png\",dpi=300,bbox_inches='tight')\n",
    "            plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grid = [\n",
    "    \n",
    "    [[\"flatten\"],[100],[\"flatten\"],[100],['dense'],[100]],\n",
    "    [[\"flatten\"],[200],[\"flatten\"],[200],['dense'],[200]],\n",
    "    [[\"flatten\"],[300],[\"flatten\"],[300],['dense'],[300]],\n",
    "\n",
    "    [[\"flatten\",\"dense\"],[100,100],[\"flatten\",\"dense\"],[100,100],['dense'],[100]],\n",
    "    [[\"flatten\",\"dense\"],[200,200],[\"flatten\",\"dense\"],[200,200],['dense'],[200]],\n",
    "    [[\"flatten\",\"dense\"],[300,300],[\"flatten\",\"dense\"],[300,300],['dense'],[300]],\n",
    "    \n",
    "    [[\"flatten\",\"dense\"],[100,100],[\"flatten\",\"dense\"],[100,100],['dense',\"dense\"],[100,100]],\n",
    "    [[\"flatten\",\"dense\"],[200,200],[\"flatten\",\"dense\"],[200,200],['dense',\"dense\"],[200,200]],\n",
    "    [[\"flatten\",\"dense\"],[300,300],[\"flatten\",\"dense\"],[300,300],['dense',\"dense\"],[300,300]],\n",
    "    \n",
    "    [[\"flatten\",\"dense\",\"dense\"],[100,100,100],[\"flatten\",\"dense\",\"dense\"],[100,100,100],['dense'],[100]],\n",
    "    [[\"flatten\",\"dense\",\"dense\"],[200,200,200],[\"flatten\",\"dense\",\"dense\"],[200,200,200],['dense'],[200]],\n",
    "    [[\"flatten\",\"dense\",\"dense\"],[300,300,300],[\"flatten\",\"dense\",\"dense\"],[300,300,300],['dense'],[300]],\n",
    "    \n",
    "       \n",
    "    [[\"flatten\",\"dense\",\"dense\"],[100,100,100],[\"flatten\",\"dense\",\"dense\"],[100,100,100],['dense',\"dense\"],[100,100]],\n",
    "    [[\"flatten\",\"dense\",\"dense\"],[200,200,200],[\"flatten\",\"dense\",\"dense\"],[200,200,200],['dense',\"dense\"],[200,200]],\n",
    "    [[\"flatten\",\"dense\",\"dense\"],[300,300,300],[\"flatten\",\"dense\",\"dense\"],[300,300,300],['dense',\"dense\"],[300,300]],   \n",
    "    \n",
    "    [[\"lstm\"],[100],[\"lstm\"],[100],['dense'],[100]],\n",
    "    [[\"lstm\"],[200],[\"lstm\"],[200],['dense'],[200]],\n",
    "    [[\"lstm\"],[300],[\"lstm\"],[300],['dense'],[300]],\n",
    "\n",
    "    [[\"lstm\",\"dense\"],[100,100],[\"lstm\",\"dense\"],[100,100],['dense'],[100]],\n",
    "    [[\"lstm\",\"dense\"],[200,200],[\"lstm\",\"dense\"],[200,200],['dense'],[200]],\n",
    "    [[\"lstm\",\"dense\"],[300,300],[\"lstm\",\"dense\"],[300,300],['dense'],[300]],\n",
    "    \n",
    "    [[\"lstm\",\"dense\"],[100,100],[\"lstm\",\"dense\"],[100,100],['dense',\"dense\"],[100,100]],\n",
    "    [[\"lstm\",\"dense\"],[200,200],[\"lstm\",\"dense\"],[200,200],['dense',\"dense\"],[200,200]],\n",
    "    [[\"lstm\",\"dense\"],[300,300],[\"lstm\",\"dense\"],[300,300],['dense',\"dense\"],[300,300]],\n",
    "    \n",
    "    [[\"lstm\",\"dense\",\"dense\"],[100,100,100],[\"lstm\",\"dense\",\"dense\"],[100,100,100],['dense'],[100]],\n",
    "    [[\"lstm\",\"dense\",\"dense\"],[200,200,200],[\"lstm\",\"dense\",\"dense\"],[200,200,200],['dense'],[200]],\n",
    "    [[\"lstm\",\"dense\",\"dense\"],[300,300,300],[\"lstm\",\"dense\",\"dense\"],[300,300,300],['dense'],[300]],\n",
    "    \n",
    "        \n",
    "    [[\"lstm\",\"dense\",\"dense\"],[100,100,100],[\"lstm\",\"dense\",\"dense\"],[100,100,100],['dense',\"dense\"],[100,100]],\n",
    "    [[\"lstm\",\"dense\",\"dense\"],[200,200,200],[\"lstm\",\"dense\",\"dense\"],[200,200,200],['dense',\"dense\"],[200,200]],\n",
    "    [[\"lstm\",\"dense\",\"dense\"],[300,300,300],[\"lstm\",\"dense\",\"dense\"],[300,300,300],['dense',\"dense\"],[300,300]],\n",
    " \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res={}\n",
    "gridSearch(my_grid,batch_sizes=[20,10,5,2,3,8,15,25,30,50,64,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf=pd.DataFrame(res,index =[\"name\",\"params\",\"tmse\",\"vmse\",\"tmae\",\"vmae\",\"arch\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf.sort_values('vmae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLSTM(speed_layers,speed_layers_units,count_layers,count_layers_units,merge_layers,merge_layers_units,x_train,count_train,train_days):\n",
    "    main_input = tf.keras.layers.Input( x_train.shape[1:], name=\"speed_input\")\n",
    "    count_input = tf.keras.layers.Input( count_train.shape[1:], name=\"count_input\")\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    \n",
    "    speed_layer =main_input\n",
    "    count_layer=count_input\n",
    "    for x,n_units in zip(speed_layers,speed_layers_units) :\n",
    "        if x.lower() == \"lstm\":\n",
    "            speed_layer= tf.keras.layers.LSTM(n_units)(speed_layer)\n",
    "        if x.lower() == \"dense\":\n",
    "            speed_layer= tf.keras.layers.Dense(n_units,activation=\"sigmoid\")(speed_layer)\n",
    "        if x.lower() == \"dropout\":\n",
    "            speed_layer= tf.keras.layers.Dropout(n_units)(speed_layer)\n",
    "        if x.lower() == \"flatten\":\n",
    "            speed_layer= tf.keras.layers.Flatten()(speed_layer)\n",
    "    for x,n_units in zip( count_layers,count_layers_units ):\n",
    "        if x.lower() == \"lstm\":\n",
    "            count_layer= tf.keras.layers.LSTM(n_units)(count_layer)\n",
    "        if x.lower() == \"dense\":\n",
    "            count_layer= tf.keras.layers.Dense(n_units,activation=\"sigmoid\")(count_layer)\n",
    "        if x.lower() == \"dropout\":\n",
    "            count_layer= tf.keras.layers.Dropout(n_units)(count_layer)\n",
    "        if x.lower() == \"flatten\":\n",
    "            count_layer= tf.keras.layers.Flatten()(count_layer)           \n",
    "\n",
    "    layer = tf.keras.layers.Concatenate(1,name=\"count_speed_merge\")([speed_layer, count_layer,daytime_input])\n",
    "        \n",
    "\n",
    "    for x,n_units in  zip(merge_layers,merge_layers_units ):\n",
    "        if x.lower() == \"dense\":\n",
    "            layer= tf.keras.layers.Dense(n_units,activation=\"sigmoid\")(layer)\n",
    "        if x.lower() == \"dropout\":\n",
    "            layer= tf.keras.layers.Dropout(n_units)(layer)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\")(layer)\n",
    "\n",
    "\n",
    "    model =tf.keras.Model(inputs = [main_input,count_input,daytime_input], outputs= [output_layer])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(loss=smape_loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r2_score(data_model.predict('train'),data_model.getYAtStep(y_train_00),multioutput=\"raw_values\"))\n",
    "r2s =r2_score(data_model.predict('train'),data_model.getYAtStep(y_train_00),multioutput=\"raw_values\").round(3)\n",
    "r2s.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(data_model.predict('train'),data_model.getYAtStep(y_train_00),multioutput=\"variance_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(data_model.predict('test'),data_model.getYAtStep(y_test_00),multioutput=\"variance_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(data_model.getRawYData(data_model.predict('test')),data_model.getRawYData(data_model.getYAtStep(y_test_00))))\n",
    "r2_score(data_model.predict('train'),data_model.getYAtStep(y_train_00))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models= reload(models)\n",
    "model_plotting = models.ModelPlots(data_model, data_cleaner)\n",
    "# model_plotting = models.ModelPlots(data_model,data_cleaner,split=\"test\",y=data_model.getRawYData(y_test_00))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDF=data_model.restorePredictionsAsDF(data_model.getYAtStep(data_model.y))\n",
    "predDF = data_model.restorePredictionsAsDF( data_model.predict('full'))\n",
    "\n",
    "time_error_df  = abs(predDF-yDF).groupby(pd.DatetimeIndex(abs(predDF-yDF).columns).time,axis=1).mean()\n",
    "\n",
    "tag_error_df  = abs(predDF-yDF).assign(tag = data_cleaner.segments_tags).groupby('tag').mean()\n",
    "\n",
    "fig=plt.figure(figsize=(24,12))\n",
    "gs = matplotlib.gridspec.GridSpec(3, 3, figure=fig)\n",
    "fig.add_subplot(gs[0, 0])\n",
    "int_err = (data_model.getRawYData(data_model.predict('full')).flatten() - data_model.getRawYData(data_model.getYAtStep(data_model.y)).flatten()).astype(int)\n",
    "err_series = pd.Series(int_err).value_counts()\n",
    "plt.hist(int_err//10*10,bins=26);\n",
    "plt.title(\"error hist\");\n",
    "fig.add_subplot(gs[0, 1])\n",
    "err_series.plot(\"pie\",label=\"error\")\n",
    "plt.title(\"error pie plot\");\n",
    "int_err.sort()\n",
    "fig.add_subplot(gs[0, 2])\n",
    "plt.plot(int_err)\n",
    "fig.add_subplot(gs[1,:])\n",
    "time_error_df.boxplot(figsize=(18,6));\n",
    "fig.add_subplot(gs[2,:])\n",
    "tag_error_df.T.boxplot(figsize=(18,6));\n",
    "# saveFig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plotting.plotSegmentSeries(265,plot_surface=True)\n",
    "model_plotting.plotSegmentSeries(265)\n",
    "data_cleaner.plotSegmentComponents(265)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plotting.plotSegmentSeries(395,plot_surface=True)\n",
    "model_plotting.plotSegmentSeries(395)\n",
    "data_cleaner.plotSegmentComponents(395)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_plotting.plotMultipleSegmentsSeries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plotting.plotMultipleSegmentsSeries(plot_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.regularizers.L1L2??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Layer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plotting.plotPredictionMatchHeatMap(\"train\")\n",
    "# saveFig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plotting.plotPredictionMatchHeatMap(\"valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDF=data_model.restorePredictionsAsDF(data_model.y)\n",
    "predDF = data_model.restorePredictionsAsDF( data_model.predict('full'))\n",
    "\n",
    "# folium_map = plotPredictions(segmentsMeta,data_model, yDF, predDF, yDF.columns[np.r_[:len(yDF.columns)-1:10j].astype(int)],mergedIndex,updatedcounts)\n",
    "folium_map=folium.plugins.DualMap(location=[48.10301,-1.65537],\n",
    "                    zoom_start=13,\n",
    "                    tiles=\"OpenStreetMap\")\n",
    "\n",
    "folium.features.LinearColormap([plt.cm.brg_r(0),plt.cm.brg_r(0.5)],caption=\"Error rate\").add_to(folium_map.m1)\n",
    "\n",
    "folium_train_map = model_plotting.plotPredictions(yDF,predDF,yDF.columns[np.r_[8:14:1].astype(int)],folium_map=folium_map.m1)\n",
    "folium_validation_map = model_plotting.plotPredictions(yDF,predDF,yDF.columns[np.r_[len(yDF.columns)*valid_split+8:len(yDF.columns)*valid_split+14:1].astype(int)],folium_map=folium_map.m2)\n",
    "\n",
    "folium_map.save(results_path+model_name+input()+' map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predSegs = segmentsMeta[segmentsMeta.segmentID.isin(mergedIndex[mergedIndex.isin(speedDF.index)].index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDF=data_model.restorePredictionsAsDF(data_model.y)\n",
    "predDF = data_model.restorePredictionsAsDF( data_model.predict('full'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_20_error=abs(predDF-yDF)\n",
    "_20_error.mean().plot(use_index=False,figsize=(36,6))\n",
    "plt.xticks(*list(zip(*list(enumerate(predDF.columns))[::20])),rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_20_error=abs(predDF-yDF)>20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(36,6))\n",
    "pd.plotting.autocorrelation_plot(_20_error.mean())\n",
    "plt.axvline(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_20_error.sum(axis=1).values[np.where(_20_error.sum(axis=1).values>80)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(_20_error.sum(axis=1).values>80)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "plt.plot(_20_error.sum(axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = data_model.getRawYData(data_model.predict(\"test\").clip(0)) - data_model.getRawYData(y_test_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_error= abs(error.flatten().round())//5*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(*np.unique(round_error,return_counts=True),);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.unique(round_error,return_counts=True),index=[\"error range\",\"number of occurrences\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,12))\n",
    "plt.hist(sorted(round_error),bins=23);\n",
    "plt.xticks(range(0,100,5),range(0,100,5));\n",
    "plt.xlabel(\"error range (5km/h groups)\")\n",
    "plt.ylabel(\"number of occurrences\")\n",
    "plt.title(\"error distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params        = {}\n",
    "\n",
    "lastValue_data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "lastValue_data_model.preprocessData()\n",
    "\n",
    "model =models.BaseModels(\"lastValue\",historic_data=speedDF[speedDF.columns[:int(0.7*len(speedDF.columns))]])\n",
    "lastValue_data_model.model=model\n",
    "lastValue_model_plotting = models.ModelPlots(lastValue_data_model, data_cleaner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params        = {}\n",
    "\n",
    "timehistoric_data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "timehistoric_data_model.preprocessData()\n",
    "\n",
    "model =models.BaseModels(\"timeHistoric\",historic_data=speedDF[speedDF.columns[:int(0.7*len(speedDF.columns))]])\n",
    "timehistoric_data_model.model=model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params        = {}\n",
    "\n",
    "historic_data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "historic_data_model.preprocessData()\n",
    "\n",
    "model =models.BaseModels(\"historic\",historic_data=speedDF[speedDF.columns[:int(0.7*len(speedDF.columns))]])\n",
    "historic_data_model.model=model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params        = {}\n",
    "\n",
    "AR5_data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "AR5_data_model.preprocessData()\n",
    "\n",
    "model =models.BaseModels(\"AR5\",historic_data=speedDF[speedDF.columns[:int(0.7*len(speedDF.columns))]],lag=input_lag)\n",
    "AR5_data_model.model=model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_model_plotting = models.ModelPlots(mlpModel, data_cleaner, split=\"test\", y=data_model.getRawYData(y_test_00))\n",
    "lastValue_model_plotting = models.ModelPlots(lastValue_data_model,data_cleaner,split=\"test\",y=data_model.getRawYData(y_test_00))\n",
    "historic_model_plotting = models.ModelPlots(historic_data_model,data_cleaner,split=\"test\",y=data_model.getRawYData(y_test_00))\n",
    "timehistoric_model_plotting = models.ModelPlots(timehistoric_data_model,data_cleaner,split=\"test\",y=data_model.getRawYData(y_test_00))\n",
    "AR5_model_plotting = models.ModelPlots(AR5_data_model,data_cleaner,split=\"test\",y=data_model.getRawYData(y_test_00))\n",
    "model_plotting = models.ModelPlots(data_model,data_cleaner,split=\"test\",y=data_model.getRawYData(y_test_00))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plotting = models.ModelPlots(data_model,data_cleaner,split=\"test\",y=data_model.getRawYData(y_test_00))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,18))\n",
    "ax=plt.gca()\n",
    "oldModelPlotting.plotDiscreteSpeedError(ax,\"old\")\n",
    "lastValue_model_plotting.plotDiscreteSpeedError(ax,\"last_value\")\n",
    "timehistoric_model_plotting.plotDiscreteSpeedError(ax,\"timehistoric\")\n",
    "historic_model_plotting.plotDiscreteSpeedError(ax,\"historic\")\n",
    "AR5_model_plotting.plotDiscreteSpeedError(ax,\"AR5\")\n",
    "model_plotting.plotDiscreteSpeedError(ax,\"Current model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,18))\n",
    "ax=plt.gca()\n",
    "oldModelPlotting.plotDiscreteSpeedError(ax,\"old\")\n",
    "lastValue_model_plotting.plotDiscreteSpeedError(ax,\"last_value\")\n",
    "timehistoric_model_plotting.plotDiscreteSpeedError(ax,\"timehistoric\")\n",
    "historic_model_plotting.plotDiscreteSpeedError(ax,\"historic\")\n",
    "AR5_model_plotting.plotDiscreteSpeedError(ax,\"AR5\")\n",
    "model_plotting.plotDiscreteSpeedError(ax,\"Current model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "oldModelPlotting.cdfPlot(error_type=\"mape\",label=\"old\",plot_lines=False)\n",
    "lastValue_model_plotting.cdfPlot(error_type=\"mape\",label=\"lastValue\",plot_lines=False)\n",
    "timehistoric_model_plotting.cdfPlot(error_type=\"mape\",label=\"timehistoric\",plot_lines=False)\n",
    "historic_model_plotting.cdfPlot(error_type=\"mape\",label=\"historic\",plot_lines=False)\n",
    "AR5_model_plotting.cdfPlot(error_type=\"mape\",label=\"AR5\",plot_lines=False)\n",
    "model_plotting.cdfPlot(error_type=\"mape\",label=\"current\",plot_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "oldModelPlotting.cdfPlot(error_type=\"mae\",label=\"old\",plot_lines=False)\n",
    "lastValue_model_plotting.cdfPlot(error_type=\"mae\",label=\"lastValue\",plot_lines=False)\n",
    "timehistoric_model_plotting.cdfPlot(error_type=\"mae\",label=\"timehistoric\",plot_lines=False)\n",
    "historic_model_plotting.cdfPlot(error_type=\"mae\",label=\"historic\",plot_lines=False)\n",
    "AR5_model_plotting.cdfPlot(error_type=\"mae\",label=\"AR5\",plot_lines=False)\n",
    "model_plotting.cdfPlot(error_type=\"mae\",label=\"current\",plot_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "oldModelPlotting.cdfPlot(error_type=\"mse\",label=\"old\",plot_lines=False)\n",
    "lastValue_model_plotting.cdfPlot(error_type=\"mse\",label=\"lastValue\",plot_lines=False)\n",
    "timehistoric_model_plotting.cdfPlot(error_type=\"mse\",label=\"timehistoric\",plot_lines=False)\n",
    "historic_model_plotting.cdfPlot(error_type=\"mse\",label=\"historic\",plot_lines=False)\n",
    "AR5_model_plotting.cdfPlot(error_type=\"mse\",label=\"AR5\",plot_lines=False)\n",
    "model_plotting.cdfPlot(error_type=\"mse\",label=\"current\",plot_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldModelPlotting = model_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,18))\n",
    "ax=plt.gca()\n",
    "lastValue_model_plotting.plotDiscreteSpeedError(ax,\"last_value\")\n",
    "timehistoric_model_plotting.plotDiscreteSpeedError(ax,\"timehistoric\")\n",
    "historic_model_plotting.plotDiscreteSpeedError(ax,\"historic\")\n",
    "AR5_model_plotting.plotDiscreteSpeedError(ax,\"AR5\")\n",
    "oldMLPModel.plotDiscreteSpeedError(ax,\"oldMLPModel\")\n",
    "oldLstmModel.plotDiscreteSpeedError(ax,\"oldLstmModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18,18))\n",
    "lastValue_model_plotting.cdfPlot(error_type=\"mae\",label=\"lastValue\",plot_lines=False)\n",
    "timehistoric_model_plotting.cdfPlot(error_type=\"mae\",label=\"timehistoric\",plot_lines=False)\n",
    "historic_model_plotting.cdfPlot(error_type=\"mae\",label=\"historic\",plot_lines=False)\n",
    "AR5_model_plotting.cdfPlot(error_type=\"mae\",label=\"AR5\",plot_lines=False)\n",
    "oldMLPModel.cdfPlot(error_type=\"mae\",label=\"oldMLPModel\",plot_lines=False)\n",
    "oldLstmModel.cdfPlot(error_type=\"mae\",label=\"oldLstmModel\",plot_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"old mae.png\",dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,18))\n",
    "ax=plt.gca()\n",
    "lastValue_model_plotting.plotDiscreteSpeedError(ax,\"last_value\")\n",
    "timehistoric_model_plotting.plotDiscreteSpeedError(ax,\"timehistoric\")\n",
    "historic_model_plotting.plotDiscreteSpeedError(ax,\"historic\")\n",
    "AR5_model_plotting.plotDiscreteSpeedError(ax,\"AR5\")\n",
    "balancedLstmsPlotting.plotDiscreteSpeedError(ax,\"balancedLstmsPlotting\")\n",
    "unbalancedLstmsPlotting.plotDiscreteSpeedError(ax,\"unbalancedLstmsPlotting\")\n",
    "balancedConvsPlotting.plotDiscreteSpeedError(ax,\"balancedConvsPlotting\")\n",
    "unbalancedConvsPlotting.plotDiscreteSpeedError(ax,\"unbalancedConvsPlotting\")\n",
    "balancedConvsTanhPlotting.plotDiscreteSpeedError(ax,\"balancedConvsTanhPlotting\")\n",
    "unbalancedConvsTanhPlotting.plotDiscreteSpeedError(ax,\"unbalancedConvsTanhPlotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(18,18))\n",
    "lastValue_model_plotting.cdfPlot(error_type=\"mape\",label=\"lastValue\",plot_lines=False)\n",
    "timehistoric_model_plotting.cdfPlot(error_type=\"mape\",label=\"timehistoric\",plot_lines=False)\n",
    "historic_model_plotting.cdfPlot(error_type=\"mape\",label=\"historic\",plot_lines=False)\n",
    "AR5_model_plotting.cdfPlot(error_type=\"mape\",label=\"AR5\",plot_lines=False)\n",
    "balancedLstmsPlotting.cdfPlot(error_type=\"mape\",label=\"balancedLstmsPlotting\",plot_lines=False)\n",
    "unbalancedLstmsPlotting.cdfPlot(error_type=\"mape\",label=\"unbalancedLstmsPlotting\",plot_lines=False)\n",
    "balancedConvsPlotting.cdfPlot(error_type=\"mape\",label=\"balancedConvsPlotting\",plot_lines=False)\n",
    "unbalancedConvsPlotting.cdfPlot(error_type=\"mape\",label=\"unbalancedConvsPlotting\",plot_lines=False)\n",
    "balancedConvsTanhPlotting.cdfPlot(error_type=\"mape\",label=\"balancedConvsTanhPlotting\",plot_lines=False)\n",
    "unbalancedConvsTanhPlotting.cdfPlot(error_type=\"mape\",label=\"unbalancedConvsTanhPlotting\",plot_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmodels = [lastValue_model_plotting,\n",
    "timehistoric_model_plotting,\n",
    "historic_model_plotting,\n",
    "AR5_model_plotting,\n",
    "balancedLstmsPlotting,\n",
    "unbalancedLstmsPlotting,\n",
    "balancedConvsPlotting,\n",
    "unbalancedConvsPlotting,\n",
    "balancedConvsTanhPlotting,\n",
    "unbalancedConvsTanhPlotting]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmodelsnames = [\"lastValue\",\n",
    "\"timehistoric\",\n",
    "\"historic_model\",\n",
    "\"AR5_model\",\n",
    "\"balancedLstms\",\n",
    "\"unbalancedLstms\",\n",
    "\"balancedConvs\",\n",
    "\"unbalancedConvs\",\n",
    "\"balancedConvsTanh\",\n",
    "\"unbalancedConvsTanh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldmodels = [lastValue_model_plotting,\n",
    "timehistoric_model_plotting,\n",
    "historic_model_plotting,\n",
    "AR5_model_plotting,\n",
    "oldMLPModel,\n",
    "oldLstmModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldplotmodelsnames = [\"lastValue\",\n",
    "\"timehistoric\",\n",
    "\"historic_model\",\n",
    "\"AR5_model\",\n",
    "\"oldMLPModel\",\n",
    "\"oldLstmModel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastmse(x,y):\n",
    "    return np.mean((x-y)**2)\n",
    "def fastmae(x,y):\n",
    "    return np.mean(abs(x-y))\n",
    "\n",
    "def fastmseclip(x,y):\n",
    "    return np.mean((x.clip(15)-y.clip(15))**2)\n",
    "def fastmaeclip(x,y):\n",
    "    return np.mean(abs(x.clip(15)-y.clip(15)))\n",
    "\n",
    "def fastmape(x,y):\n",
    "    return np.mean(abs(x.clip(15)-y.clip(15))/y.clip(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mderrorsDF = pd.DataFrame([[fastmse(mopl.y.flatten(),mopl.preds.flatten()) for mopl in plotmodels],\n",
    "              [fastmae(mopl.y.flatten(),mopl.preds.flatten()) for mopl in plotmodels],\n",
    "              [fastmseclip(mopl.y.flatten(),mopl.preds.flatten()) for mopl in plotmodels],\n",
    "              [fastmaeclip(mopl.y.flatten(),mopl.preds.flatten()) for mopl in plotmodels],\n",
    "              [fastmape(mopl.y.flatten(),mopl.preds.flatten()) for mopl in plotmodels],\n",
    "             ],\n",
    "             index=[\"mse\",\"mae\",\"mse clipped 15\",\"mae clipped 15\",\"mape clipped 15\"],\n",
    "             columns=plotmodelsnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldmderrorsDF = pd.DataFrame([[fastmse(mopl.y.flatten(),mopl.preds.flatten()) for mopl in oldmodels],\n",
    "              [fastmae(mopl.y.flatten(),mopl.preds.flatten()) for mopl in oldmodels],\n",
    "              [fastmseclip(mopl.y.flatten(),mopl.preds.flatten()) for mopl in oldmodels],\n",
    "              [fastmaeclip(mopl.y.flatten(),mopl.preds.flatten()) for mopl in oldmodels],\n",
    "              [fastmape(mopl.y.flatten(),mopl.preds.flatten()) for mopl in oldmodels],\n",
    "             ],\n",
    "             index=[\"mse\",\"mae\",\"mse clipped 15\",\"mae clipped 15\",\"mape clipped 15\"],\n",
    "             columns=oldplotmodelsnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mderrorsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldmderrorsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = OsmProcessing.getSegments(osmWays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = OsmProcessing.setOneWay(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap = Plotting.getFoliumMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst =list(map(lambda x :int(x[:-2]),data_cleaner.mergedIndex[data_cleaner.mergedIndex.isin(speedDF.index)].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Plotting.plotRoads(segments[segments.index.isin(lst)],fmap=fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.add_to(fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap.save(\"primary_map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedaydata = coyoteData.find({\"time\":{\"$gte\":1547560800,\"$lt\":1547578800}})\n",
    "\n",
    "onedaydata = list(onedaydata)\n",
    "\n",
    "onedayDF=pd.DataFrame(onedaydata)\n",
    "onedayDF.sort_values(\"time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedayDF=onedayDF.assign(h=pd.to_datetime( onedayDF.time,unit = \"s\").apply(lambda x: x.hour))\n",
    "onedayDF=onedayDF.assign(m=pd.to_datetime( onedayDF.time,unit = \"s\").apply(lambda x:  x.minute//15*15))\n",
    "onedayDF=onedayDF.assign(m1=pd.to_datetime( onedayDF.time,unit = \"s\").apply(lambda x:  x.minute//1*1))\n",
    "onedayDF=onedayDF.assign(m3=pd.to_datetime( onedayDF.time,unit = \"s\").apply(lambda x:  x.minute//3*3))\n",
    "onedayDF=onedayDF.assign(m5=pd.to_datetime( onedayDF.time,unit = \"s\").apply(lambda x:  x.minute//5*5))\n",
    "onedayDF=onedayDF.assign(m10=pd.to_datetime( onedayDF.time,unit = \"s\").apply(lambda x:  x.minute//10*10))\n",
    "onedayDF=onedayDF.assign(m15=pd.to_datetime( onedayDF.time,unit = \"s\").apply(lambda x:  x.minute//15*15))\n",
    "\n",
    "maxT=onedayDF.time.max()\n",
    "minT=onedayDF.time.min()\n",
    "colors=[matplotlib.colors.rgb2hex(x) for x in plt.cm.hot((onedayDF.time-minT)/(maxT-minT))]\n",
    "onedayDF=onedayDF.assign(color = colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedayDF.groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.MultiIndex([(x,y) for x,y in zip(range())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segID = 372736550\n",
    "onedayDF[onedayDF.matching_road==segID].groupby([\"h\",\"m1\"])[\"speed\"].mean().plot()\n",
    "plt.figure()\n",
    "onedayDF[onedayDF.matching_road==segID].groupby([\"h\",\"m3\"])[\"speed\"].mean().plot()\n",
    "plt.figure()\n",
    "\n",
    "onedayDF[onedayDF.matching_road==segID].groupby([\"h\",\"m5\"])[\"speed\"].mean().plot()\n",
    "plt.figure()\n",
    "\n",
    "onedayDF[onedayDF.matching_road==segID].groupby([\"h\",\"m10\"])[\"speed\"].mean().plot()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "onedayDF[onedayDF.matching_road==segID].groupby([\"h\",\"m15\"])[\"speed\"].mean().plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedayDF[onedayDF.matching_road==segID].groupby([\"time\"])[\"speed\"].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timelocs=onedayDF[onedayDF.matching_road==23162202].groupby([\"h\",\"m\"])['loc'].agg(lambda x : tuple(x.apply(lambda x : x['coordinates'])))\n",
    "timeIds =onedayDF[onedayDF.matching_road==23162202].groupby([\"h\",\"m\"])['id'].agg(lambda x :tuple(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[]\n",
    "fmap = Plotting.getFoliumMap()\n",
    "\n",
    "for time_idx in zip( timelocs.index[1:-1],timelocs.index[2:],timelocs.index[:-2]):\n",
    "    current_locs = timelocs.loc[time_idx[0]]\n",
    "    current_ids = timeIds.loc[time_idx[0]]\n",
    "    full_locs = onedayDF[onedayDF.id.isin(current_ids)].groupby([\"h\",\"m\"])['loc'].agg(lambda x : tuple(x.apply(lambda x : x['coordinates'])))\n",
    "    if time_idx[1]  not  in full_locs.index or time_idx[2]  not  in full_locs.index:\n",
    "        continue\n",
    "    \n",
    "    current_full_locs = full_locs.loc[time_idx[0]]                 \n",
    "    current_colors = [\"red\" for _ in range(len(current_locs))]\n",
    "    current_full_colors = [\"green\" for _ in range(len(current_full_locs))]\n",
    "    \n",
    "    next_locs = full_locs.loc[time_idx[1]]\n",
    "    next_colors = [\"blue\" for _ in range(len(next_locs))]\n",
    "    \n",
    "    previous_locs = full_locs.loc[time_idx[2]]\n",
    "    previous_colors = [\"purple\" for _ in range(len(previous_locs))]\n",
    "           \n",
    "    locs = np.concatenate((current_full_locs,current_locs ,next_locs,previous_locs))\n",
    "    \n",
    "    colors = np.concatenate(( current_full_colors,current_colors,next_colors,previous_colors))\n",
    "    layer= Plotting.getLayerWithPositions(locs,colors,fmap,fill_colors=colors,name=\"N: \"+str(len(current_ids))+str(time_idx),radius=2)\n",
    "\n",
    "    layers.append(layer)\n",
    "    \n",
    "fmap=Plotting.stackHistotyLayers(layers,fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[]\n",
    "fmap = Plotting.getFoliumMap()\n",
    "\n",
    "current_locs = []\n",
    "current_full_locs=[]\n",
    "next_locs = []\n",
    "current_colors = []\n",
    "current_full_colors = []\n",
    "next_colors = []\n",
    "\n",
    "previous_locs =[]\n",
    "previous_colors =[]\n",
    "\n",
    "def getLocsAndColors(time_idx,full_locs,color=\"red\"):\n",
    "    locs = full_locs.loc[time_idx]\n",
    "    colors = [color]*len(locs)\n",
    "    return locs,colors\n",
    "\n",
    "for time_idx in zip( timelocs.index[1:-1],timelocs.index[2:],timelocs.index[:-2]):\n",
    "    current_locs += timelocs.loc[time_idx[0]]\n",
    "    current_colors += [\"red\" for _ in range(len(current_locs))]\n",
    "\n",
    "    current_ids = timeIds.loc[time_idx[0]]\n",
    "    full_locs = onedayDF[onedayDF.id.isin(current_ids)].groupby([\"h\",\"m\"])['loc'].agg(lambda x : tuple(x.apply(lambda x : x['coordinates'])))\n",
    "    if time_idx[1]  not  in full_locs.index or time_idx[2]  not  in full_locs.index:\n",
    "        continue\n",
    "        \n",
    "    current_full_locs += full_locs.loc[time_idx[0]]           \n",
    "    current_full_colors += [\"green\" for _ in range(len(current_full_locs))]\n",
    "    \n",
    "    next_locs += full_locs.loc[time_idx[1]]\n",
    "    next_colors += [\"blue\" for _ in range(len(next_locs))]\n",
    "\n",
    "    previous_locs += full_locs.loc[time_idx[2]]\n",
    "    previous_colors += [\"purple\" for _ in range(len(previous_locs))]\n",
    "        \n",
    "layer= Plotting.getLayerWithPositions(current_locs,current_colors,fmap,fill_colors=current_colors,name=\" segment\",radius=2)\n",
    "layers.append(layer)\n",
    "\n",
    "layer= Plotting.getLayerWithPositions(current_full_locs,current_full_colors,fmap,fill_colors=current_full_colors,name=\"current\",radius=2)\n",
    "layers.append(layer)\n",
    "\n",
    "layer= Plotting.getLayerWithPositions(next_locs,next_colors,fmap,fill_colors=next_colors,name=\"next\",radius=2)\n",
    "layers.append(layer)\n",
    "\n",
    "layer= Plotting.getLayerWithPositions(previous_locs,previous_colors,fmap,fill_colors=previous_colors,name=\"previous\",radius=2)\n",
    "layers.append(layer)\n",
    "\n",
    "fmap=Plotting.stackHistotyLayers(layers,fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap.save(\"diffuse_map.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusionDF=None\n",
    "dates = list(set([pd.Timestamp(x.date()).value//10**9 for x in speedDF.columns]))\n",
    "\n",
    "for date in dates:\n",
    "    onedaydata = coyoteData.find({\"time\":{\"$gte\":date+14*60*60,\"$lt\":date+19*60*60}})\n",
    "    onedayDF=pd.DataFrame(list(onedaydata))\n",
    "    onedayDF.sort_values(\"time\", inplace=True)\n",
    "    onedayDF=onedayDF.assign(segmentID = onedayDF[['matching_road','heading_road']].apply(lambda x : str(x[0])+'_'+str(x[1]),axis=1))\n",
    "    onedayDF=onedayDF.assign(groupID =  mergedIndex.reindex(onedayDF.segmentID.values).values)\n",
    "    onedayDF=onedayDF.dropna()\n",
    "    onedayDF=onedayDF.assign(quarter=(onedayDF.time-date)//(15*60))\n",
    "    quarter_group_by=onedayDF.groupby([\"quarter\",\"groupID\",\"id\"]).size()\n",
    "    quartersIDS = [56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]\n",
    "    if diffusionDF is None :\n",
    "        diffusionDF = pd.DataFrame(0,index=onedayDF.groupID.unique(),columns=onedayDF.groupID.unique())\n",
    "        np.fill_diagonal(diffusionDF.values, 1)\n",
    "    for quarter,futur in zip(quartersIDS[:-1],quartersIDS[1:]) :\n",
    "        quarte_vals=quarter_group_by.loc[quarter]\n",
    "        futur_vals=quarter_group_by.loc[futur]\n",
    "        seg_idxs=quarte_vals.index.get_level_values(0).unique().values\n",
    "        l=list(map(lambda x : futur_vals.loc[:,quarte_vals.loc[x].index.values].groupby(\"groupID\").sum(),seg_idxs))\n",
    "        a=pd.concat(l,axis=1,sort=False)\n",
    "        a.columns = seg_idxs\n",
    "        diffusionDF=diffusionDF.add(a.T,fill_value=0)\n",
    "\n",
    "diffusionDF=diffusionDF.reindex(speedDF.index).reindex(speedDF.index,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedaydata = coyoteData.find({\"time\":{\"$gte\":1547560800,\"$lt\":1547578800}})\n",
    "\n",
    "onedaydata = list(onedaydata)\n",
    "\n",
    "onedayDF=pd.DataFrame(onedaydata)\n",
    "onedayDF.sort_values(\"time\", inplace=True)\n",
    "\n",
    "\n",
    "onedayDF=onedayDF.assign(segmentID = onedayDF[['matching_road','heading_road']].apply(lambda x : str(x[0])+'_'+str(x[1]),axis=1))\n",
    "\n",
    "onedayDF=onedayDF.assign(groupID =  mergedIndex.reindex(onedayDF.segmentID.values).values)\n",
    "\n",
    "onedayDF=onedayDF.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=pd.Timestamp(pd.to_datetime(1547560800,unit=\"s\").date()).value//10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedayDF=onedayDF.assign(quarter=(onedayDF.time-d)//(15*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_group_by=onedayDF.groupby([\"quarter\",\"groupID\",\"id\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartersIDS=[56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_56 = quarter_group_by.loc[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusionDF = pd.DataFrame(0, index = onedayDF.groupID.unique(), columns = onedayDF.groupID.unique())\n",
    "np.fill_diagonal(diffusionDF.values, 1)\n",
    "for quarter,futur in zip(quartersIDS[:-1],quartersIDS[1:]) :\n",
    "    quarte_vals=quarter_group_by.loc[quarter]\n",
    "    futur_vals=quarter_group_by.loc[futur]\n",
    "    seg_idxs=quarte_vals.index.get_level_values(0).unique().values\n",
    "    l=list(map(lambda x : futur_vals.loc[:,quarte_vals.loc[x].index.values].groupby(\"groupID\").sum(),seg_idxs))\n",
    "    a=pd.concat(l,axis=1,sort=False)\n",
    "    a.columns = seg_idxs\n",
    "    diffusionDF=diffusionDF.add(a.T,fill_value=0)\n",
    "diffusionDF=diffusionDF.reindex(speedDF.index).reindex(speedDF.index,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(diffusionDF<=10).sum().sum()/(536**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusionDF.stack().clip(upper=50).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(diffusionDF.stack().values,reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(18,18))\n",
    "plt.imshow(diffusionDF.clip(upper=10))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=x_train_00[:,-1,:]\n",
    "\n",
    "Y=y_train_00\n",
    "\n",
    "Y.shape,X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassos = list(map(lambda y : sklearn.linear_model.LassoCV(fit_intercept=False,cv=5).fit(X,y), Y.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
