{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example script to generate text from Nietzsche's writings.\n",
    "At least 20 epochs are required before the generated text\n",
    "starts sounding coherent.\n",
    "It is recommended to run this script on GPU, as recurrent\n",
    "networks are quite computationally intensive.\n",
    "If you try this script on new data, make sure your corpus\n",
    "has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "import sklearn\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import time\n",
    "import sys\n",
    "import pymongo\n",
    "sys.path.append('../source/')\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "import Plotting\n",
    "import folium\n",
    "import matplotlib\n",
    "import osmMerger\n",
    "import CustomUtils\n",
    "import OsmProcessing\n",
    "import pandas as pd\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModel:\n",
    "    def __init__(self,data, input_lag, output_lag, sequence_length,scale_max=False,scale_log=False,shift_mean=False,y_only=False,add_time=False,max_value=130,valid_split=0.8,min_max_scale=False):\n",
    "        self.data = data\n",
    "        self.input_lag = input_lag\n",
    "        self.output_lag = output_lag\n",
    "        self.sequence_length = sequence_length\n",
    "        self.scale_max = scale_max\n",
    "        self.scale_log = scale_log\n",
    "        self.shift_mean = shift_mean\n",
    "        self.y_only=y_only\n",
    "        self.add_time = add_time\n",
    "        self.max_value = max_value\n",
    "        self.min_max_scale = min_max_scale\n",
    "        self.model=None\n",
    "        self.valid_split=valid_split\n",
    "        self.x,self.y,self.t =self.getXY()\n",
    "        self.__reversed_process=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getDaysTypes(self):\n",
    "        day_types = pd.DatetimeIndex(self.t.reshape(-1)).weekday.values.reshape(self.t.shape)\n",
    "        time_fraction = (CustomUtils.timeToSeconds(pd.DatetimeIndex(data_model.t.reshape(-1)))/(60*60)).values.reshape(self.t.shape)\n",
    "        time_input = np.concatenate([day_types,time_fraction],1)\n",
    "        return time_input[:int(len(self.x)*(self.valid_split))],time_input[int(len(self.x)*(self.valid_split)):]\n",
    "    \n",
    "    def getExamples(self,sequence,hours):\n",
    "        sequence_length=len(sequence)\n",
    "        sub_sequence_length = self.input_lag+self.output_lag\n",
    "        if sub_sequence_length > sequence_length :\n",
    "            raise ValueError(\"sequence length {} too small for lags : {},{}\".format(sequence_length,input_lag,output_lag))\n",
    "        return [sequence[i:i+input_lag] for i in range(0,sequence_length-sub_sequence_length+1,1)],\\\n",
    "               [sequence[i+input_lag:i+input_lag+output_lag] for i in range(0,sequence_length-sub_sequence_length+1,1)],\\\n",
    "               [hours[i+input_lag:i+input_lag+output_lag] for i in range(0,sequence_length-sub_sequence_length+1,1)]  \n",
    "    \n",
    "    def getXY(self):\n",
    "        nsegs,ntime=self.data.shape\n",
    "        if(ntime%self.sequence_length)!= 0 :\n",
    "            raise ValueError(\"sequence length {} not compatible with number of time features {}\".format(self.sequence_length,ntime))\n",
    "\n",
    "        shapedData = self.data.values.T.reshape(int(ntime/self.sequence_length),self.sequence_length,nsegs)\n",
    "        timestamps = pd.Series(self.data.columns).values.reshape(int(ntime/self.sequence_length),self.sequence_length)\n",
    "        \n",
    "        examples=[self.getExamples(x,hours) for x,hours in zip(shapedData,timestamps)]\n",
    "\n",
    "        x,y,t = list(zip(*examples))\n",
    "        return np.concatenate(x), np.concatenate(y), np.concatenate(t)\n",
    "    \n",
    "    \n",
    "    def getIndexes(self,idx):\n",
    "        cx,cy= (idx +(self.input_lag+self.output_lag-1)*(idx//(self.sequence_length - self.input_lag-self.output_lag+1)),\\\n",
    "                idx +(self.input_lag+self.output_lag-1)*(idx//(self.sequence_length - self.input_lag-self.output_lag+1))+self.input_lag )\n",
    "        return (self.data.columns[cx:cy].values,self.data.columns[cy:cy+self.output_lag].values)\n",
    "    \n",
    "    def scaleMax(self):\n",
    "        self.__reversed_process.append(self.reverseScaleMax)\n",
    "        self.x/=self.max_value\n",
    "        if not self.y_only:\n",
    "            self.y/=self.max_value\n",
    "        \n",
    "    def scaleMinMax(self):\n",
    "        self.__reversed_process.append(self.reverseMinMaxScale)\n",
    "        self.min =self.x[:int(len(self.x)*(self.valid_split))].min()\n",
    "        self.max =self.x[:int(len(self.x)*(self.valid_split))].max()\n",
    "        diff = self.max - self.min\n",
    "        self.x = (self.x-self.min)/diff\n",
    "        self.y = (self.y-self.min)/diff\n",
    "\n",
    "        \n",
    "    def reverseMinMaxScale(self,x):\n",
    "        return x*(self.max-self.min)+self.min\n",
    "    \n",
    "    def reverseScaleMax(self,y):\n",
    "        return y*self.max_value\n",
    "\n",
    "        \n",
    "    def scaleLog(self):\n",
    "        self.__reversed_process.append(self.reverseScaleLog)\n",
    "        \n",
    "        self.x=np.log1p(self.x)\n",
    "        self.y=np.log1p(self.y)\n",
    "        \n",
    "    def reverseScaleLog(self,y):\n",
    "        return np.expm1(y)       \n",
    "        \n",
    "    def addTime(self):\n",
    "        self.__reversed_process.append(self.removeTime)\n",
    "        self.x=np.concatenate((self.x,self.t.reshape(-1,self.t.shape[1],1)),2)\n",
    "        \n",
    "        \n",
    "    def removeTime(self,y):\n",
    "        if y.shape == self.x.shape :\n",
    "            return np.delete(data_model.x,data_model.x.shape[2]-1,axis=2)\n",
    "        return y\n",
    "        \n",
    "    def shiftMean(self):\n",
    "        self.__reversed_process.append(self.resetMean)\n",
    "        self.means  =  self.data[self.data.columns[:(int(len(self.data.columns)*self.valid_split))]].mean(axis=1).values\n",
    "        if not self.y_only :\n",
    "            self.x-=self.means\n",
    "        self.y-=self.means\n",
    "        \n",
    "    def resetMean(self,y):\n",
    "        return y+self.means\n",
    "        \n",
    "        \n",
    "    def preprocessData(self):\n",
    "        if self.shift_mean :\n",
    "            self.shiftMean()\n",
    "        if self.scale_max :\n",
    "            self.scaleMax()\n",
    "        if self.scale_log :\n",
    "            self.scaleLog()\n",
    "        if self.min_max_scale : \n",
    "            self.scaleMinMax()\n",
    "        if self.add_time :\n",
    "            self.addTime()\n",
    "\n",
    "        if self.output_lag == 1 :\n",
    "            self.y=self.y.reshape(-1,self.y.shape[2])\n",
    "\n",
    "    def getRawYData(self,y):\n",
    "        \n",
    "        return reduce(lambda res, f:f(res), self.__reversed_process[::-1], y)\n",
    "\n",
    "    \n",
    "    def mse(self,p,y=None):\n",
    "        pred = self.getRawYData(p)\n",
    "        if y is not None :\n",
    "            raw_y = self.getRawYData(y)\n",
    "        else :\n",
    "            raw_y = self.getRawYData(self.y)\n",
    "        return np.mean((pred-raw_y)**2)\n",
    "    \n",
    "    def mae(self,p,y=None):\n",
    "        pred = self.getRawYData(p)\n",
    "        if y is not None :\n",
    "            raw_y = self.getRawYData(y)\n",
    "        else :\n",
    "            raw_y = self.getRawYData(self.y)\n",
    "        return np.mean(abs(pred-raw_y))\n",
    "    \n",
    "    def trainSplit(self):\n",
    "        \n",
    "        x_train = self.x[:int(len(self.x)*(self.valid_split))]\n",
    "        x_test = self.x[int(len(self.x)*(self.valid_split)):]\n",
    "        y_train = self.y[:int(len(self.x)*(self.valid_split))]\n",
    "        y_test = self.y[int(len(self.x)*(self.valid_split)):]\n",
    "        return x_train,y_train,x_test,y_test\n",
    "    \n",
    "    def getSplitSequences(self,values,sequence_length,skip=0):\n",
    "        def addNans(values,sequence_length,skip):\n",
    "\n",
    "            values=values.reshape(-1,sequence_length)\n",
    "            nans=np.array([np.nan]*(values.shape[0]*(skip+1))).reshape(values.shape[0],-1)\n",
    "            values = np.concatenate((values,nans),axis=1).reshape(-1)\n",
    "            return values\n",
    "        return addNans(np.arange(len(values)),sequence_length,skip), addNans(values,sequence_length,skip)\n",
    "    \n",
    "    def restorePredictionsAsDF(self,preds):\n",
    "        \n",
    "        index = [self.getIndexes(i)[1][0] for i in range(len(preds))]\n",
    "        df = pd.DataFrame(self.getRawYData(preds),index=index,columns=self.data.index)\n",
    "        return df.T\n",
    "    \n",
    "    def restoreXAsDF(self,x):\n",
    "        index = [self.getIndexes(i)[1][0] for i in range(len(x))]\n",
    "        df = pd.DataFrame(self.getRawYData(x).swapaxes(1,2).tolist(),index=index,columns=self.data.index)\n",
    "        return df.T\n",
    "    \n",
    "    def predict(self,split=\"full\"):\n",
    "        secondary_input = self.getDaysTypes()\n",
    "        if split.lower() == \"full\":\n",
    "            main_input = self.x\n",
    "            secondary_input = np.concatenate(secondary_input)\n",
    "        if split.lower() == \"train\":\n",
    "            main_input,*_ = self.trainSplit()\n",
    "            secondary_input=secondary_input[0]\n",
    "        if split.lower() == \"test\":\n",
    "            *_,main_input,_ = self.trainSplit()\n",
    "            secondary_input=secondary_input[1]\n",
    "            \n",
    "        if(len(self.model.input_shape)==1):\n",
    "            return self.model.predict(main_input)\n",
    "        \n",
    "        return self.model.predict([main_input,secondary_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillNaWithHistoricalValues(df):\n",
    "    oldIdx = df.columns\n",
    "    idx=[pd.to_datetime(df.columns.values).date,pd.to_datetime(df.columns.values).time]\n",
    "    mIdx=pd.MultiIndex.from_arrays(idx,names=['day','time'])\n",
    "    df.set_axis(mIdx,axis=1,inplace=True)\n",
    "    df = df.add(df.isna()*df.groupby(by=df.columns.get_level_values(1),axis=1).mean(),fill_value=0)\n",
    "    df.set_axis(oldIdx,axis=1,inplace=True)\n",
    "    return df\n",
    "def dropWeekends(data):\n",
    "    data.drop(data.columns[[ x.date().weekday()>=5 for x  in data.columns]],axis=1,inplace=True)\n",
    "#     data.drop(data.columns[[ x.date()==data.columns[0].date() for x  in data.columns]],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeds = pd.read_pickle('../data/speeds1419.pckl')\n",
    "counts = pd.read_pickle('../data/counts1419.pckl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropWeekends(speeds)\n",
    "dropWeekends(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeResults,segmentsMeta= osmMerger.mergeSegments(minValidData = osmMerger.MIN_VALID_DATA, weights=osmMerger.WEIGHTS,speedsMx=speeds)\n",
    "# mergeResults.to_pickle(\"mergeResults.pckl\")\n",
    "# segmentsMeta.to_pickle(\"segmentsMeta.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeResults=pd.read_pickle(\"../data/mergeResults.pckl\")\n",
    "segmentsMeta=pd.read_pickle(\"../data/segmentsMeta.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongoConnection import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments=OsmProcessing.getSegments(osmWays)\n",
    "segments = OsmProcessing.setOneWay(segments)\n",
    "osmMerger.mergeRoundaboutChunks(segments)\n",
    "segmentsMeta = OsmProcessing.buildSegmentsMeta(segments,linearOnly=True)\n",
    "osmMerger.removeRounabouts(segmentsMeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentsMeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentsMeta.loc[segmentsMeta.segmentID.isin(updatedSpeed.index.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentsMeta.loc[segmentsMeta.segmentID.isin(updatedSpeed.index.values)].apply(lambda x : (len(x['ins']),len(x['outs'])),axis = 1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedIndex=pd.Series(data=segmentsMeta.loc[mergeResults]['segmentID'].values,index = segmentsMeta['segmentID'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedSpeed = speeds.assign(newIndex =mergedIndex.reindex(speeds.index).values)\n",
    "updatedSpeed = updatedSpeed[~updatedSpeed.newIndex.isna()]\n",
    "updatedSpeed.groupby('newIndex').mean().to_pickle(\"rawUpdatedSpeed1419.pckl\")\n",
    "updatedSpeed=updatedSpeed.groupby('newIndex').mean().dropna(thresh = int(0.8*len(updatedSpeed.columns)))\n",
    "updatedSpeed.to_pickle(\"updatedSpeed1419.pckl\")\n",
    "updatedSpeed=fillNaWithHistoricalValues(updatedSpeed)\n",
    "updatedSpeed.to_pickle(\"updatedSpeedWithHistoricalValues1419.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawSpeed= fillNaWithHistoricalValues(speeds.groupby('matching_road').mean().dropna(thresh = int(0.8*len(speeds.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullSegs=speeds.groupby('matching_road').mean().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedcounts = counts.assign(newIndex =mergedIndex.reindex(counts.index).values)\n",
    "updatedcounts = updatedcounts[~updatedcounts.newIndex.isna()]\n",
    "updatedcounts = updatedcounts.groupby('newIndex').sum().loc[updatedSpeed.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMlpModel(loss,optimizer,nlayers,neuronsPerLayer,relu_output):\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for i in range(nlayers)  :    \n",
    "        model.add(tf.keras.layers.Dense(neuronsPerLayer*(i+1)))\n",
    "    model.add(tf.keras.layers.Dense(nSegments,name='out_layer'))\n",
    "    if relu_output :\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createStackedLstmModel(loss,optimizer,nlayers,neuronsPerLayer,relu_output):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #layers\n",
    "    for i in range(nlayers)  :    \n",
    "        model.add(tf.keras.layers.LSTM(neuronsPerLayer*(i+1),return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(nSegments,name='out_layer'))\n",
    "    if relu_output :\n",
    "        model.add(tf.keras.layers.ReLU())\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_grid= {'optimizer':['Adam'],\n",
    "              'nlayers':[0,1,3],\n",
    "              'neuronsPerLayer':[64],\n",
    "              'loss':['MSE','MAE'],\n",
    "              'relu_output':[False]\n",
    "             }\n",
    "\n",
    "train_grid={\n",
    "    \"batch_size\":[8,32,64], \n",
    "    \"epochs\":[100], \n",
    "    \"validation_split\":[0.5]\n",
    "}\n",
    "\n",
    "preprocess_grid={\n",
    "    'scale_Max':[True],\n",
    "    'scale_log':[False],\n",
    "    'input_lag':[1,6],\n",
    "    'output_lag':[1],\n",
    "    'time_as_input':[True],\n",
    "    \"validation_split\":[0.5]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParamsFromGrid(grid):\n",
    "    return [dict(list(zip(list(grid.keys()),x ))) for x in list(itertools.product(*list(grid.values())))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(data,buildfunc,build_grid,train_grid,preprocess_grid):\n",
    "\n",
    "    grid = list(itertools.product( getParamsFromGrid(build_grid),\n",
    "                                   getParamsFromGrid(train_grid),\n",
    "                                   getParamsFromGrid(preprocess_grid)\n",
    "                                 )\n",
    "               )\n",
    "    \n",
    "    losses=[]\n",
    "    val_losses=[]\n",
    "    mseLoss=[]\n",
    "    mseValLoss=[]\n",
    "    for build_param,train_param,preprocess_param in grid :\n",
    "        print(build_param,train_param,preprocess_param)\n",
    "        x,y = preprocessData(data,**preprocess_param)\n",
    "        xtrain = x[:int(len(x)*(train_param['validation_split']))]\n",
    "        xtest = x[int(len(x)*(train_param['validation_split'])):]\n",
    "        ytrain = y.reshape(-1,nSegments)[:int(len(x)*(train_param['validation_split']))]\n",
    "        ytest = y.reshape(-1,nSegments)[int(len(x)*(train_param['validation_split'])):]\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.keras.backend.set_session(tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=32, inter_op_parallelism_threads=32)))\n",
    "\n",
    "        model=buildfunc(**build_param)\n",
    "        model.fit(xtrain,ytrain.reshape(-1,nSegments),validation_data = (xtest,ytest), **train_param,verbose=0)\n",
    "        \n",
    "        losses.append(model.evaluate(xtrain,ytrain,verbose=0))\n",
    "        val_losses.append(model.evaluate(xtest,ytest,verbose=0))\n",
    "        \n",
    "        mseLoss.append(mse(reversePreProcess(ytrain,**preprocess_param),reversePreProcess(model.predict(xtrain),**preprocess_param)))\n",
    "        mseValLoss.append(mse(reversePreProcess(ytest,**preprocess_param),reversePreProcess(model.predict(xtest),**preprocess_param)))\n",
    "        print(losses[-1],val_losses[-1],mseLoss[-1],mseValLoss[-1])\n",
    "        \n",
    "    resDF=pd.DataFrame(list(zip(grid,losses,val_losses,mseLoss,mseValLoss)),columns=['params','loss_value','val_loss_value','mseLoss','mseValLoss'])\n",
    "    \n",
    "    return  pd.concat([resDF.params.apply(lambda x : pd.Series({**x[0],**x[1],**x[2]})),resDF[['loss_value','val_loss_value','mseLoss','mseValLoss']]],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resMlpDF = grid_search(data,createMlpModel,build_grid,train_grid,preprocess_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resLstmDF.groupby(\"time_as_input\")['mseLoss'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resLstmDF = grid_search(data,createStackedLstmModel,build_grid,train_grid,preprocess_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiInputModelMLP():\n",
    "    \"\"\"\n",
    "    __________________________________________________________________________________________________\n",
    "    Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "    ==================================================================================================\n",
    "    speed_input (InputLayer)        (None, 4, 748)       0                                            \n",
    "    __________________________________________________________________________________________________\n",
    "    flatten_multi_speed (Flatten)   (None, 2992)         0           speed_input[0][0]                \n",
    "    __________________________________________________________________________________________________\n",
    "    full_speed_mpl (Dense)          (None, 100)          299300      flatten_multi_speed[0][0]        \n",
    "    __________________________________________________________________________________________________\n",
    "    last_quarter_speeds (Lambda)    (None, 748)          0           speed_input[0][0]                \n",
    "    __________________________________________________________________________________________________\n",
    "    full_mlp_branch (Dense)         (None, 100)          10100       full_speed_mpl[0][0]             \n",
    "    __________________________________________________________________________________________________\n",
    "    slice_MLP_branch (Dense)        (None, 100)          74900       last_quarter_speeds[0][0]        \n",
    "    __________________________________________________________________________________________________\n",
    "    day_time_input (InputLayer)     (None, 2)            0                                            \n",
    "    __________________________________________________________________________________________________\n",
    "    Merge (Concatenate)             (None, 202)          0           full_mlp_branch[0][0]            \n",
    "                                                                     slice_MLP_branch[0][0]           \n",
    "                                                                     day_time_input[0][0]             \n",
    "    __________________________________________________________________________________________________\n",
    "    dense (Dense)                   (None, 100)          20300       Merge[0][0]                      \n",
    "    __________________________________________________________________________________________________\n",
    "    Output (Dense)                  (None, 748)          75548       dense[0][0]                      \n",
    "    ==================================================================================================\n",
    "    Total params: 480,148\n",
    "    Trainable params: 480,148\n",
    "    Non-trainable params: 0\n",
    "    \"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    flatten_layer = tf.keras.layers.Flatten(name=\"flatten_multi_speed\")(main_input)\n",
    "    mlp=tf.keras.layers.Dense(100,name=\"full_speed_mpl\")(flatten_layer)\n",
    "    mlp = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"full_mlp_branch\")(mlp)\n",
    "    lambda_slice_layer = tf.keras.layers.Lambda(lambda x : x[:,data_model.input_lag-1,:],name=\"last_quarter_speeds\")(main_input)\n",
    "    mlp_breanch = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"slice_MLP_branch\")(lambda_slice_layer)\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([mlp, mlp_breanch, daytime_input])\n",
    "    x=tf.keras.layers.Dense( 100, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\" )(x)\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse , optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiInputModelLstmWithLastQuarter():\n",
    "    \"\"\"\n",
    "        __________________________________________________________________________________________________\n",
    "        Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "        ==================================================================================================\n",
    "        speed_input (InputLayer)        (None, 4, 748)       0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        speed_lstm (LSTM)               (None, 50)           159800      speed_input[0][0]                \n",
    "        __________________________________________________________________________________________________\n",
    "        last_quarter_speeds (Lambda)    (None, 748)          0           speed_input[0][0]                \n",
    "        __________________________________________________________________________________________________\n",
    "        lstm_branch (Dense)             (None, 100)          5100        speed_lstm[0][0]                 \n",
    "        __________________________________________________________________________________________________\n",
    "        MLP_branch (Dense)              (None, 100)          74900       last_quarter_speeds[0][0]        \n",
    "        __________________________________________________________________________________________________\n",
    "        day_time_input (InputLayer)     (None, 2)            0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        Merge (Concatenate)             (None, 202)          0           lstm_branch[0][0]                \n",
    "                                                                         MLP_branch[0][0]                 \n",
    "                                                                         day_time_input[0][0]             \n",
    "        __________________________________________________________________________________________________\n",
    "        dense (Dense)                   (None, 100)          20300       Merge[0][0]                      \n",
    "        __________________________________________________________________________________________________\n",
    "        Output (Dense)                  (None, 748)          75548       dense[0][0]                      \n",
    "        ==================================================================================================\n",
    "        Total params: 335,648\n",
    "        Trainable params: 335,648\n",
    "        Non-trainable params: 0\n",
    "        __________________________________________________________________________________________________\n",
    "    \n",
    "    \"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    lstm=tf.keras.layers.LSTM(50,name=\"speed_lstm\")(main_input)\n",
    "    lstm_branch = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"lstm_branch\")(lstm)\n",
    "    lambda_slice_layer = tf.keras.layers.Lambda(lambda x : x[:,data_model.input_lag-1,:],name=\"last_quarter_speeds\")(main_input)\n",
    "    mlp_breanch = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"MLP_branch\")(lambda_slice_layer)\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([lstm_branch, mlp_breanch, daytime_input])\n",
    "    x=tf.keras.layers.Dense( 100, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\" )(x)\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse , optimizer=optimizer)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    lstm=tf.keras.layers.LSTM(100,name=\"speed_lstm\")(main_input)\n",
    "    lstm_branch = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"lstm_branch\")(lstm)\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([lstm, daytime_input])\n",
    "    x=tf.keras.layers.Dense( 100, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\" )(x)\n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse , optimizer=optimizer)   \n",
    "    data_model.model.summary()\n",
    "\"\"\"\n",
    "def multiInputModelLstm():\n",
    "    \"\"\"\n",
    "        __________________________________________________________________________________________________\n",
    "        Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "        ==================================================================================================\n",
    "        speed_input (InputLayer)        (None, 5, 748)       0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        speed_lstm (LSTM)               (None, 100)          339600      speed_input[0][0]                \n",
    "        __________________________________________________________________________________________________\n",
    "        day_time_input (InputLayer)     (None, 2)            0                                            \n",
    "        __________________________________________________________________________________________________\n",
    "        Merge (Concatenate)             (None, 102)          0           speed_lstm[0][0]                 \n",
    "                                                                         day_time_input[0][0]             \n",
    "        __________________________________________________________________________________________________\n",
    "        dense_15 (Dense)                (None, 100)          10300       Merge[0][0]                      \n",
    "        __________________________________________________________________________________________________\n",
    "        Output (Dense)                  (None, 748)          75548       dense_15[0][0]                   \n",
    "        ==================================================================================================\n",
    "        Total params: 425,448\n",
    "        Trainable params: 425,448\n",
    "        Non-trainable params: 0\n",
    "        __________________________________________________________________________________________________\n",
    "\n",
    "    \"\"\"\n",
    "    main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "    lstm=tf.keras.layers.LSTM(100,name=\"speed_lstm\")(main_input)\n",
    "    lstm_branch = tf.keras.layers.Dense(100,activation= tf.keras.activations.sigmoid,name=\"lstm_branch\")(lstm)\n",
    "    daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "    merge_layer = tf.keras.layers.Concatenate(1,name=\"Merge\")([lstm, daytime_input])\n",
    "    x=tf.keras.layers.Dense( 100, activation = tf.keras.activations.sigmoid )( merge_layer)\n",
    "    x=tf.keras.layers.Dense( nSegments, activation = tf.keras.activations.sigmoid )( x)\n",
    "    output_layer = tf.keras.layers.Dense(nSegments,name=\"Output\" )(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer] )\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=tf.keras.losses.mse, optimizer=optimizer)   \n",
    "    data_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSegments = len(updatedSpeed)\n",
    "input_lag, output_lag, sequence_length = 2, 1, 20\n",
    "valid_split = 0.6\n",
    "data_model = PredictionModel(updatedSpeed, input_lag, output_lag, sequence_length, valid_split=valid_split,shift_mean=True,min_max_scale=True)\n",
    "data_model.preprocessData()\n",
    "x_train, y_train, x_test, y_test = data_model.trainSplit()\n",
    "train_days, test_days= data_model.getDaysTypes()\n",
    "tf.keras.backend.clear_session()\n",
    "multiInputModelLstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.model.summary()\n",
    "modelHist = data_model.model.fit([x_train,train_days], y_train, validation_data = ([x_test,test_days], y_test), batch_size=28, epochs=75, verbose=1,shuffle=False)\n",
    "print(data_model.mse(data_model.predict('train'),y_train),data_model.mse(data_model.predict('test'),y_test))\n",
    "print(data_model.mae(data_model.predict('train'),y_train),data_model.mae(data_model.predict('test'),y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([x.date() for x in speeds.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mns = data_model.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "nSegments = len(updatedSpeed)\n",
    "input_lag, output_lag, sequence_length = 5, 1, 20\n",
    "valid_split = 0.7\n",
    "data_model = PredictionModel(updatedSpeed, input_lag, output_lag, sequence_length, valid_split=valid_split,min_max_scale=True)\n",
    "data_model.preprocessData()\n",
    "x_train, y_train, x_test, y_test = data_model.trainSplit()\n",
    "train_days, test_days= data_model.getDaysTypes()\n",
    "\n",
    "# xcounts,_=preprocessData(updatedcounts/updatedcounts.max(),input_lag=input_lag, output_lag=output_lag)\n",
    "# if add_counts :\n",
    "#     x=np.concatenate((x,xcounts),axis = 2)\n",
    "\n",
    "# tensorborad callbacks\n",
    "tfBoard=tf.keras.callbacks.TensorBoard(log_dir='./tflogs/'+time.ctime(), histogram_freq=10,write_grads=True,  \n",
    "          write_graph=True, write_images=True)\n",
    "\n",
    "# simple MLP model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "main_input = tf.keras.layers.Input(x_train.shape[1:],name=\"speed_input\")\n",
    "daytime_input = tf.keras.layers.Input(shape=train_days.shape[1:],name=\"day_time_input\")\n",
    "reshape_layer = tf.keras.layers.Reshape(x_train.shape[1:][::-1])(main_input)\n",
    "\n",
    "x = tf.keras.layers.LSTM(350)(main_input)\n",
    "\n",
    "\n",
    "merge_layer = tf.keras.layers.Concatenate(1, name=\"Merge\")([x, daytime_input])\n",
    "\n",
    "x=tf.keras.layers.Dense(350, activation = tf.keras.activations.relu )( merge_layer )\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(nSegments, name=\"Output\")(x)\n",
    "\n",
    "data_model.model = tf.keras.Model(inputs = [main_input,daytime_input], outputs= [output_layer])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "data_model.model.compile( loss=tf.keras.losses.mse, optimizer=optimizer ) #,metrics=['mse','msle','mae','mape'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelHist = data_model.model.fit([x_train,train_days], y_train, validation_data = ([x_test,test_days], y_test), batch_size=8, epochs=100, verbose=1,shuffle=False)\n",
    "data_model.mse(data_model.predict('train'),y_train),data_model.mse(data_model.predict('test'),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.mse(data_model.predict('train'),y_train),data_model.mse(data_model.predict('test'),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights shapes\n",
    "[x.shape for x in data_model.model.get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.mse(data_model.predict('train'),y_train),data_model.mse(data_model.predict('test'),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubPlots(data,pltFunc=plt.plot,figsize=(12,12),titles=None):\n",
    "    nCols= int(np.sqrt(len(data)))+1\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i,vals in enumerate(data):\n",
    "        plt.subplot(nCols,nCols,i+1)\n",
    "        plt.plot(vals)\n",
    "        if type(titles)!=type(None):\n",
    "            plt.title(titles[i])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createSubPlots(modelHist.history.values(),titles=list(modelHist.history.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forceDiscontinuity(values,sequenceLength,skip=0):\n",
    "    def addNans(values,sequenceLength,skip):\n",
    "        \n",
    "        values=values.reshape(-1,sequenceLength)\n",
    "        nans=np.array([np.nan]*(values.shape[0]*(skip+1))).reshape(values.shape[0],-1)\n",
    "        values = np.concatenate((values,nans),axis=1).reshape(-1)\n",
    "        return values\n",
    "    return addNans(np.arange(len(values)),sequenceLength,skip), addNans(values,sequenceLength,skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplePred = np.argsort(data_model.y.mean(axis=0)[0]-data_model.predict('full').mean(axis=0))[np.r_[:nSegments-1:20j].astype(int)]\n",
    "\n",
    "plt.figure(figsize=(24,36))\n",
    "preds = data_model.getRawYData(data_model.predict('full'))\n",
    "yys = data_model.getRawYData(data_model.y)\n",
    "for ix, xSample in enumerate(samplePred):\n",
    "    plt.subplot(len(samplePred),1,ix+1)\n",
    "    plt.plot(*data_model.getSplitSequences(yys[:,ix],data_model.sequence_length-data_model.input_lag,skip=data_model.input_lag))\n",
    "    plt.plot(*data_model.getSplitSequences(preds[:,ix],data_model.sequence_length-data_model.input_lag,skip=data_model.input_lag))\n",
    "\n",
    "    plt.axvline((data_model.valid_split)*data_model.x.shape[0],c='r')\n",
    "    plt.legend(['y','pred','validationSplit'])\n",
    "    plt.title(\" segment : {}\".format(xSample))\n",
    "plt.tight_layout()\n",
    "# preds = pd.concat([getPred(x,y,i,idx=30,mul=MUL) for i in range(len(x))],axis=1).T\n",
    "# plt.figure(figsize=(18,4))\n",
    "# plt.plot(preds['y'].values)\n",
    "# plt.plot(preds['pred'].values)\n",
    "# plt.axvline(0.75*x.shape[0],c='r')\n",
    "# plt.legend(['y','pred','validationSplit'])\n",
    "# plt.savefig(\"imgs/Dense instant overfit with counts and time.png\",dpi=1200,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPredictions(segments,data_model, yDF,predDF, timesteps,mergedIndex,updatedcounts,folium_map=None):\n",
    "    if folium_map == None :\n",
    "        folium_map = Plotting.getFoliumMap()\n",
    "    layers=[]\n",
    "    colors = ((np.abs(yDF.clip(lower=15) - predDF.clip(lower=15))+1)/(yDF.clip(lower=15)+1)).clip(upper=1)\n",
    "    laggedX = data_model.restoreXAsDF(data_model.x)\n",
    "    predSegs = segments[segments.segmentID.isin(mergedIndex[mergedIndex.isin(yDF.index)].index)]\n",
    "    segment_overall_mean = [data_model.data.mean(axis=1).loc[mergedIndex.loc[idx]] for idx in predSegs.segmentID]\n",
    "    segment_timestamp_mean = data_model.data.groupby(pd.DatetimeIndex(data_model.data.columns).time,axis=1).mean()\n",
    "    for t in timesteps :  \n",
    "        colorList=[colors[t].loc[mergedIndex.loc[idx]] for idx in predSegs.segmentID]\n",
    "        y= [yDF[t].loc[mergedIndex.loc[idx]] for idx in predSegs.segmentID]\n",
    "        preds = [predDF[t].loc[mergedIndex.loc[idx]] for idx in predSegs.segmentID]\n",
    "        segCounts=[updatedcounts[t].loc[mergedIndex.loc[idx]] for idx in predSegs.segmentID]\n",
    "        timestampLaggedX= [laggedX[t].loc[mergedIndex.loc[idx]] for idx in predSegs.segmentID]\n",
    "        \n",
    "        current_segment_timestamp_mean = [segment_timestamp_mean[t.time()].loc[mergedIndex.loc[idx]] for idx in predSegs.segmentID]\n",
    "        \n",
    "        popups = [\"segment : {:},<br> y : {:.2f},<br> pred : {:.2f},<br> %error : {:.0f}%,<br> count : {:}<br>mean: {:}<br>timestamp_mean: {:}<br>x: {:} \"\\\n",
    "                  .format(seg,yi,predi,props*100,count,mean,timestamp_mean,np.array(x).astype(int)) \n",
    "                  for seg,yi,predi,props,count,mean,timestamp_mean,x \n",
    "                  in zip(predSegs.segmentID,y,preds,colorList,segCounts,segment_overall_mean,current_segment_timestamp_mean,timestampLaggedX)]\n",
    "        pos = yDF.columns.get_loc(t)\n",
    "        layer = getPredictionLayer(predSegs,colorList,folium_map,str(t),popups)\n",
    "        layers.append(layer)\n",
    "        \n",
    "    return Plotting.stackHistotyLayers([*layers,folium.TileLayer()],folium_map)\n",
    "\n",
    "def getPredictionLayer(segments,colors,folium_map,name='layer',popups=[]):\n",
    "    layer = folium.plugins.FeatureGroupSubGroup(folium_map,name=name,show=False, overlay=False)\n",
    "    [folium.PolyLine(locations=[lo[::-1] for lo in x['coordinates']], color=matplotlib.colors.rgb2hex(plt.cm.brg_r(color/2)),popup=pop).add_to(layer) for x,color,pop in zip(segments['loc'],colors,popups)]\n",
    "    return layer\n",
    "\n",
    "yDF=data_model.restorePredictionsAsDF(data_model.y)\n",
    "predDF = data_model.restorePredictionsAsDF( data_model.predict('full'))\n",
    "\n",
    "\n",
    "# folium_map = plotPredictions(segmentsMeta,data_model, yDF, predDF, yDF.columns[np.r_[:len(yDF.columns)-1:10j].astype(int)],mergedIndex,updatedcounts)\n",
    "\n",
    "\n",
    "folium_map=folium.plugins.DualMap(location=[48.10301,-1.65537],\n",
    "                    zoom_start=13,\n",
    "                    tiles=\"OpenStreetMap\")\n",
    "folium.features.LinearColormap([plt.cm.brg_r(0),plt.cm.brg_r(0.5)],caption=\"Error rate\").add_to(folium_map.m1)\n",
    "\n",
    "folium_train_map = plotPredictions(segmentsMeta,data_model,yDF,predDF,yDF.columns[np.r_[8:14:1].astype(int)],mergedIndex,updatedcounts,folium_map=folium_map.m1)\n",
    "folium_validation_map = plotPredictions(segmentsMeta,data_model,yDF,predDF,yDF.columns[np.r_[len(yDF.columns)*valid_split+8:len(yDF.columns)*valid_split+14:1].astype(int)],mergedIndex,updatedcounts,folium_map=folium_map.m2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium_map.save('predsLstmTimeMerge0.6 6lokkup.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yDF=data_model.restorePredictionsAsDF(data_model.y)\n",
    "predDF = data_model.restorePredictionsAsDF( data_model.predict('full'))\n",
    "\n",
    "colors = ((np.abs(yDF - predDF)+1)/(yDF+1)).clip(upper=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPredHeatMap(yDF,predDF,cost=None):\n",
    "    if cost :\n",
    "        colors= cost(yDF,predDF)\n",
    "    else :\n",
    "        colors = ((np.abs(yDF - predDF)+1)/(yDF+1)).clip(upper=1)\n",
    "    \n",
    "    plt.figure(figsize=(18,12))\n",
    "    plt.imshow(colors,aspect='auto')\n",
    "    plt.colorbar()\n",
    "    \n",
    "plotPredHeatMap(yDF,predDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPredHeatMap(yDF,predDF,cost=None):\n",
    "    if cost :\n",
    "        colors= cost(yDF,predDF)\n",
    "    else :\n",
    "        colors = ((np.abs(yDF - predDF)+1)/(yDF+1)).clip(upper=1)\n",
    "    \n",
    "    plt.figure(figsize=(18,12))\n",
    "    plt.imshow(colors,aspect='auto')\n",
    "    plt.colorbar()\n",
    "    \n",
    "plotPredHeatMap(yDF,predDF)\n",
    "plt.savefig(\"imgs/Dense instant overfit with counts and time heatmap.png\",dpi=1200,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
