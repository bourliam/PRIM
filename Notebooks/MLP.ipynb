{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Input, Dense, Dropout, ReLU\n",
    "from tensorflow.keras.layers import Concatenate, Lambda\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# Printing\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../source/')\n",
    "sys.path.append('../scripts/')\n",
    "from predictionModel import PredictionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speeds = pd.read_pickle('data/speeds1419.pckl')\n",
    "#speeds80 = speeds.groupby('data/matching_road').mean().dropna(thresh = int(0.8*len(speeds.columns)))\n",
    "\n",
    "#counts = pd.read_pickle('data/counts1419.pckl')\n",
    "#mergeResults=pd.read_pickle(\"../data/mergeResults.pckl\")\n",
    "#segmentsMeta=pd.read_pickle(\"../data/segmentsMeta.pckl\")\n",
    "\n",
    "updatedSpeed = pd.read_pickle(\"../data/updatedSpeedWithHistoricalValues1419.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>time</th>\n",
       "      <th>2018-11-13 14:00:00</th>\n",
       "      <th>2018-11-13 14:15:00</th>\n",
       "      <th>2018-11-13 14:30:00</th>\n",
       "      <th>2018-11-13 14:45:00</th>\n",
       "      <th>2018-11-13 15:00:00</th>\n",
       "      <th>2018-11-13 15:15:00</th>\n",
       "      <th>2018-11-13 15:30:00</th>\n",
       "      <th>2018-11-13 15:45:00</th>\n",
       "      <th>2018-11-13 16:00:00</th>\n",
       "      <th>2018-11-13 16:15:00</th>\n",
       "      <th>...</th>\n",
       "      <th>2018-12-11 16:30:00</th>\n",
       "      <th>2018-12-11 16:45:00</th>\n",
       "      <th>2018-12-11 17:00:00</th>\n",
       "      <th>2018-12-11 17:15:00</th>\n",
       "      <th>2018-12-11 17:30:00</th>\n",
       "      <th>2018-12-11 17:45:00</th>\n",
       "      <th>2018-12-11 18:00:00</th>\n",
       "      <th>2018-12-11 18:15:00</th>\n",
       "      <th>2018-12-11 18:30:00</th>\n",
       "      <th>2018-12-11 18:45:00</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>newIndex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10275171_0</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>6.487879</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.8875</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.850000</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11027377_0</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>25.198313</td>\n",
       "      <td>22.916667</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>13.5000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>17.828788</td>\n",
       "      <td>...</td>\n",
       "      <td>12.766667</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>25.375000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11027379_0</th>\n",
       "      <td>80.166667</td>\n",
       "      <td>72.750000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>75.200000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>69.833333</td>\n",
       "      <td>63.666667</td>\n",
       "      <td>73.6250</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>70.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>55.250000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>75.666667</td>\n",
       "      <td>76.777778</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>89.750000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>71.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111437857_0</th>\n",
       "      <td>84.707242</td>\n",
       "      <td>89.400000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>76.333333</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>74.750000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>77.5000</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>24.625000</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>48.500000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>62.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111848631_0</th>\n",
       "      <td>102.746479</td>\n",
       "      <td>102.333333</td>\n",
       "      <td>101.183099</td>\n",
       "      <td>100.279070</td>\n",
       "      <td>101.642857</td>\n",
       "      <td>98.578313</td>\n",
       "      <td>109.395349</td>\n",
       "      <td>111.0000</td>\n",
       "      <td>109.882353</td>\n",
       "      <td>109.030303</td>\n",
       "      <td>...</td>\n",
       "      <td>108.959184</td>\n",
       "      <td>107.123457</td>\n",
       "      <td>106.111111</td>\n",
       "      <td>109.049180</td>\n",
       "      <td>112.061224</td>\n",
       "      <td>113.850575</td>\n",
       "      <td>113.764706</td>\n",
       "      <td>117.500000</td>\n",
       "      <td>116.323529</td>\n",
       "      <td>109.552632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "time         2018-11-13 14:00:00  2018-11-13 14:15:00  2018-11-13 14:30:00  \\\n",
       "newIndex                                                                     \n",
       "10275171_0              6.000000            13.500000             6.487879   \n",
       "11027377_0             38.000000            17.000000            11.500000   \n",
       "11027379_0             80.166667            72.750000            72.000000   \n",
       "111437857_0            84.707242            89.400000            94.000000   \n",
       "111848631_0           102.746479           102.333333           101.183099   \n",
       "\n",
       "time         2018-11-13 14:45:00  2018-11-13 15:00:00  2018-11-13 15:15:00  \\\n",
       "newIndex                                                                     \n",
       "10275171_0             12.333333            10.000000             2.500000   \n",
       "11027377_0             25.198313            22.916667             7.000000   \n",
       "11027379_0             75.200000            78.000000            69.833333   \n",
       "111437857_0            76.333333            87.500000            74.750000   \n",
       "111848631_0           100.279070           101.642857            98.578313   \n",
       "\n",
       "time         2018-11-13 15:30:00  2018-11-13 15:45:00  2018-11-13 16:00:00  \\\n",
       "newIndex                                                                     \n",
       "10275171_0             10.000000               3.8875             3.750000   \n",
       "11027377_0              4.666667              13.5000            46.500000   \n",
       "11027379_0             63.666667              73.6250            82.000000   \n",
       "111437857_0            95.000000              77.5000            44.500000   \n",
       "111848631_0           109.395349             111.0000           109.882353   \n",
       "\n",
       "time         2018-11-13 16:15:00  ...  2018-12-11 16:30:00  \\\n",
       "newIndex                          ...                        \n",
       "10275171_0              3.875000  ...             0.000000   \n",
       "11027377_0             17.828788  ...            12.766667   \n",
       "11027379_0             70.285714  ...            71.500000   \n",
       "111437857_0            19.000000  ...             6.800000   \n",
       "111848631_0           109.030303  ...           108.959184   \n",
       "\n",
       "time         2018-12-11 16:45:00  2018-12-11 17:00:00  2018-12-11 17:15:00  \\\n",
       "newIndex                                                                     \n",
       "10275171_0              7.500000             5.750000             2.285714   \n",
       "11027377_0              8.000000            20.000000            35.000000   \n",
       "11027379_0             69.000000            55.250000            72.000000   \n",
       "111437857_0            24.625000             8.333333            15.833333   \n",
       "111848631_0           107.123457           106.111111           109.049180   \n",
       "\n",
       "time         2018-12-11 17:30:00  2018-12-11 17:45:00  2018-12-11 18:00:00  \\\n",
       "newIndex                                                                     \n",
       "10275171_0             12.500000             4.000000             3.850000   \n",
       "11027377_0             25.375000            19.500000            38.000000   \n",
       "11027379_0             75.666667            76.777778            50.500000   \n",
       "111437857_0            15.000000            48.500000            86.000000   \n",
       "111848631_0           112.061224           113.850575           113.764706   \n",
       "\n",
       "time         2018-12-11 18:15:00  2018-12-11 18:30:00  2018-12-11 18:45:00  \n",
       "newIndex                                                                    \n",
       "10275171_0              1.444444             6.000000             3.000000  \n",
       "11027377_0              4.000000             0.000000            12.500000  \n",
       "11027379_0             89.750000            60.000000            71.125000  \n",
       "111437857_0            70.500000            55.000000            62.476190  \n",
       "111848631_0           117.500000           116.323529           109.552632  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updatedSpeed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (148, 748)\n",
      "y_train shape: (148, 748)\n",
      "x_test shape: (99, 748)\n",
      "y_test shape: (99, 748)\n"
     ]
    }
   ],
   "source": [
    "input_lag, output_lag, sequence_length, valid_split =  1, 1, 20, 0.6\n",
    "\n",
    "data_model = PredictionModel(updatedSpeed, input_lag, output_lag, sequence_length, valid_split= valid_split)\n",
    "data_model.preprocessData()\n",
    "x_train, y_train, x_test, y_test = data_model.trainSplit()\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMlpModel(loss='MSE',optimizer='Adam',nlayers=1, neuronsPerLayer=64,relu_output=False, nSegments=748):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuronsPerLayer, input_dim = nSegments, name='input_layer'))\n",
    "    \n",
    "    for i in range(nlayers-1):    \n",
    "        model.add(Dense(neuronsPerLayer))\n",
    "        \n",
    "    model.add(Dense(nSegments, name='out_layer'))\n",
    "    if relu_output :\n",
    "        model.add(ReLU())\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (Dense)          (None, 64)                47936     \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 748)               48620     \n",
      "=================================================================\n",
      "Total params: 96,556\n",
      "Trainable params: 96,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = createMlpModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMlpModel(loss='MSE',optimizer='Adam', nlayers=1, neuronsPerLayer=64, relu_output=False, nSegments=748):\n",
    "    \n",
    "    inputs = Input(shape=(nSegments,), name='input')\n",
    "    x = Flatten(name='flatten')(inputs)\n",
    "    \n",
    "    for i in range(nlayers):    \n",
    "        x = Dense(neuronsPerLayer/(i+1))(x)\n",
    "        \n",
    "    if relu_output :\n",
    "        predictions = Dense(nSegments, activation='relu', name='out_layer')(x)\n",
    "    else:\n",
    "        predictions = Dense(nSegments, name='out_layer')(x)\n",
    "        \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 748)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 748)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                47936     \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 748)               48620     \n",
      "=================================================================\n",
      "Total params: 96,556\n",
      "Trainable params: 96,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 148 samples, validate on 99 samples\n",
      "Epoch 1/40\n",
      "148/148 [==============================] - 0s 2ms/step - loss: 4338.1596 - val_loss: 3259.0622\n",
      "Epoch 2/40\n",
      "148/148 [==============================] - 0s 199us/step - loss: 2598.4082 - val_loss: 1598.6934\n",
      "Epoch 3/40\n",
      "148/148 [==============================] - 0s 243us/step - loss: 1208.2044 - val_loss: 671.5162\n",
      "Epoch 4/40\n",
      "148/148 [==============================] - 0s 235us/step - loss: 529.3186 - val_loss: 367.5148\n",
      "Epoch 5/40\n",
      "148/148 [==============================] - 0s 245us/step - loss: 353.4728 - val_loss: 303.3155\n",
      "Epoch 6/40\n",
      "148/148 [==============================] - 0s 228us/step - loss: 305.1104 - val_loss: 266.0585\n",
      "Epoch 7/40\n",
      "148/148 [==============================] - 0s 227us/step - loss: 268.6551 - val_loss: 230.5732\n",
      "Epoch 8/40\n",
      "148/148 [==============================] - 0s 226us/step - loss: 238.4866 - val_loss: 211.3901\n",
      "Epoch 9/40\n",
      "148/148 [==============================] - 0s 221us/step - loss: 224.4139 - val_loss: 206.3295\n",
      "Epoch 10/40\n",
      "148/148 [==============================] - 0s 225us/step - loss: 217.1198 - val_loss: 197.6272\n",
      "Epoch 11/40\n",
      "148/148 [==============================] - 0s 228us/step - loss: 211.5843 - val_loss: 195.9499\n",
      "Epoch 12/40\n",
      "148/148 [==============================] - 0s 263us/step - loss: 207.4442 - val_loss: 192.7145\n",
      "Epoch 13/40\n",
      "148/148 [==============================] - 0s 203us/step - loss: 203.4247 - val_loss: 191.3086\n",
      "Epoch 14/40\n",
      "148/148 [==============================] - 0s 247us/step - loss: 201.6482 - val_loss: 189.7058\n",
      "Epoch 15/40\n",
      "148/148 [==============================] - 0s 208us/step - loss: 199.3997 - val_loss: 188.5504\n",
      "Epoch 16/40\n",
      "148/148 [==============================] - 0s 255us/step - loss: 198.0052 - val_loss: 189.2052\n",
      "Epoch 17/40\n",
      "148/148 [==============================] - 0s 222us/step - loss: 196.9374 - val_loss: 188.7277\n",
      "Epoch 18/40\n",
      "148/148 [==============================] - 0s 228us/step - loss: 194.9142 - val_loss: 187.1064\n",
      "Epoch 19/40\n",
      "148/148 [==============================] - 0s 216us/step - loss: 192.8866 - val_loss: 184.6611\n",
      "Epoch 20/40\n",
      "148/148 [==============================] - 0s 227us/step - loss: 190.8717 - val_loss: 185.1003\n",
      "Epoch 21/40\n",
      "148/148 [==============================] - 0s 219us/step - loss: 188.3953 - val_loss: 182.0353\n",
      "Epoch 22/40\n",
      "148/148 [==============================] - 0s 221us/step - loss: 186.5541 - val_loss: 181.9816\n",
      "Epoch 23/40\n",
      "148/148 [==============================] - 0s 223us/step - loss: 184.8800 - val_loss: 180.6635\n",
      "Epoch 24/40\n",
      "148/148 [==============================] - 0s 213us/step - loss: 183.3691 - val_loss: 179.6188\n",
      "Epoch 25/40\n",
      "148/148 [==============================] - 0s 216us/step - loss: 180.6206 - val_loss: 180.3555\n",
      "Epoch 26/40\n",
      "148/148 [==============================] - 0s 193us/step - loss: 179.2524 - val_loss: 177.6357\n",
      "Epoch 27/40\n",
      "148/148 [==============================] - 0s 215us/step - loss: 176.2614 - val_loss: 176.2288\n",
      "Epoch 28/40\n",
      "148/148 [==============================] - 0s 257us/step - loss: 173.7376 - val_loss: 174.6242\n",
      "Epoch 29/40\n",
      "148/148 [==============================] - 0s 311us/step - loss: 171.4784 - val_loss: 173.6861\n",
      "Epoch 30/40\n",
      "148/148 [==============================] - 0s 324us/step - loss: 169.1671 - val_loss: 173.2799\n",
      "Epoch 31/40\n",
      "148/148 [==============================] - 0s 225us/step - loss: 166.7929 - val_loss: 171.0364\n",
      "Epoch 32/40\n",
      "148/148 [==============================] - 0s 208us/step - loss: 164.5812 - val_loss: 170.9350\n",
      "Epoch 33/40\n",
      "148/148 [==============================] - 0s 253us/step - loss: 163.4000 - val_loss: 168.8706\n",
      "Epoch 34/40\n",
      "148/148 [==============================] - 0s 251us/step - loss: 160.7660 - val_loss: 167.9942\n",
      "Epoch 35/40\n",
      "148/148 [==============================] - 0s 234us/step - loss: 158.6821 - val_loss: 167.6853\n",
      "Epoch 36/40\n",
      "148/148 [==============================] - 0s 205us/step - loss: 157.3137 - val_loss: 166.8431\n",
      "Epoch 37/40\n",
      "148/148 [==============================] - 0s 315us/step - loss: 156.1433 - val_loss: 166.2758\n",
      "Epoch 38/40\n",
      "148/148 [==============================] - 0s 205us/step - loss: 154.3993 - val_loss: 165.8621\n",
      "Epoch 39/40\n",
      "148/148 [==============================] - 0s 219us/step - loss: 153.1406 - val_loss: 165.5133\n",
      "Epoch 40/40\n",
      "148/148 [==============================] - 0s 234us/step - loss: 151.8771 - val_loss: 164.2595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc8303573c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_model.model = createMlpModel(nlayers=1, neuronsPerLayer=64, relu_output=False)\n",
    "data_model.model.summary()\n",
    "data_model.model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[  6.        ,  38.        ,  80.16666667, ..., 106.34375   ,\n         23.61507937,  41.33333333],\n       [ 13.5       ,  17.        ,  72.75      , ..., 106.25806452,\n         20.5       ,  4...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-38c87819bb3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0myDF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestorePredictionsAsDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestorePredictionsAsDF\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdata_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myDF\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myDF\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/CoursX/Telecom/PRIM/source/predictionModel.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmain_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msecondary_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     \u001b[0;31m# means that we end up calculating it twice which we should avoid.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1864\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 992\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    291\u001b[0m                        \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                        \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m       raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays: [array([[  6.        ,  38.        ,  80.16666667, ..., 106.34375   ,\n         23.61507937,  41.33333333],\n       [ 13.5       ,  17.        ,  72.75      , ..., 106.25806452,\n         20.5       ,  4..."
     ]
    }
   ],
   "source": [
    "yDF=data_model.restorePredictionsAsDF(data_model.y)\n",
    "predDF = data_model.restorePredictionsAsDF(data_model.predict('full'))\n",
    "\n",
    "colors = ((np.abs(yDF - predDF)+1)/(yDF+1)).clip(upper=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPredHeatMap(yDF,predDF,cost=None):\n",
    "    if cost :\n",
    "        colors= cost(yDF,predDF)\n",
    "    else :\n",
    "        colors = ((np.abs(yDF - predDF)+1)/(yDF+1)).clip(upper=1)\n",
    "    \n",
    "    plt.figure(figsize=(18,12))\n",
    "    plt.imshow(colors,aspect='auto')\n",
    "    plt.colorbar()\n",
    "    \n",
    "plotPredHeatMap(yDF,predDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
