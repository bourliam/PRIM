{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/dsi/mbouchouia/code/git/PRIM/Notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"40\" \n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"40\" \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"40\" \n",
    "os.nice(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countDF.mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import itertools\n",
    "from hyperopt import fmin, tpe, pyll, hp, Trials, STATUS_OK, STATUS_FAIL\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import pymongo\n",
    "import os\n",
    "sys.path.append('../scripts/')\n",
    "sys.path.append('../papers code/DCRNN/')\n",
    "import Plotting\n",
    "import folium\n",
    "import matplotlib\n",
    "import osmMerger\n",
    "import CustomUtils\n",
    "import OsmProcessing\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tf_cuda_subprocess import *\n",
    "from   functools import reduce\n",
    "import seaborn as sns\n",
    "import models\n",
    "from mongoConnection import *\n",
    "sns.set()\n",
    "results_path = \"../images/model results/\"\n",
    "def saveFig():\n",
    "    if not os.path.isdir(results_path+model_name):\n",
    "        os.makedirs(results_path+model_name)\n",
    "    print(results_path+model_name)\n",
    "    plt.savefig(results_path+model_name+input(),dpi=300,bbox_inches='tight')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# @RunAsCUDASubprocess(num_gpus=1)\n",
    "def createSession():\n",
    "    config = tf.ConfigProto(intra_op_parallelism_threads=32, \n",
    "                            inter_op_parallelism_threads=32,\n",
    "                            allow_soft_placement=True,\n",
    "                           )\n",
    "\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "    session = tf.Session(config=config,graph=tf.get_default_graph())\n",
    "\n",
    "    tf.keras.backend.set_session(session)\n",
    "createSession()\n",
    "oldModelPlotting=None\n",
    "from model_comparator import ModelCompare\n",
    "K = tf.keras.backend\n",
    "from sklearn import linear_model\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeds = pd.read_pickle(\"../data/monthsSpeed__0.pckl\").join(pd.read_pickle(\"../data/monthsSpeedMarJul__0.pckl\"))\n",
    "\n",
    "counts = pd.read_pickle('../data/monthsCount__0.pckl').join(pd.read_pickle('../data/monthsCountMarJul__0.pckl'))\n",
    "\n",
    "speeds.columns = pd.to_datetime(speeds.columns,utc=True).tz_localize(\"utc\").tz_convert(pytz.timezone(\"Europe/Paris\"))\n",
    "speeds = speeds[speeds.columns[((speeds.columns.time>=datetime.time(15, 0)) & (speeds.columns.time<datetime.time(20, 0) ))]]\n",
    "\n",
    "counts.columns = pd.to_datetime(counts.columns,utc=True).tz_localize(\"utc\").tz_convert(pytz.timezone(\"Europe/Paris\"))\n",
    "counts = counts[counts.columns[((counts.columns.time >= datetime.time(15, 0)) & (counts.columns.time<datetime.time(20, 0) ))]]\n",
    "\n",
    "\n",
    "\n",
    "# speeds = pd.read_pickle(\"../data/monthsSpeedfullday__0.pckl\")\n",
    "\n",
    "# counts = pd.read_pickle('../data/monthsCountfullday__0.pckl')\n",
    "\n",
    "\n",
    "mergeResults=pd.read_pickle(\"../data/mergeResults.pckl\")\n",
    "segmentsMeta=pd.read_pickle(\"../data/segmentsMeta.pckl\")\n",
    "mergedSegments=pd.read_pickle(\"../data/mergedSegments.pckl\")\n",
    "\n",
    "data_cleaner = models.DataCleaner(speeds,segmentsMeta,mergeResults,counts,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedsTest = data_cleaner.data[data_cleaner.data.columns[-300:]]\n",
    "data_cleaner.data=data_cleaner.data[data_cleaner.data.columns[:-300]]\n",
    "speedDF = data_cleaner.data\n",
    "\n",
    "\n",
    "countsTest = data_cleaner.counts[data_cleaner.counts.columns[-300:]]\n",
    "data_cleaner.counts=data_cleaner.counts[data_cleaner.counts.columns[:-300]]\n",
    "\n",
    "countDF = data_cleaner.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" we don't want to center using unseen data\"\"\"\n",
    "valid_split = 0.7\n",
    "true_sequence_length=len(set(pd.to_datetime(speedDF.columns).time))\n",
    "n_dates=len(set(pd.to_datetime(speedDF.columns).date)) \n",
    "train_dates_index = speedDF.columns[:int(valid_split*n_dates)*true_sequence_length]\n",
    "train_intercept = speedDF[train_dates_index].groupby(pd.to_datetime(train_dates_index).time,axis=1).mean()\n",
    "intercept_extended_df = pd.concat([train_intercept]*n_dates,axis=1)\n",
    "intercept_extended_df.columns=speedDF.columns\n",
    "# centering\n",
    "speedDF = speedDF-intercept_extended_df\n",
    "\n",
    "test_intercept = pd.concat([train_intercept]*(len(speedsTest.columns)//true_sequence_length),axis=1)\n",
    "test_intercept.columns = speedsTest.columns\n",
    "speedsTest = speedsTest- test_intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(speedDF+intercept_extended_df).mean().plot(use_index=False,figsize=(24,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = OsmProcessing.getAdjacencyMatrix(segmentsMeta)\n",
    "adjacency_matrix = adjacency_matrix.add(adjacency_matrix.T.values,fill_value=0)\n",
    "adjacency_matrix = OsmProcessing.addLevel(adjacency_matrix,3)\n",
    "adjacency_matrix = OsmProcessing.mergeAdjacencyMatrix(adjacency_matrix, mergeResults, segmentsMeta)\n",
    "adjacency_matrix = adjacency_matrix[speedDF.index].loc[speedDF.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_predictions = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" warning:  this takes a long time\"\"\"\n",
    "\n",
    "\n",
    "nSegments = len(speedDF)\n",
    "\n",
    "input_lag, output_lag, sequence_length = 1, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "intercept_data_model = models.DataModel(intercept_extended_df,input_lag,output_lag,sequence_length,valid_split=valid_split)\n",
    "intercept_data_model.preprocessData()\n",
    "\n",
    "params        = { \"scale_output\" : True,\"name\":\"OLS\" }\n",
    "\n",
    "data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params)\n",
    "\n",
    "data_model.preprocessData()\n",
    "\n",
    "x_train_00, y_train_00, x_test_00, y_test_00 = data_model.trainSplit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_test=x_test_00.reshape(x_test_00.shape[0:3:2])\n",
    "y_test=y_test_00\n",
    "\n",
    "x=x_train_00.reshape(x_train_00.shape[0:3:2])\n",
    "y=y_train_00\n",
    "\n",
    "\n",
    "A = [linear_model.LinearRegression() for i in range(nSegments)]\n",
    "\n",
    "\n",
    "for i in range(nSegments):\n",
    "    A[i].fit(x, y[:, i])    \n",
    "    \n",
    "preds = []\n",
    "\n",
    "for i in range(nSegments):\n",
    "    preds.append(A[i].predict(x_test))\n",
    "\n",
    "preds = np.array(preds)    \n",
    "    \n",
    "models_predictions[\"OLS\" ] = data_model.restorePredictionsAsDF(preds.T,'test')\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"MSE:\", mean_squared_error(preds.T.flatten(), y_test.flatten()))\n",
    "print(\"MAE:\", mean_absolute_error(preds.T.flatten(), y_test.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lag, output_lag, sequence_length = 1, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "params        = { \"scale_output\" : True,\"name\":\"OLS\" }\n",
    "test_data_model    = models.DataModel( speedsTest,    input_lag, output_lag, sequence_length, valid_split = 1, **params)\n",
    "test_data_model.preprocessData()\n",
    "test_data_x, test_data_y, _, _ = test_data_model.trainSplit()\n",
    "\n",
    "x_test=test_data_x.reshape(test_data_x.shape[0:3:2])\n",
    "y_test=test_data_y\n",
    "preds=[]\n",
    "for i in range(nSegments):\n",
    "    preds.append(A[i].predict(x_test))\n",
    "\n",
    "preds = np.array(preds)    \n",
    "\n",
    "models_predictions[\"OLS\" ] = test_data_model.restorePredictionsAsDF(preds.T)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(preds.T.flatten(), y_test.flatten()))\n",
    "print(\"MAE:\", mean_absolute_error(preds.T.flatten(), y_test.flatten()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"warning:  this takes a long time\"\"\"\n",
    "\n",
    "\n",
    "nSegments = len(speedDF)\n",
    "\n",
    "input_lag, output_lag, sequence_length = 1, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "intercept_data_model = models.DataModel(intercept_extended_df,input_lag,output_lag,sequence_length,valid_split=valid_split)\n",
    "intercept_data_model.preprocessData()\n",
    "\n",
    "params        = { \"scale_output\" : True,\"name\":\"Lasso-OLS\" }\n",
    "\n",
    "data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params)\n",
    "\n",
    "data_model.preprocessData()\n",
    "\n",
    "x_train_00, y_train_00, x_test_00, y_test_00 = data_model.trainSplit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_test=x_test_00.reshape(x_test_00.shape[0:3:2])\n",
    "y_test=y_test_00\n",
    "\n",
    "x=x_train_00.reshape(x_train_00.shape[0:3:2])\n",
    "y=y_train_00\n",
    "\n",
    "\n",
    "A_lasso = [linear_model.LassoCV(n_jobs=5, cv=5, max_iter=10000) for i in range(nSegments)]\n",
    "\n",
    "\n",
    "for i in range(nSegments):\n",
    "    A_lasso[i].fit(x, y[:, i])\n",
    "    print(i,\"n iter:\", A_lasso[i].n_iter_)\n",
    "    \n",
    "    \n",
    "preds_lasso = []\n",
    "\n",
    "for i in range(nSegments):\n",
    "    preds_lasso.append(A_lasso[i].predict(x_test))\n",
    "\n",
    "preds_lasso = np.array(preds_lasso)    \n",
    "    \n",
    "models_predictions[\"Lasso-OLS\" ] = data_model.restorePredictionsAsDF(preds_lasso.T,'test')\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"Avec CV\")\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(preds_lasso.T.flatten(), y_test.flatten()))\n",
    "print(\"MAE:\", mean_absolute_error(preds_lasso.T.flatten(), y_test.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = [A_lasso[i].n_iter_ for i in range(556)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lag, output_lag, sequence_length = 1, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "params        = { \"scale_output\" : True,\"name\":\"Lasso-OLS\" }\n",
    "\n",
    "test_data_model    = models.DataModel( speedsTest,    input_lag, output_lag, sequence_length, valid_split = 1, **params)\n",
    "test_data_model.preprocessData()\n",
    "test_data_x, test_data_y, _, _ = test_data_model.trainSplit()\n",
    "\n",
    "x_test=test_data_x.reshape(test_data_x.shape[0:3:2])\n",
    "y_test=test_data_y\n",
    "preds=[]\n",
    "for i in range(nSegments):\n",
    "    preds.append(A_lasso[i].predict(x_test))\n",
    "\n",
    "preds = np.array(preds)    \n",
    "models_predictions[\"Lasso-OLS\" ]= test_data_model.restorePredictionsAsDF(preds.T)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(preds.T.flatten(), y_test.flatten()))\n",
    "print(\"MAE:\", mean_absolute_error(preds.T.flatten(), y_test.flatten()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS LASSO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time(df,t_interval_length = 2,overlapping = True):\n",
    "    t_index  = np.unique(pd.to_datetime(df.columns).time)\n",
    "    index_subsets = np.split(t_index,range(t_interval_length,len(t_index),t_interval_length)) if not overlapping else np.array([np.array([t_index[i]  for i in range(j,t_interval_length+j)]) for j in range(len(t_index)-t_interval_length+1)])\n",
    "    index_subsets = [ pd.Index(pd.to_datetime(df.columns).time).isin(subset) for subset in index_subsets]\n",
    "    return index_subsets\n",
    "\n",
    "t_interval_length = 2\n",
    "overlapping = True\n",
    "t_index  = np.unique(pd.to_datetime(speedDF.columns).time)\n",
    "\n",
    "index_subsets =  split_time(speedDF,t_interval_length,overlapping )\n",
    "all_models=[]\n",
    "K = tf.keras.backend\n",
    "\n",
    "A_t = []\n",
    "losses = []\n",
    "maes=[]\n",
    "full_preds,full_y = [],[]\n",
    "try:\n",
    "    err_rates\n",
    "except Exception:\n",
    "    err_rates =np.zeros(len(index_subsets))\n",
    "\n",
    "for i,subset in enumerate(index_subsets) : \n",
    "    print(\"subset \"+ str(i))\n",
    "    X = speedDF[speedDF.columns[subset]].copy()\n",
    "    nSegments = len(X)\n",
    "    input_lag, output_lag, sequence_length = 1, 1, t_interval_length # speedDF.columns.size\n",
    "    \n",
    "    time_intercept = intercept_extended_df[intercept_extended_df.columns[subset]].copy()\n",
    "    time_intercept_model=models.DataModel(time_intercept, input_lag, output_lag, sequence_length, valid_split = valid_split,scale_output=True)\n",
    "\n",
    "    params        = { \"scale_output\" : True}\n",
    "    \n",
    "    \n",
    "    A_t_model    = models.DataModel( X,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params)                                                \n",
    "\n",
    "    A_t_model.preprocessData()\n",
    "\n",
    "    A_t_x_train_00, A_t_y_train_00, A_t_x_test_00, A_t_y_test_00 = A_t_model.trainSplit()\n",
    " \n",
    "    x_test=A_t_x_test_00.reshape(A_t_x_test_00.shape[0:3:2])\n",
    "    y_test=A_t_y_test_00\n",
    "\n",
    "    x=A_t_x_train_00.reshape(A_t_x_train_00.shape[0:3:2])\n",
    "    y=A_t_y_train_00\n",
    "\n",
    "\n",
    "    A_lasso = [linear_model.LassoCV(n_jobs=5, cv=5, max_iter=10000) for i in range(nSegments)]\n",
    "\n",
    "\n",
    "    for i in range(nSegments):\n",
    "        A_lasso[i].fit(x, y[:, i])\n",
    "        print(i,\"n iter:\", A_lasso[i].n_iter_)\n",
    "\n",
    "    all_models.append(A_lasso)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time(df,t_interval_length = 2,overlapping = True):\n",
    "    t_index  = np.unique(pd.to_datetime(df.columns).time)\n",
    "    index_subsets = np.split(t_index,range(t_interval_length,len(t_index),t_interval_length)) if not overlapping else np.array([np.array([t_index[i]  for i in range(j,t_interval_length+j)]) for j in range(len(t_index)-t_interval_length+1)])\n",
    "    index_subsets = [ pd.Index(pd.to_datetime(df.columns).time).isin(subset) for subset in index_subsets]\n",
    "    return index_subsets\n",
    "\n",
    "t_interval_length = 2\n",
    "overlapping = True\n",
    "t_index  = np.unique(pd.to_datetime(speedsTest.columns).time)\n",
    "\n",
    "index_subsets =  split_time(speedsTest,t_interval_length,overlapping )\n",
    "K = tf.keras.backend\n",
    "\n",
    "A_t = []\n",
    "losses = []\n",
    "maes=[]\n",
    "full_preds,full_y = [],[]\n",
    "\n",
    "try:\n",
    "    err_rates\n",
    "except Exception:\n",
    "    err_rates =np.zeros(len(index_subsets))\n",
    "\n",
    "for i,subset in enumerate(index_subsets) : \n",
    "    print(\"subset \"+ str(i))\n",
    "    X = speedsTest[speedsTest.columns[subset]].copy()\n",
    "    nSegments = len(X)\n",
    "    input_lag, output_lag, sequence_length = 1, 1, t_interval_length # speedDF.columns.size\n",
    "    \n",
    "    params        = { \"scale_output\" : True}\n",
    "    \n",
    "    \n",
    "    A_t_model    = models.DataModel( X,    input_lag, output_lag, sequence_length, valid_split = 1, **params)                                                \n",
    "\n",
    "    A_t_model.preprocessData()\n",
    "\n",
    "    A_t_x_train_00, A_t_y_train_00, A_t_x_test_00, A_t_y_test_00 = A_t_model.trainSplit()\n",
    " \n",
    "    x_test=A_t_x_train_00.reshape(A_t_x_train_00.shape[0:3:2])\n",
    "    y_test=A_t_y_train_00\n",
    "\n",
    "    preds=[]\n",
    "    A_lasso = all_models[i]\n",
    "    for j in range(nSegments):\n",
    "        preds.append(A_lasso[j] .predict(x_test))\n",
    "    preds = np.array(preds)    \n",
    "    full_preds.append(A_t_model.restorePredictionsAsDF(preds.T))\n",
    "\n",
    "    print(\"MSE:\", mean_squared_error(preds.T.flatten(), y_test.flatten()))\n",
    "    print(\"MAE:\", mean_absolute_error(preds.T.flatten(), y_test.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_predictions[\"TS-OLS\" ]= pd.concat(full_preds,axis=1).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regime switching lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarters[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(speedDF,t=2):\n",
    "    quarters = np.unique(pd.to_datetime(speedDF.columns).time)\n",
    "    quarters.sort()\n",
    "    quarter_split=quarters[t]\n",
    "    return{ True : speedDF.groupby(pd.to_datetime(speedDF.columns).time<=quarter_split,axis=1).get_group(True),False : speedDF.groupby(pd.to_datetime(speedDF.columns).time>=quarter_split,axis=1).get_group(True)}\n",
    "\n",
    "\n",
    "split_t=13\n",
    "left_speed_df,right_speed_df =  split_data(speedDF,13)[True],split_data(speedDF,13)[False]\n",
    "left_speed_test_df,right_speed_test_df =  split_data(speedsTest,13)[True], split_data(speedsTest,13)[False]\n",
    "\n",
    "nSegments = len(speedDF)\n",
    "\n",
    "\n",
    "def lassoModel(df,test_df):\n",
    "    input_lag, output_lag, sequence_length = 1, 1, len(set(pd.to_datetime(df.columns).time)) # speedDF.columns.size\n",
    "    data_model    = models.DataModel( df,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params)\n",
    "    data_model.preprocessData()\n",
    "    x_train_00, y_train_00, x_test_00, y_test_00 = data_model.trainSplit()\n",
    "    x_test=x_test_00.reshape(x_test_00.shape[0:3:2])\n",
    "    y_test=y_test_00\n",
    "\n",
    "    x=x_train_00.reshape(x_train_00.shape[0:3:2])\n",
    "    y=y_train_00\n",
    "    A_lasso = [linear_model.LassoCV(n_jobs=5, cv=5, max_iter=10000, tol=0.001) for i in range(nSegments)]\n",
    "\n",
    "    for i in range(nSegments):\n",
    "        A_lasso[i].fit(x, y[:, i])\n",
    "        print(i,\"n iter:\", A_lasso[i].n_iter_)\n",
    "    \n",
    "    \n",
    "    \n",
    "    input_lag, output_lag, sequence_length = 1, 1, len(set(pd.to_datetime(test_df.columns).time)) # speedDF.columns.size\n",
    "\n",
    "    test_data_model    = models.DataModel( test_df,    input_lag, output_lag, sequence_length, valid_split = 1)\n",
    "    test_data_model.preprocessData()\n",
    "    test_data_x, test_data_y, _, _ = test_data_model.trainSplit()\n",
    "\n",
    "    x_test=test_data_x.reshape(test_data_x.shape[0:3:2])\n",
    "    y_test=test_data_y\n",
    "    preds=[]\n",
    "    for i in range(nSegments):\n",
    "        preds.append(A_lasso[i].predict(x_test))\n",
    "\n",
    "    preds = np.array(preds)    \n",
    "    preds_df= test_data_model.restorePredictionsAsDF(preds.T)\n",
    "\n",
    "    print(\"MSE:\", mean_squared_error(preds.T.flatten(), y_test.flatten()))\n",
    "    print(\"MAE:\", mean_absolute_error(preds.T.flatten(), y_test.flatten()))\n",
    "    \n",
    "    return A_lasso,preds_df\n",
    "\n",
    "left_lasso_model,  left_lasso_preds  = lassoModel(left_speed_df,left_speed_test_df)\n",
    "right_lasso_model, right_lasso_preds = lassoModel(right_speed_df,right_speed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_predictions[\"RSM\" ]=pd.concat([left_lasso_preds,right_lasso_preds],axis=1).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name, model in models_predictions.items():\n",
    "#     model.to_pickle(\"pred results/\"+model_name+\"preds.pckl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep MLP (L1, Batch normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepMLPLassoNormalized(data_model,x_train_00,rate):\n",
    "    \n",
    "    \"\"\"\n",
    "        params        = { \"scale_output\" : True,\"name\":\"Lasso\",\"scale_max\":True }\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    main_input = tf.keras.layers.Input( x_train_00.shape[1:], name=\"speed_input\")\n",
    "    \n",
    "    flatten_input = tf.keras.layers.Lambda(lambda x : x[:,-1,:])(main_input )\n",
    "#     norm=tf.keras.layers.BatchNormalization()(flatten_input)\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(nSegments, activation=\"tanh\",kernel_regularizer = tf.keras.regularizers.l1(rate))(flatten_input)\n",
    "    norm = tf.keras.layers.BatchNormalization()(hidden)\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(nSegments, activation=\"tanh\",kernel_regularizer = tf.keras.regularizers.l1(rate))(norm)\n",
    "\n",
    "    output_layer = tf.keras.layers.Dense(nSegments, name=\"speed_lstm\",kernel_regularizer = tf.keras.regularizers.l1(rate))( hidden)\n",
    "    \n",
    "    data_model.model = tf.keras.Model(inputs = [main_input], outputs= [output_layer])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=\"mse\", optimizer=optimizer,metrics=[\"mse\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSegments = len(speedDF)\n",
    "\n",
    "input_lag, output_lag, sequence_length = 1, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "intercept_data_model = models.DataModel(intercept_extended_df,input_lag,output_lag,sequence_length,valid_split=valid_split)\n",
    "intercept_data_model.preprocessData()\n",
    "\n",
    "params        = { \"scale_output\" : True,\"name\":\"MLP\",\"scale_max\":True }\n",
    "\n",
    "data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params)\n",
    "\n",
    "data_model.preprocessData()\n",
    "\n",
    "x_train_00, y_train_00, x_test_00, y_test_00 = data_model.trainSplit()\n",
    "\n",
    "count_data = models.DataModel( countDF, input_lag, output_lag, sequence_length, valid_split = valid_split)\n",
    "\n",
    "count_train_00, _, count_test_00, _ = count_data.trainSplit()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "deepMLPLassoNormalized(data_model, x_train_00, rate = 2e-6)\n",
    "\n",
    "reduce_lr  = tf.keras.callbacks.ReduceLROnPlateau( monitor = 'val_mean_squared_error', factor = 0.5, verbose=1, patience=5, cooldown=5)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping( monitor='val_mean_squared_error', patience = 15, restore_best_weights=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "modelHist = data_model.model.fit([x_train_00], [y_train_00], validation_data=([x_test_00],[y_test_00]), batch_size=batch_size,epochs=400, callbacks=[reduce_lr,early_stop], verbose=0)\n",
    "print(data_model.mse(data_model.predict('train'), y_train_00), data_model.mse(data_model.predict('test'),y_test_00))\n",
    "print(data_model.mae(data_model.predict('train'), y_train_00), data_model.mae(data_model.predict('test'),y_test_00))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lag, output_lag, sequence_length = 1, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "params        = { \"scale_output\" : True,\"name\":\"MLP\",\"scale_max\":True }\n",
    "\n",
    "test_data_model    = models.DataModel( speedsTest,    input_lag, output_lag, sequence_length, valid_split = 1, **params)\n",
    "test_data_model.preprocessData()\n",
    "test_data_x, test_data_y, _, _ = test_data_model.trainSplit()\n",
    "\n",
    "print(data_model.mse(data_model.predict('custom',x=test_data_x), test_data_y))\n",
    "print(data_model.mae(data_model.predict('custom',x=test_data_x), test_data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_predictions[data_model.name] = data_model.restorePredictionsAsDF(data_model.predict('test'),'test')\n",
    "models_predictions[data_model.name] = test_data_model.restorePredictionsAsDF(data_model.predict('custom',x=test_data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_max = 0.1\n",
    "# eps=1e-5\n",
    "# n_alphas=50\n",
    "# alphas =np.logspace(np.log10(alpha_max * eps), np.log10(alpha_max),\n",
    "#                        num=n_alphas)[::-1]\n",
    "# mlpResults=[]\n",
    "# for alpha in alphas :\n",
    "#     nSegments = len(speedDF)\n",
    "\n",
    "#     input_lag, output_lag, sequence_length = 1, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "#     intercept_data_model = models.DataModel(intercept_extended_df,input_lag,output_lag,sequence_length,valid_split=valid_split)\n",
    "#     intercept_data_model.preprocessData()\n",
    "\n",
    "#     params        = { \"scale_output\" : True,\"name\":\"MLP\",\"scale_max\":True }\n",
    "\n",
    "#     data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params)\n",
    "\n",
    "#     data_model.preprocessData()\n",
    "\n",
    "#     x_train_00, y_train_00, x_test_00, y_test_00 = data_model.trainSplit()\n",
    "\n",
    "#     count_data = models.DataModel( countDF, input_lag, output_lag, sequence_length, valid_split = valid_split)\n",
    "\n",
    "#     count_train_00, _, count_test_00, _ = count_data.trainSplit()\n",
    "\n",
    "#     tf.keras.backend.clear_session()\n",
    "\n",
    "#     # deepMLPLassoNormalized(data_model, x_train_00, rate = 0.54e-5)\n",
    "# #     deepMLPLassoNormalized(data_model, x_train_00, rate = 0.24e-5)\n",
    "#     deepMLPLassoNormalized(data_model, x_train_00, rate = alpha)\n",
    "\n",
    "#     reduce_lr  = tf.keras.callbacks.ReduceLROnPlateau( monitor = 'val_mean_squared_error', factor = 0.5, verbose=1, patience=5, cooldown=5)\n",
    "#     early_stop = tf.keras.callbacks.EarlyStopping( monitor='val_mean_squared_error', patience = 15, restore_best_weights=True)\n",
    "\n",
    "#     batch_size = 8\n",
    "\n",
    "#     modelHist = data_model.model.fit([x_train_00], [y_train_00], validation_data=([x_test_00],[y_test_00]), batch_size=batch_size,epochs=400, callbacks=[reduce_lr,early_stop], verbose=0)\n",
    "#     print(data_model.mse(data_model.predict('train'), y_train_00), data_model.mse(data_model.predict('test'),y_test_00))\n",
    "#     print(data_model.mae(data_model.predict('train'), y_train_00), data_model.mae(data_model.predict('test'),y_test_00))\n",
    "#     mlpResults.append((alpha,data_model.mse(data_model.predict('test'),y_test_00),data_model.mae(data_model.predict('test'),y_test_00)))\n",
    "# data_model.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmLassoNormalized(data_model,x_train_00,rate):\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    main_input = tf.keras.layers.Input( x_train_00.shape[1:], name=\"speed_input\")\n",
    "    \n",
    "\n",
    "    hidden = tf.keras.layers.LSTM(200,activation=\"tanh\",kernel_regularizer = tf.keras.regularizers.l1(rate))(main_input)\n",
    "\n",
    "\n",
    "    output_layer=tf.keras.layers.Dense(nSegments, name=\"speed_lstm\",kernel_regularizer = tf.keras.regularizers.l1(rate))(hidden)\n",
    "    \n",
    "    data_model.model = tf.keras.Model(inputs = [main_input], outputs= [output_layer])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    data_model.model.compile(loss=\"mse\", optimizer=optimizer,metrics=[\"mse\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSegments = len(speedDF)\n",
    "\n",
    "input_lag, output_lag, sequence_length = 2, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "intercept_data_model = models.DataModel(intercept_extended_df,input_lag,output_lag,sequence_length,valid_split=valid_split)\n",
    "intercept_data_model.preprocessData()\n",
    "\n",
    "params        = { \"scale_output\" : True,\"name\":\"FC-\"+str(input_lag)+\"LSTM\",\"scale_max\":True }\n",
    "\n",
    "data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params)\n",
    "\n",
    "data_model.preprocessData()\n",
    "\n",
    "x_train_00, y_train_00, x_test_00, y_test_00 = data_model.trainSplit()\n",
    "\n",
    "count_data = models.DataModel( countDF, input_lag, output_lag, sequence_length, valid_split = valid_split)\n",
    "\n",
    "count_train_00, _, count_test_00, _ = count_data.trainSplit()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#     lstmLassoNormalized(data_model,x_train_00,rate=0.08e-5)\n",
    "lstmLassoNormalized(data_model,x_train_00,rate=8e-7)\n",
    "\n",
    "reduce_lr  = tf.keras.callbacks.ReduceLROnPlateau( monitor = 'val_mean_squared_error', factor = 0.5, verbose=1, patience=5, cooldown=5)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping( monitor='val_mean_squared_error', patience = 20, restore_best_weights=True)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "modelHist = data_model.model.fit([x_train_00], [y_train_00], validation_data=([x_test_00],[y_test_00]), batch_size=batch_size,epochs=400, verbose=1,callbacks=[reduce_lr,early_stop])\n",
    "print(data_model.mse(data_model.predict('train'), y_train_00), data_model.mse(data_model.predict('test'),y_test_00))\n",
    "print(data_model.mae(data_model.predict('train'), y_train_00), data_model.mae(data_model.predict('test'),y_test_00))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lag, output_lag, sequence_length = 2, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "params        = { \"scale_output\" : True,\"name\":\"FC-\"+str(input_lag)+\"LSTM\",\"scale_max\":True }\n",
    "\n",
    "test_data_model    = models.DataModel( speedsTest,    input_lag, output_lag, sequence_length, valid_split = 1, **params)\n",
    "test_data_model.preprocessData()\n",
    "test_data_x, test_data_y, _, _ = test_data_model.trainSplit()\n",
    "\n",
    "print(data_model.mse(data_model.predict('custom',x=test_data_x), test_data_y))\n",
    "print(data_model.mae(data_model.predict('custom',x=test_data_x), test_data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_predictions[data_model.name] = test_data_model.restorePredictionsAsDF(data_model.predict('custom',x=test_data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_max = 5e-7\n",
    "# eps=2e-1\n",
    "# n_alphas=50\n",
    "# alphas =np.logspace(np.log10(alpha_max * eps), np.log10(alpha_max),\n",
    "#                        num=n_alphas)[::-1]\n",
    "# lstmResults=[]\n",
    "# for alpha in alphas :\n",
    "    \n",
    "#     nSegments = len(speedDF)\n",
    "\n",
    "#     input_lag, output_lag, sequence_length = 2, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "#     intercept_data_model = models.DataModel(intercept_extended_df,input_lag,output_lag,sequence_length,valid_split=valid_split)\n",
    "#     intercept_data_model.preprocessData()\n",
    "\n",
    "#     params        = { \"scale_output\" : True,\"name\":\"FC-\"+str(input_lag)+\"LSTM\",\"scale_max\":True }\n",
    "\n",
    "#     data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params)\n",
    "\n",
    "#     data_model.preprocessData()\n",
    "\n",
    "#     x_train_00, y_train_00, x_test_00, y_test_00 = data_model.trainSplit()\n",
    "\n",
    "#     count_data = models.DataModel( countDF, input_lag, output_lag, sequence_length, valid_split = valid_split)\n",
    "\n",
    "#     count_train_00, _, count_test_00, _ = count_data.trainSplit()\n",
    "\n",
    "#     tf.keras.backend.clear_session()\n",
    "\n",
    "# #     lstmLassoNormalized(data_model,x_train_00,rate=0.08e-5)\n",
    "#     lstmLassoNormalized(data_model,x_train_00,rate=alpha)\n",
    "\n",
    "#     reduce_lr  = tf.keras.callbacks.ReduceLROnPlateau( monitor = 'val_mean_squared_error', factor = 0.5, verbose=1, patience=5, cooldown=5)\n",
    "#     early_stop = tf.keras.callbacks.EarlyStopping( monitor='val_mean_squared_error', patience = 20, restore_best_weights=True)\n",
    "\n",
    "#     batch_size = 8\n",
    "\n",
    "#     modelHist = data_model.model.fit([x_train_00], [y_train_00], validation_data=([x_test_00],[y_test_00]), batch_size=batch_size,epochs=400, verbose=0,callbacks=[reduce_lr,early_stop])\n",
    "#     print(data_model.mse(data_model.predict('train'), y_train_00), data_model.mse(data_model.predict('test'),y_test_00))\n",
    "#     print(data_model.mae(data_model.predict('train'), y_train_00), data_model.mae(data_model.predict('test'),y_test_00))\n",
    "#     lstmResults.append((alpha,data_model.mse(data_model.predict('test'),y_test_00),data_model.mae(data_model.predict('test'),y_test_00)))\n",
    "# data_model.model.summary()\n",
    "# pd.DataFrame(lstmResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lag = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params        = {\"name\":\"PO\"}\n",
    "\n",
    "lastValue_data_model = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "lastValue_data_model.preprocessData()\n",
    "\n",
    "model = models.BaseModels(\"lastValue\",historic_data = speedDF[speedDF.columns[:int(valid_split*n_dates)*true_sequence_length]])\n",
    "lastValue_data_model.model=model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lastValue_model_plotting = models.ModelPlots(lastValue_data_model, data_cleaner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params        = {\"name\" : \"HA\"}\n",
    "\n",
    "timehistoric_data_model    = models.DataModel( speedDF,    input_lag, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "timehistoric_data_model.preprocessData()\n",
    "\n",
    "model =models.BaseModels(\"timeHistoric\",historic_data=speedDF[speedDF.columns[:int(valid_split*n_dates)*true_sequence_length]])\n",
    "timehistoric_data_model.model=model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR_data_models ={}\n",
    "for i in range(1,input_lag+1):\n",
    "    params        = {\"name\":\"AR \"+str(i)}\n",
    "    AR_data_models[i]=models.DataModel( speedDF,    i, output_lag, sequence_length, valid_split = valid_split, **params )\n",
    "    AR_data_models[i].preprocessData()\n",
    "    model =models.BaseModels(\"AR5\",historic_data=speedDF[speedDF.columns[:int(valid_split*n_dates)*true_sequence_length]],lag=i)\n",
    "    AR_data_models[i].model = model\n",
    "\n",
    "    \n",
    "    input_lag, output_lag, sequence_length = i, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "    params        = { \"scale_output\" : True }\n",
    "    test_data_model    = models.DataModel( speedsTest,    input_lag, output_lag, sequence_length, valid_split = 1, **params)\n",
    "    test_data_model.preprocessData()\n",
    "    test_data_x, test_data_y, _, _ = test_data_model.trainSplit()\n",
    "    \n",
    "    models_predictions[AR_data_models[i].name]=test_data_model.restorePredictionsAsDF(AR_data_models[i].predict('custom',x=test_data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params=[AR_data_models[5].model.models[section].params for section in np.arange(nSegments)]\n",
    "all_params=pd.DataFrame(all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = 497#np.random.randint(0,556)\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(AR_data_models[5].model.models[section].Y[0:10])\n",
    "plt.plot(AR_data_models[1].model.models[section].predict(start=speedDF.columns[5], end=speedDF.columns[15*1]))\n",
    "plt.plot(AR_data_models[2].model.models[section].predict(start=speedDF.columns[5], end=speedDF.columns[15*1]))\n",
    "plt.plot(AR_data_models[3].model.models[section].predict(start=speedDF.columns[5], end=speedDF.columns[15*1]))\n",
    "plt.plot(AR_data_models[4].model.models[section].predict(start=speedDF.columns[5], end=speedDF.columns[15*1]))\n",
    "plt.plot(AR_data_models[5].model.models[section].predict(start=speedDF.columns[5], end=speedDF.columns[15*1]))\n",
    "plt.subplot(122)\n",
    "plt.plot(AR_data_models[5].model.models[section].params)\n",
    "plt.ylim(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_baseline_models = [ timehistoric_data_model, lastValue_data_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in all_baseline_models:\n",
    "#     print(model.name)\n",
    "#     models_predictions[model.name]=model.restorePredictionsAsDF(model.predict('test'),'test')\n",
    "\n",
    "input_lag, output_lag, sequence_length = 5, 1, len(set(pd.to_datetime(speedDF.columns).time)) # speedDF.columns.size\n",
    "params        = { \"scale_output\" : True }\n",
    "test_data_model    = models.DataModel( speedsTest,    input_lag, output_lag, sequence_length, valid_split = 1, **params)\n",
    "test_data_model.preprocessData()\n",
    "test_data_x, test_data_y, _, _ = test_data_model.trainSplit()\n",
    "\n",
    "for model in all_baseline_models:\n",
    "    print(model.name)\n",
    "    models_predictions[model.name]=test_data_model.restorePredictionsAsDF(model.predict('custom',x=test_data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names =os.listdir(\"pred results/\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_predictions = {x[:-10]:pd.read_pickle(\"pred results/\"+x) for x in file_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dict_models_predictions = {k:v for k,v in models_predictions.items() if k in models_predictions.keys()}\n",
    "# sub_dict_models_predictions = {k:v for k,v in models_predictions.items() if k in ['Lasso-OLS', 'OLS', 'MLP', 'FC-2LSTM', 'TS-OLS', 'HA', 'PO', 'AR 1', 'AR 2']}\n",
    "sub_dict_models_predictions = {k:v for k,v in models_predictions.items() if k in ['Lasso-OLS', 'mix_preds','RSM','TS-OLS','RSM_12_15']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_predictions.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dict_models_predictions = {k:v for k,v in models_predictions.items() if k in models_predictions.keys()}\n",
    "\n",
    "import model_comparator\n",
    "model_comparator=reload(model_comparator)\n",
    "input_lag = 5\n",
    "output_lag=1\n",
    "true_model=models.DataModel(speedsTest,input_lag,output_lag,20,valid_split=1)\n",
    "true_model.preprocessData()\n",
    "true_values=true_model.restorePredictionsAsDF(true_model.trainSplit()[1],'train')\n",
    "mc=model_comparator.ModelCompare(sub_dict_models_predictions.copy(),\n",
    "                                 true_values=true_values)\n",
    "# mc.time_index=pd.to_datetime(mc.time_index).tz_localize(\"utc\").tz_convert(pytz.timezone(\"Europe/Paris\"))\n",
    "res_table=mc.comparisonTable().round(2)\n",
    "res_table=res_table.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_table.T.to_latex())\n",
    "res_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dict_models_predictions = {k:v for k,v in models_predictions.items() if k in ['Lasso','RS-Lasso','TS-Lasso',\"MLP\",\"FC-2LSTM\",'AR 3','HA']}\n",
    "\n",
    "import model_comparator\n",
    "model_comparator=reload(model_comparator)\n",
    "input_lag = 5\n",
    "output_lag=1\n",
    "true_model=models.DataModel(speedsTest,input_lag,output_lag,20,valid_split=1)\n",
    "true_model.preprocessData()\n",
    "true_values=true_model.restorePredictionsAsDF(true_model.trainSplit()[1],'train')\n",
    "mc=model_comparator.ModelCompare(sub_dict_models_predictions.copy(),\n",
    "                                 true_values=true_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dict_models_predictions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plt.figure(figsize=(18,6))\n",
    "# mc.sample_predictions(seg_ids=[speedsTest.index[6]],day_ids=pd.Series(speedsTest.columns).dt.date.unique()[-1:],intercept=test_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(18,18))\n",
    "# mc.plotAverageError(smooth_sigma=1,subplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "mc.fontsize=20\n",
    "mc.futurError()\n",
    "plt.legend(ncol=4,loc=(0.0,1.01),fontsize=13)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.savefig(\"average_daily_error.png\",dpi=900,pad_inches=0,bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "mc.plotTimeError()\n",
    "plt.legend(ncol=1,fontsize=12)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.savefig(\"average_time_error.png\",dpi=900,pad_inches=0,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(18,18))\n",
    "# mc.plotAveragePrediction(subplot=True,intercept=test_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(18,18))\n",
    "\n",
    "# mc.plotDiscreteSpeedError(test_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days_var = speedDF.abs().mean()\n",
    "# days_var=speedDF.abs().groupby(pd.to_datetime(speedDF.columns).date,axis=1).mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(22,10))\n",
    "# plt.plot(days_var.values)\n",
    "# plt.xticks(np.r_[:len(days_var.index)-1:66j].astype(int),days_var.index[np.r_[:len(days_var.index)-1:66j].astype(int)],rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(18,18))\n",
    "# mc.errorSegmentStdOrder(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "\n",
    "\"\"\"\n",
    "Kmeans clustering of segments\n",
    "\"\"\"\n",
    "grouped_data=(speedDF+intercept_extended_df).T.groupby(pd.to_datetime(speedDF.columns).time,axis=0).describe()\n",
    "\n",
    "raw_segs_desc=grouped_data.stack().T\n",
    "# raw_segs_desc=raw_segs_desc[raw_segs_desc.columns[raw_segs_desc.columns.get_level_values(1)=='std']]\n",
    "segs_desc=raw_segs_desc.apply(sklearn.preprocessing.minmax_scale,feature_range=(-1,1))\n",
    "\n",
    "lbls=sklearn.cluster.k_means(segs_desc,3,n_init=100)[1]\n",
    "\n",
    "mean_ordered_groups_index=(speedDF + intercept_extended_df).groupby(lbls).mean().mean(axis=1).sort_values().index\n",
    "\n",
    "group_labels=[\"low\",\"medium_low\",\"medium_high\",\"high\"]\n",
    "\n",
    "group_dict = dict(zip(mean_ordered_groups_index,group_labels))\n",
    "lbls=[group_dict[x] for x in lbls]\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,2,1)\n",
    "pd.Series(dict(zip(*np.unique(lbls,return_counts = True))),name=\"segment per group\").plot.bar(ax=plt.gca())\n",
    "plt.ylabel(\"#Roads\")\n",
    "\n",
    "\"\"\"\n",
    "average day timeseries\n",
    "\"\"\"\n",
    "plt.subplot(1,2,2)\n",
    "(speedDF+intercept_extended_df).groupby(pd.to_datetime(speedDF.columns).time,axis=1).mean().groupby(lbls).mean().T.plot(subplots=False,use_index=False,ax=plt.gca())\n",
    "xlbls=sorted(np.unique(pd.to_datetime(speedDF.columns).time))\n",
    "plt.xticks(np.arange(len(xlbls)),xlbls,rotation=90);\n",
    "plt.ylabel(\"Speed (km/h)\")\n",
    "plt.savefig(\"roads_clusters.png\", dpi=900, bbox_inches=\"tight\")\n",
    "\"\"\"\n",
    "full timeseries\n",
    "\"\"\"\n",
    "# plt.subplot(1,3,3)\n",
    "# (speedDF+intercept_extended_df).groupby(lbls).mean().T.plot(subplots=False,use_index=False,ax=plt.gca())\n",
    "# plt.title(\"full timeseries\")\n",
    "groups_seg_index = speedDF.groupby(lbls).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dict_models_predictions = {k:v for k,v in models_predictions.items() if k in ['Lasso','RS-Lasso','TS-Lasso',\"MLP\",\"FC-2LSTM\",'AR 3','HA']}\n",
    "\n",
    "input_lag = 5\n",
    "true_model=models.DataModel(speedsTest,input_lag,output_lag,20,valid_split=1)\n",
    "true_model.preprocessData()\n",
    "true_values=true_model.restorePredictionsAsDF(true_model.trainSplit()[1],'train')\n",
    "\n",
    "mc=model_comparator.ModelCompare(sub_dict_models_predictions.copy(),\n",
    "                                 true_values=true_values,\n",
    "                                 segments_index=groups_seg_index['medium_low'])\n",
    "print(mc.comparisonTable().round(2).T.to_latex())\n",
    "mc.comparisonTable().round(2).T\n",
    "res_table=mc.comparisonTable().round(2)\n",
    "res_table=res_table.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_table.T.to_latex())\n",
    "res_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "mc.fontsize=20\n",
    "\n",
    "mc.futurError()\n",
    "plt.legend(ncol=4,loc=(0.0,1.01),fontsize=13)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.savefig(\"average_daily_error_cluster.png\",dpi=900,pad_inches=0,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "mc.plotTimeError()\n",
    "plt.legend(ncol=1,fontsize=12)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "plt.savefig(\"average_time_error_cluster.png\",dpi=900,pad_inches=0,bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lag=5\n",
    "true_model=models.DataModel(speedDF,input_lag,output_lag,20,valid_split=0.7)\n",
    "true_model.preprocessData()\n",
    "true_values=true_model.restorePredictionsAsDF(true_model.trainSplit()[3],'test')\n",
    "mc=model_comparator.ModelCompare(sub_dict_models_predictions.copy(),\n",
    "                                 true_values=true_values[true_values[true_values.columns[pd.to_datetime(true_values.columns).date==datetime.datetime(2019,6,7).date()]].columns])\n",
    "mc.comparisonTable()\n",
    "print(mc.comparisonTable())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.plotTimeError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kepler_plotable_data(segmentsMeta,speedDF,At):\n",
    "    submeta = segmentsMeta.copy()\n",
    "    submeta = submeta.set_index(\"segmentID\").loc[speedDF.index]\n",
    "    loc_idx = submeta['loc'].apply(lambda x : x['coordinates'][len(x['coordinates'])//2][0]).values\n",
    "    df =pd.DataFrame(np.array([loc_idx]*len(speedDF)).flatten(),columns=['long_r'])\n",
    "    df = df.assign(long_l=pd.Series(loc_idx , index=range(0,nSegments**2,nSegments)))\n",
    "    df =df.assign(segmentId_r = np.array([speedDF.index.values]*len(speedDF)).flatten())\n",
    "\n",
    "    loc_idx=submeta['loc'].apply(lambda x : x['coordinates'][len(x['coordinates'])//2][1]).values\n",
    "    \n",
    "    df = df.assign(lat_r=np.array([loc_idx]*len(speedDF)).flatten() )\n",
    "    df = df.assign(lat_l=pd.Series(loc_idx,index=range(0,nSegments**2,nSegments)))\n",
    "    df = df.assign(segmentId_l=pd.Series(speedDF.index.values,index=range(0,nSegments**2,nSegments)))\n",
    "    df = df.ffill()\n",
    "\n",
    "    df=df.assign( rate = At.flatten() )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([A.coef_ for A in A_lasso])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_kepler_plotable_data(segmentsMeta,speedDF,abs(A))\n",
    "\n",
    "# df = df.assign( adjacency = adjacency_matrix.values.flatten() )\n",
    "\n",
    "# df = df.assign( diffusion = diffusion_matrix.values.flatten() )\n",
    "\n",
    "df.to_csv(\"../data/segment_flow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,9))\n",
    "plt.subplot(121)\n",
    "plt.imshow(A)\n",
    "plt.subplot(122)\n",
    "plt.imshow(adjacency_matrix>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_sum   = abs(A).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_count = (abs(A)!=0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_mean  = active_sum/((active_count==0) + active_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(active_mean))\n",
    "plt.figure()\n",
    "plt.plot(sorted(active_sum))\n",
    "plt.figure()\n",
    "plt.plot(sorted(active_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMap(A_l,log=False,name=\"map\"):\n",
    "\n",
    "    if log :\n",
    "        A_l=sklearn.preprocessing.minmax_scale(np.log1p(A_l))\n",
    "    else:\n",
    "        A_l=sklearn.preprocessing.minmax_scale(A_l)\n",
    "\n",
    "    color_coefs=speedDF.assign(color=A_l)['color']\n",
    "    color_coefs=color_coefs.apply(lambda x: matplotlib.colors.rgb2hex(plt.cm.hot(x)))\n",
    "    color_coefs=data_cleaner.mergedIndex.replace(color_coefs).replace(\"^[^#]+\",\"#000000\",regex=True).values\n",
    "\n",
    "    fmap = Plotting.getFoliumMap()\n",
    "    layer= Plotting.plotRoads(segmentsMeta,fmap=fmap,colors=color_coefs,inverseIndexes=data_cleaner.mergedIndex,weight=4)\n",
    "    Plotting.stackHistotyLayers([layer],fmap)\n",
    "    \n",
    "    fmap.save(name+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createMap(active_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createMap(active_sum, log=True, name=\"active_sum\")\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(np.log1p(active_sum.reshape(1,-1)).T,  cmap=plt.cm.hot)\n",
    "cbar=plt.colorbar()\n",
    "cbar.ax.set_yticklabels(np.expm1( cbar.get_ticks()).round(2));\n",
    "cbar.set_label(\"sum of active coefs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createMap(active_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs_std=(speedDF[train_dates_index]+intercept_extended_df[train_dates_index]).std(axis=1)\n",
    "createMap(segs_std,name=\"segstd\")\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(segs_std.values.reshape(1,-1).T,  cmap=plt.cm.hot)\n",
    "cbar=plt.colorbar()\n",
    "cbar.set_label(\"STD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs_std.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dict_models_predictions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "day_ids = np.unique(pd.to_datetime(mc.true_values.columns).date)[:2]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "for i,seg_i in enumerate(seg_ids):\n",
    "    \n",
    "    for j,day_i in enumerate(day_ids):\n",
    "        plt.subplot(len(seg_ids),len(day_ids),i*len(day_ids)+j+1)\n",
    "        time_ids  = mc.true_values.columns[pd.Series(mc.true_values.columns).dt.date.isin([day_i]).values]\n",
    "        true_vals = (mc.true_values+intercept_extended_df).loc[seg_i][time_ids]\n",
    "        Lasso_vals = (sub_dict_models_predictions['Lasso-OLS']+intercept_extended_df).loc[seg_i][time_ids]\n",
    "        plt.plot(true_vals.values)\n",
    "        plt.plot(Lasso_vals.values)\n",
    "        print(len(true_vals.values),len(Lasso_vals.values))\n",
    "        myslice=slice(None,None,2)\n",
    "        plt.xticks(np.arange(len(true_vals))[myslice],pd.Series(true_vals.index).dt.time.astype(str).values[myslice],rotation=90)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length=20\n",
    "all_results_df=pd.read_pickle('allresults_vf5.pckl')\n",
    "quarters = np.unique(pd.to_datetime(speedDF.columns).time)\n",
    "quarters.sort()\n",
    "A1_results=all_results_df[['loss','t','left']][all_results_df['left']==True]\n",
    "\n",
    "A1_results=A1_results.assign(weighted_loss_A1=A1_results.apply(lambda x: (x['t'])*x['loss'] ,axis=1))\n",
    "\n",
    "A2_results=all_results_df[['loss','t','left']][all_results_df['left']==False]\n",
    "\n",
    "A2_results=A2_results.assign(weighted_loss_A2=A2_results.apply(lambda x: ((sequence_length-1)-x['t'])*x['loss'] ,axis=1))\n",
    "\n",
    "plt.plot(((A1_results.groupby('t').min()['weighted_loss_A1']+A2_results.groupby('t').min()['weighted_loss_A2'])/(sequence_length-1)))\n",
    "plt.xticks(range(len(quarters)),quarters,rotation=90)\n",
    "plt.axhline(115.29880658445208,c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_A = pd.read_pickle(\"left_lasso_model.pckl\")\n",
    "R_A = pd.read_pickle(\"right_lasso_model.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defultdf=pd.DataFrame(index=range(556),columns=range(556))\n",
    "R_A=R_A.unstack()\n",
    "defultdf.update(R_A)\n",
    "R_A =defultdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defultdf=pd.DataFrame(index=range(556),columns=range(556))\n",
    "L_A=L_A.unstack()\n",
    "defultdf.update(L_A)\n",
    "L_A =defultdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(R_A>0).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(L_A>0).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((R_A>0)&(L_A>0)).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(((R_A>0)|(L_A>0))&(A>0)).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A>0).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(((R_A>0))&(L_A>0)).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(((R_A>0))&(A>0)).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(((L_A>0))&(A>0)).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.normalized_mutual_info_score((R_A>0).values.flatten(),(A>0).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.normalized_mutual_info_score((L_A>0).values.flatten(),(A>0).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmis=[sklearn.metrics.adjusted_mutual_info_score((L_A>tresh).values.flatten(),(R_A>tresh).values.flatten()  )for tresh in np.arange(0,1,0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,1,0.01),nmis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_kepler_plotable_data(segmentsMeta,speedDF,abs(R_A.fillna(0).values))\n",
    "df.to_csv(\"../data/segment_flow_R.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_kepler_plotable_data(segmentsMeta,speedDF,abs(L_A.fillna(0).values))\n",
    "df.to_csv(\"../data/segment_flow_L.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=R_A.fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_sum   = abs(A).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_count = (abs(A)!=0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_mean  = active_sum/((active_count==0) + active_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(active_mean))\n",
    "plt.figure()\n",
    "plt.plot(sorted(active_sum))\n",
    "plt.figure()\n",
    "plt.plot(sorted(active_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createMap(active_sum, log=True, name=\"active_sum_R\")\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(np.log1p(active_sum.reshape(1,-1)).T,  cmap=plt.cm.hot)\n",
    "cbar=plt.colorbar()\n",
    "cbar.ax.set_yticklabels(np.expm1( cbar.get_ticks()).round(2));\n",
    "cbar.set_label(\"sum of active coefs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=L_A.fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_sum   = abs(A).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_count = (abs(A)!=0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_mean  = active_sum/((active_count==0) + active_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(active_mean))\n",
    "plt.figure()\n",
    "plt.plot(sorted(active_sum))\n",
    "plt.figure()\n",
    "plt.plot(sorted(active_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createMap(active_sum, log=True, name=\"active_sum_L\")\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(np.log1p(active_sum.reshape(1,-1)).T,  cmap=plt.cm.hot)\n",
    "cbar=plt.colorbar()\n",
    "cbar.ax.set_yticklabels(np.expm1( cbar.get_ticks()).round(2));\n",
    "cbar.set_label(\"sum of active coefs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4*sum(np.random.uniform(0,1,1000000)**2+np.random.uniform(0,1,1000000)**2<=1)/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
